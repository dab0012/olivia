{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Olivia-Finder introduction**\n",
    "\n",
    "Olivia Finder is an open source tool for extracting data from software package dependency networks in package managers, designed to be used in conjunction with Olivia.\n",
    "Olivia Finder uses the web-scraping technique to get updated data, in addition to CSV files as another data source.\n",
    "\n",
    "\n",
    "**You can find the documentation in:**\n",
    "\n",
    "<a href=\"https://dab0012.github.io/olivia-finder\">\n",
    "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"132\" height=\"20\" role=\"img\" aria-label=\"docs: at Github Pages\"><title>docs: at Github Pages</title><linearGradient id=\"s\" x2=\"0\" y2=\"100%\"><stop offset=\"0\" stop-color=\"#bbb\" stop-opacity=\".1\"/><stop offset=\"1\" stop-opacity=\".1\"/></linearGradient><clipPath id=\"r\"><rect width=\"132\" height=\"20\" rx=\"3\" fill=\"#fff\"/></clipPath><g clip-path=\"url(#r)\"><rect width=\"35\" height=\"20\" fill=\"#555\"/><rect x=\"35\" width=\"97\" height=\"20\" fill=\"#4c1\"/><rect width=\"132\" height=\"20\" fill=\"url(#s)\"/></g><g fill=\"#fff\" text-anchor=\"middle\" font-family=\"Verdana,Geneva,DejaVu Sans,sans-serif\" text-rendering=\"geometricPrecision\" font-size=\"110\"><text aria-hidden=\"true\" x=\"185\" y=\"150\" fill=\"#010101\" fill-opacity=\".3\" transform=\"scale(.1)\" textLength=\"250\">docs</text><text x=\"185\" y=\"140\" transform=\"scale(.1)\" fill=\"#fff\" textLength=\"250\">docs</text><text aria-hidden=\"true\" x=\"825\" y=\"150\" fill=\"#010101\" fill-opacity=\".3\" transform=\"scale(.1)\" textLength=\"870\">at Github Pages</text><text x=\"825\" y=\"140\" transform=\"scale(.1)\" fill=\"#fff\" textLength=\"870\">at Github Pages</text></g></svg>\n",
    "</a>\n",
    "\n",
    "**Author:**\n",
    "\n",
    "Daniel Alonso BÃ¡scones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Previous requirements**\n",
    "\n",
    "**<span style=\"color: crimson\">\n",
    "Important:\n",
    "</span>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure to have the requirements installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the Library Route to Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the olivia_finder package\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DataSource**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasource interface allows us to obtain data from different sources:\n",
    "\n",
    "At this time we have two implementations available\n",
    "\n",
    "-   Web Scraping based\n",
    "-   CSV files based\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first place we import the implementation of the data source we want, for this example we will use the Bioconductor Scraper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of the class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance based on Web Scraping to obtain bioconductor packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olivia_finder.data_source.scrapers.bioconductor import BiocScraper\n",
    "bioc_scraper_ds = BiocScraper()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another instance but now using a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olivia_finder.data_source.csv_network import CSVNetwork\n",
    "# Load the network\n",
    "bioc_csv_ds = CSVNetwork(\n",
    "    \"results/csv_datasets/bioconductor_adjlist_scraping.csv\",  # Path to the CSV file\n",
    "    \"Bioconductor\",                         # Name of the data source\n",
    "    \"Bioconductor as a CSV file\",            # Description of the data source\n",
    "    dependent_field=\"name\",                 # Name of the field that contains the dependencies\n",
    "    dependency_field=\"dependency\",          # Name of the field that contains the name of the package\n",
    "    dependent_version_field=\"version\",      # Name of the field that contains the version of the package\n",
    "    dependency_version_field=\"dependency_version\",     # Name of the field that contains the version of the dependency\n",
    "    dependent_url_field=\"url\",              # Name of the field that contains the URL of the package\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain package names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the BiocScraper class gets the list of packages from <a href=\"https://bioconductor.org/packages/release/BiocViews.html#___Software\">Bioconductor packages list</a>\n",
    "\n",
    "Each specific implementation of a Scraper must manage this process on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABSSeq',\n",
       " 'ABarray',\n",
       " 'ACE',\n",
       " 'ACME',\n",
       " 'ADAM',\n",
       " 'ADAMgui',\n",
       " 'ADImpute',\n",
       " 'ADaCGH2',\n",
       " 'AGDEX',\n",
       " 'AIMS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package_list = bioc_scraper_ds.obtain_package_names()\n",
    "package_list[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the CSV-based implementation obtains the names of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABSSeq',\n",
       " 'ABarray',\n",
       " 'ACE',\n",
       " 'ACME',\n",
       " 'ADAM',\n",
       " 'ADAMgui',\n",
       " 'ADImpute',\n",
       " 'ADaCGH2',\n",
       " 'AGDEX',\n",
       " 'AIMS']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package_list = bioc_csv_ds.obtain_package_names()\n",
    "package_list[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:***\n",
    "\n",
    "As you can see the bioconductor dataset CSV that we have imported contains the same data as the web-based scraping.\n",
    "\n",
    "This is because as we will explain later, you can export the data we are using in several formats using olivia finder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain package data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the data from a package using his name:\n",
    "-   ```python\n",
    "    obtain_package_data(str)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'DeepBlueR',\n",
       " 'version': '1.24.1',\n",
       " 'dependencies': [{'name': 'R', 'version': '>= 3.3'},\n",
       "  {'name': 'XML', 'version': ''},\n",
       "  {'name': 'RCurl', 'version': ''},\n",
       "  {'name': 'GenomicRanges', 'version': ''},\n",
       "  {'name': 'data.table', 'version': ''},\n",
       "  {'name': 'stringr', 'version': ''},\n",
       "  {'name': 'diffr', 'version': ''},\n",
       "  {'name': 'dplyr', 'version': ''},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'rjson', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'R.utils', 'version': ''},\n",
       "  {'name': 'foreach', 'version': ''},\n",
       "  {'name': 'withr', 'version': ''},\n",
       "  {'name': 'rtracklayer', 'version': ''},\n",
       "  {'name': 'GenomeInfoDb', 'version': ''},\n",
       "  {'name': 'settings', 'version': ''},\n",
       "  {'name': 'filehash', 'version': ''}],\n",
       " 'url': 'https://www.bioconductor.org/packages/release/bioc/html/DeepBlueR.html'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepbluer = bioc_scraper_ds.obtain_package_data(\"DeepBlueR\")\n",
    "deepbluer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the sensitivity to **caps**, if the package has not been found, an **ScraperError** exception is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScraperError: Package deepbluer not found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    deepbluer2 = bioc_scraper_ds.obtain_package_data(\"deepbluer\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same with the data source based on CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'DeepBlueR',\n",
       " 'version': '1.24.1',\n",
       " 'url': 'https://www.bioconductor.org/packages/release/bioc/html/DeepBlueR.html',\n",
       " 'dependencies': [{'name': 'R', 'version': '>= 3.3'},\n",
       "  {'name': 'XML', 'version': nan},\n",
       "  {'name': 'RCurl', 'version': nan},\n",
       "  {'name': 'GenomicRanges', 'version': nan},\n",
       "  {'name': 'data.table', 'version': nan},\n",
       "  {'name': 'stringr', 'version': nan},\n",
       "  {'name': 'diffr', 'version': nan},\n",
       "  {'name': 'dplyr', 'version': nan},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'rjson', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'R.utils', 'version': nan},\n",
       "  {'name': 'foreach', 'version': nan},\n",
       "  {'name': 'withr', 'version': nan},\n",
       "  {'name': 'rtracklayer', 'version': nan},\n",
       "  {'name': 'GenomeInfoDb', 'version': nan},\n",
       "  {'name': 'settings', 'version': nan},\n",
       "  {'name': 'filehash', 'version': nan}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepbluer = bioc_csv_ds.obtain_package_data(\"DeepBlueR\")\n",
    "deepbluer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain packages data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the data from a list of package names using the function:\n",
    "-   ```python\n",
    "    obtain_packages_data(list[str])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ABSSeq',\n",
       "  'version': '1.52.0',\n",
       "  'dependencies': [{'name': 'R', 'version': '>= 2.10'},\n",
       "   {'name': 'methods', 'version': ''},\n",
       "   {'name': 'locfit', 'version': ''},\n",
       "   {'name': 'limma', 'version': ''}],\n",
       "  'url': 'https://www.bioconductor.org/packages/release/bioc/html/ABSSeq.html'},\n",
       " {'name': 'ABarray',\n",
       "  'version': '1.66.0',\n",
       "  'dependencies': [{'name': 'Biobase', 'version': ''},\n",
       "   {'name': 'graphics', 'version': ''},\n",
       "   {'name': 'grDevices', 'version': ''},\n",
       "   {'name': 'methods', 'version': ''},\n",
       "   {'name': 'multtest', 'version': ''},\n",
       "   {'name': 'stats', 'version': ''},\n",
       "   {'name': 'tcltk', 'version': ''},\n",
       "   {'name': 'utils', 'version': ''}],\n",
       "  'url': 'https://www.bioconductor.org/packages/release/bioc/html/ABarray.html'},\n",
       " {'name': 'ACE',\n",
       "  'version': '1.16.0',\n",
       "  'dependencies': [{'name': 'R', 'version': '>= 3.4'},\n",
       "   {'name': 'Biobase', 'version': ''},\n",
       "   {'name': 'QDNAseq', 'version': ''},\n",
       "   {'name': 'ggplot2', 'version': ''},\n",
       "   {'name': 'grid', 'version': ''},\n",
       "   {'name': 'stats', 'version': ''},\n",
       "   {'name': 'utils', 'version': ''},\n",
       "   {'name': 'methods', 'version': ''},\n",
       "   {'name': 'grDevices', 'version': ''},\n",
       "   {'name': 'GenomicRanges', 'version': ''}],\n",
       "  'url': 'https://www.bioconductor.org/packages/release/bioc/html/ACE.html'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkgs_data, not_found = bioc_scraper_ds.obtain_packages_data(package_list[:3])\n",
    "pkgs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same with the data source based on CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'name': 'ABSSeq',\n",
       "   'version': '1.52.0',\n",
       "   'url': 'https://www.bioconductor.org/packages/release/bioc/html/ABSSeq.html',\n",
       "   'dependencies': [{'name': 'R', 'version': '>= 2.10'},\n",
       "    {'name': 'methods', 'version': nan},\n",
       "    {'name': 'locfit', 'version': nan},\n",
       "    {'name': 'limma', 'version': nan}]},\n",
       "  {'name': 'ABarray',\n",
       "   'version': '1.66.0',\n",
       "   'url': 'https://www.bioconductor.org/packages/release/bioc/html/ABarray.html',\n",
       "   'dependencies': [{'name': 'Biobase', 'version': nan},\n",
       "    {'name': 'graphics', 'version': nan},\n",
       "    {'name': 'grDevices', 'version': nan},\n",
       "    {'name': 'methods', 'version': nan},\n",
       "    {'name': 'multtest', 'version': nan},\n",
       "    {'name': 'stats', 'version': nan},\n",
       "    {'name': 'tcltk', 'version': nan},\n",
       "    {'name': 'utils', 'version': nan}]},\n",
       "  {'name': 'ACE',\n",
       "   'version': '1.16.0',\n",
       "   'url': 'https://www.bioconductor.org/packages/release/bioc/html/ACE.html',\n",
       "   'dependencies': [{'name': 'R', 'version': '>= 3.4'},\n",
       "    {'name': 'Biobase', 'version': nan},\n",
       "    {'name': 'QDNAseq', 'version': nan},\n",
       "    {'name': 'ggplot2', 'version': nan},\n",
       "    {'name': 'grid', 'version': nan},\n",
       "    {'name': 'stats', 'version': nan},\n",
       "    {'name': 'utils', 'version': nan},\n",
       "    {'name': 'methods', 'version': nan},\n",
       "    {'name': 'grDevices', 'version': nan},\n",
       "    {'name': 'GenomicRanges', 'version': nan}]}],\n",
       " [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packages = bioc_csv_ds.obtain_packages_data(package_list[:3])\n",
    "packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages not found appear as the second object of the tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deepbluer']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkgs_data, not_found = bioc_scraper_ds.obtain_packages_data([\"deepbluer\", \"DeepBlueR\"])\n",
    "not_found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain dependencies recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DeepBlueR': [{'name': 'R', 'version': '>= 3.3'},\n",
       "  {'name': 'XML', 'version': ''},\n",
       "  {'name': 'RCurl', 'version': ''},\n",
       "  {'name': 'GenomicRanges', 'version': ''},\n",
       "  {'name': 'data.table', 'version': ''},\n",
       "  {'name': 'stringr', 'version': ''},\n",
       "  {'name': 'diffr', 'version': ''},\n",
       "  {'name': 'dplyr', 'version': ''},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'rjson', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'R.utils', 'version': ''},\n",
       "  {'name': 'foreach', 'version': ''},\n",
       "  {'name': 'withr', 'version': ''},\n",
       "  {'name': 'rtracklayer', 'version': ''},\n",
       "  {'name': 'GenomeInfoDb', 'version': ''},\n",
       "  {'name': 'settings', 'version': ''},\n",
       "  {'name': 'filehash', 'version': ''}],\n",
       " 'GenomicRanges': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'stats4', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'GenomeInfoDb', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'XVector', 'version': ''}],\n",
       " 'BiocGenerics': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'graphics', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''}],\n",
       " 'S4Vectors': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'stats4', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''}],\n",
       " 'IRanges': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'stats4', 'version': ''}],\n",
       " 'GenomeInfoDb': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'stats4', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'RCurl', 'version': ''},\n",
       "  {'name': 'GenomeInfoDbData', 'version': ''}],\n",
       " 'XVector': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'tools', 'version': ''},\n",
       "  {'name': 'zlibbioc', 'version': ''}],\n",
       " 'zlibbioc': [],\n",
       " 'rtracklayer': [{'name': 'R', 'version': '>= 3.5.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'GenomicRanges', 'version': ''},\n",
       "  {'name': 'XML', 'version': '>= 1.98-0'},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'XVector', 'version': ''},\n",
       "  {'name': 'GenomeInfoDb', 'version': ''},\n",
       "  {'name': 'Biostrings', 'version': ''},\n",
       "  {'name': 'zlibbioc', 'version': ''},\n",
       "  {'name': 'RCurl', 'version': '>= 1.4-2'},\n",
       "  {'name': 'Rsamtools', 'version': ''},\n",
       "  {'name': 'GenomicAlignments', 'version': ''},\n",
       "  {'name': 'BiocIO', 'version': ''},\n",
       "  {'name': 'tools', 'version': ''},\n",
       "  {'name': 'restfulr', 'version': '>= 0.0.13'}],\n",
       " 'Biostrings': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'XVector', 'version': ''},\n",
       "  {'name': 'GenomeInfoDb', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'grDevices', 'version': ''},\n",
       "  {'name': 'graphics', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'crayon', 'version': ''}],\n",
       " 'Rsamtools': [{'name': 'methods', 'version': ''},\n",
       "  {'name': 'GenomeInfoDb', 'version': ''},\n",
       "  {'name': 'GenomicRanges', 'version': ''},\n",
       "  {'name': 'Biostrings', 'version': ''},\n",
       "  {'name': 'R', 'version': '>= 3.5.0'},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'XVector', 'version': ''},\n",
       "  {'name': 'zlibbioc', 'version': ''},\n",
       "  {'name': 'bitops', 'version': ''},\n",
       "  {'name': 'BiocParallel', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''}],\n",
       " 'BiocParallel': [{'name': 'methods', 'version': ''},\n",
       "  {'name': 'R', 'version': '>= 3.5.0'},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'futile.logger', 'version': ''},\n",
       "  {'name': 'parallel', 'version': ''},\n",
       "  {'name': 'snow', 'version': ''},\n",
       "  {'name': 'codetools', 'version': ''}],\n",
       " 'GenomicAlignments': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'GenomeInfoDb', 'version': ''},\n",
       "  {'name': 'GenomicRanges', 'version': ''},\n",
       "  {'name': 'SummarizedExperiment', 'version': ''},\n",
       "  {'name': 'Biostrings', 'version': ''},\n",
       "  {'name': 'Rsamtools', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'BiocParallel', 'version': ''}],\n",
       " 'SummarizedExperiment': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'MatrixGenerics', 'version': ''},\n",
       "  {'name': 'GenomicRanges', 'version': ''},\n",
       "  {'name': 'Biobase', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'tools', 'version': ''},\n",
       "  {'name': 'Matrix', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'GenomeInfoDb', 'version': ''},\n",
       "  {'name': 'DelayedArray', 'version': ''}],\n",
       " 'MatrixGenerics': [{'name': 'matrixStats', 'version': '>= 0.60.1'},\n",
       "  {'name': 'methods', 'version': ''}],\n",
       " 'Biobase': [{'name': 'R', 'version': '>= 2.10'},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'methods', 'version': ''}],\n",
       " 'DelayedArray': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'stats4', 'version': ''},\n",
       "  {'name': 'Matrix', 'version': ''},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'MatrixGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'IRanges', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''}],\n",
       " 'BiocIO': [{'name': 'R', 'version': '>= 4.0'},\n",
       "  {'name': 'BiocGenerics', 'version': ''},\n",
       "  {'name': 'S4Vectors', 'version': ''},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'tools', 'version': ''}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeepBlueR_depnet_scr = bioc_scraper_ds.generate_package_dependency_network(\"DeepBlueR\")\n",
    "DeepBlueR_depnet_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DeepBlueR': [{'name': 'R', 'version': '>= 3.3'},\n",
       "  {'name': 'XML', 'version': nan},\n",
       "  {'name': 'RCurl', 'version': nan},\n",
       "  {'name': 'GenomicRanges', 'version': nan},\n",
       "  {'name': 'data.table', 'version': nan},\n",
       "  {'name': 'stringr', 'version': nan},\n",
       "  {'name': 'diffr', 'version': nan},\n",
       "  {'name': 'dplyr', 'version': nan},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'rjson', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'R.utils', 'version': nan},\n",
       "  {'name': 'foreach', 'version': nan},\n",
       "  {'name': 'withr', 'version': nan},\n",
       "  {'name': 'rtracklayer', 'version': nan},\n",
       "  {'name': 'GenomeInfoDb', 'version': nan},\n",
       "  {'name': 'settings', 'version': nan},\n",
       "  {'name': 'filehash', 'version': nan}],\n",
       " 'GenomicRanges': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'stats4', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'GenomeInfoDb', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'XVector', 'version': nan}],\n",
       " 'BiocGenerics': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'graphics', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'graphics', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan}],\n",
       " 'S4Vectors': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'stats4', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan}],\n",
       " 'IRanges': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'stats4', 'version': nan}],\n",
       " 'GenomeInfoDb': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'stats4', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'RCurl', 'version': nan},\n",
       "  {'name': 'GenomeInfoDbData', 'version': nan}],\n",
       " 'XVector': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'tools', 'version': nan},\n",
       "  {'name': 'zlibbioc', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan}],\n",
       " 'zlibbioc': [],\n",
       " 'rtracklayer': [{'name': 'R', 'version': '>= 3.5.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'GenomicRanges', 'version': nan},\n",
       "  {'name': 'XML', 'version': '>= 1.98-0'},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'XVector', 'version': nan},\n",
       "  {'name': 'GenomeInfoDb', 'version': nan},\n",
       "  {'name': 'Biostrings', 'version': nan},\n",
       "  {'name': 'zlibbioc', 'version': nan},\n",
       "  {'name': 'RCurl', 'version': '>= 1.4-2'},\n",
       "  {'name': 'Rsamtools', 'version': nan},\n",
       "  {'name': 'GenomicAlignments', 'version': nan},\n",
       "  {'name': 'BiocIO', 'version': nan},\n",
       "  {'name': 'tools', 'version': nan},\n",
       "  {'name': 'restfulr', 'version': '>= 0.0.13'}],\n",
       " 'Biostrings': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'XVector', 'version': nan},\n",
       "  {'name': 'GenomeInfoDb', 'version': nan},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'grDevices', 'version': nan},\n",
       "  {'name': 'graphics', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'crayon', 'version': nan}],\n",
       " 'Rsamtools': [{'name': 'methods', 'version': nan},\n",
       "  {'name': 'GenomeInfoDb', 'version': nan},\n",
       "  {'name': 'GenomicRanges', 'version': nan},\n",
       "  {'name': 'Biostrings', 'version': nan},\n",
       "  {'name': 'R', 'version': '>= 3.5.0'},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'XVector', 'version': nan},\n",
       "  {'name': 'zlibbioc', 'version': nan},\n",
       "  {'name': 'bitops', 'version': nan},\n",
       "  {'name': 'BiocParallel', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan}],\n",
       " 'BiocParallel': [{'name': 'methods', 'version': nan},\n",
       "  {'name': 'R', 'version': '>= 3.5.0'},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'futile.logger', 'version': nan},\n",
       "  {'name': 'parallel', 'version': nan},\n",
       "  {'name': 'snow', 'version': nan},\n",
       "  {'name': 'codetools', 'version': nan}],\n",
       " 'GenomicAlignments': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'GenomeInfoDb', 'version': nan},\n",
       "  {'name': 'GenomicRanges', 'version': nan},\n",
       "  {'name': 'SummarizedExperiment', 'version': nan},\n",
       "  {'name': 'Biostrings', 'version': nan},\n",
       "  {'name': 'Rsamtools', 'version': nan},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'GenomicRanges', 'version': nan},\n",
       "  {'name': 'Biostrings', 'version': nan},\n",
       "  {'name': 'Rsamtools', 'version': nan},\n",
       "  {'name': 'BiocParallel', 'version': nan}],\n",
       " 'SummarizedExperiment': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'MatrixGenerics', 'version': nan},\n",
       "  {'name': 'GenomicRanges', 'version': nan},\n",
       "  {'name': 'Biobase', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan},\n",
       "  {'name': 'tools', 'version': nan},\n",
       "  {'name': 'Matrix', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'GenomeInfoDb', 'version': nan},\n",
       "  {'name': 'DelayedArray', 'version': nan}],\n",
       " 'MatrixGenerics': [{'name': 'matrixStats', 'version': '>= 0.60.1'},\n",
       "  {'name': 'methods', 'version': nan}],\n",
       " 'Biobase': [{'name': 'R', 'version': '>= 2.10'},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'utils', 'version': nan},\n",
       "  {'name': 'methods', 'version': nan}],\n",
       " 'DelayedArray': [{'name': 'R', 'version': '>= 4.0.0'},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'stats4', 'version': nan},\n",
       "  {'name': 'Matrix', 'version': nan},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'MatrixGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'IRanges', 'version': nan},\n",
       "  {'name': 'stats', 'version': nan}],\n",
       " 'BiocIO': [{'name': 'R', 'version': '>= 4.0'},\n",
       "  {'name': 'BiocGenerics', 'version': nan},\n",
       "  {'name': 'S4Vectors', 'version': nan},\n",
       "  {'name': 'methods', 'version': nan},\n",
       "  {'name': 'tools', 'version': nan}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeepBlueR_depnet_csv = bioc_csv_ds.generate_package_dependency_network(\"DeepBlueR\", deep_level=10)\n",
    "DeepBlueR_depnet_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 19:39:14 [   DEBUG] Added SSLProxies to proxy builders (logger.py:122)\n",
      "2023-03-24 19:39:14 [   DEBUG] Added FreeProxyList to proxy builders (logger.py:122)\n",
      "2023-03-24 19:39:14 [   DEBUG] Added GeonodeProxy to proxy builders (logger.py:122)\n",
      "2023-03-24 19:39:14 [   DEBUG] Starting new HTTPS connection (1): www.sslproxies.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:14 [   DEBUG] https://www.sslproxies.org:443 \"GET / HTTP/1.1\" 200 None (connectionpool.py:452)\n",
      "2023-03-24 19:39:14 [   DEBUG] Found 100 proxies from SSLProxies (logger.py:122)\n",
      "2023-03-24 19:39:14 [   DEBUG] Starting new HTTPS connection (1): free-proxy-list.net:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:14 [   DEBUG] https://free-proxy-list.net:443 \"GET /anonymous-proxy.html HTTP/1.1\" 200 None (connectionpool.py:452)\n",
      "2023-03-24 19:39:14 [   DEBUG] Found 100 proxies from FreeProxyList (logger.py:122)\n",
      "2023-03-24 19:39:14 [   DEBUG] Starting new HTTPS connection (1): proxylist.geonode.com:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:32 [   DEBUG] https://proxylist.geonode.com:443 \"GET /api/proxy-list?limit=500&page=1&sort_by=lastChecked&sort_type=desc HTTP/1.1\" 200 None (connectionpool.py:452)\n",
      "2023-03-24 19:39:32 [   DEBUG] Found 500 proxies from GeonodeProxy (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Proxies len: 682 (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Proxy Handler initialized with 682 proxies (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Useragents loaded from file: /home/dnllns/Documentos/repositorios/olivia-finder/olivia_finder/olivia_finder/myrequests/data/useragents.txt (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Scraping package networkx (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Proxy list rotated, using 159.138.130.126:8999, next will be 34.135.166.24:80 (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Using proxy: {'http': 'http://159.138.130.126:8999'} (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Using user agent: Mozilla/5.0 (Windows Phone 8.1; ARM; Trident/7.0; Touch; rv:11.0; IEMobile/11.0; NOKIA; Lumia 635) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:32 [   DEBUG] https://pypi.org:443 \"GET /pypi/networkx/json HTTP/1.1\" 200 35312 (connectionpool.py:452)\n",
      "2023-03-24 19:39:32 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Package networkx scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Scraping package numpy (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Proxy list rotated, using 34.135.166.24:80, next will be 170.254.255.240:45816 (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Using proxy: {'http': 'http://34.135.166.24:80'} (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/37.0.2062.94 Chrome/37.0.2062.94 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:32 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:33 [   DEBUG] https://pypi.org:443 \"GET /pypi/numpy/json HTTP/1.1\" 200 310143 (connectionpool.py:452)\n",
      "2023-03-24 19:39:33 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Package numpy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Scraping package scipy (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Proxy list rotated, using 170.254.255.240:45816, next will be 186.251.255.101:31337 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Using proxy: {'http': 'http://170.254.255.240:45816'} (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:39.0) Gecko/20100101 Firefox/39.0 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:33 [   DEBUG] https://pypi.org:443 \"GET /pypi/scipy/json HTTP/1.1\" 200 196193 (connectionpool.py:452)\n",
      "2023-03-24 19:39:33 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Package scipy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Scraping package pytest (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Proxy list rotated, using 186.251.255.101:31337, next will be 177.220.243.130:4153 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Using proxy: {'http': 'http://186.251.255.101:31337'} (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:33 [   DEBUG] https://pypi.org:443 \"GET /pypi/pytest/json HTTP/1.1\" 200 46055 (connectionpool.py:452)\n",
      "2023-03-24 19:39:33 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Package pytest scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Scraping package attrs (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Proxy list rotated, using 177.220.243.130:4153, next will be 121.1.41.162:111 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Using proxy: {'http': 'http://177.220.243.130:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Using user agent: Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Win64; x64; Trident/7.0) (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:33 [   DEBUG] https://pypi.org:443 \"GET /pypi/attrs/json HTTP/1.1\" 200 13328 (connectionpool.py:452)\n",
      "2023-03-24 19:39:33 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Package attrs scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Scraping package coverage (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Proxy list rotated, using 121.1.41.162:111, next will be 89.25.23.212:4153 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Using proxy: {'http': 'http://121.1.41.162:111'} (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.120 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:33 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:33 [   DEBUG] https://pypi.org:443 \"GET /pypi/coverage/json HTTP/1.1\" 200 482395 (connectionpool.py:452)\n",
      "2023-03-24 19:39:34 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Package coverage scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Scraping package furo (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Proxy list rotated, using 89.25.23.212:4153, next will be 218.1.200.232:57114 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Using proxy: {'http': 'http://89.25.23.212:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:38.0) Gecko/20100101 Firefox/38.0 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:34 [   DEBUG] https://pypi.org:443 \"GET /pypi/furo/json HTTP/1.1\" 200 23146 (connectionpool.py:452)\n",
      "2023-03-24 19:39:34 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Package furo scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Scraping package sphinx (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Proxy list rotated, using 218.1.200.232:57114, next will be 51.91.20.206:20959 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Using proxy: {'http': 'http://218.1.200.232:57114'} (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:34 [   DEBUG] https://pypi.org:443 \"GET /pypi/sphinx/json HTTP/1.1\" 200 67935 (connectionpool.py:452)\n",
      "2023-03-24 19:39:34 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Package sphinx scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Scraping package myst (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Proxy list rotated, using 51.91.20.206:20959, next will be 51.89.117.242:45436 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Using proxy: {'http': 'http://51.91.20.206:20959'} (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:34 [   DEBUG] https://pypi.org:443 \"GET /pypi/myst/json HTTP/1.1\" 200 3840 (connectionpool.py:452)\n",
      "2023-03-24 19:39:34 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Package myst scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Scraping package zope (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Proxy list rotated, using 51.89.117.242:45436, next will be 65.109.84.104:80 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Using proxy: {'http': 'http://51.89.117.242:45436'} (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:34 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:34 [   DEBUG] https://pypi.org:443 \"GET /pypi/zope/json HTTP/1.1\" 200 28028 (connectionpool.py:452)\n",
      "2023-03-24 19:39:35 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Package zope scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Scraping package sphinxcontrib (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Proxy list rotated, using 65.109.84.104:80, next will be 193.8.87.43:4444 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using proxy: {'http': 'http://65.109.84.104:80'} (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.5.17 (KHTML, like Gecko) Version/7.1.5 Safari/537.85.14 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:35 [   DEBUG] https://pypi.org:443 \"GET /pypi/sphinxcontrib/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:39:35 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] ScraperError: Package sphinxcontrib not found (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] The package sphinxcontrib, as dependency of attrs does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Scraping package towncrier (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Proxy list rotated, using 193.8.87.43:4444, next will be 130.41.109.158:8080 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using proxy: {'http': 'http://193.8.87.43:4444'} (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:35 [   DEBUG] https://pypi.org:443 \"GET /pypi/towncrier/json HTTP/1.1\" 200 9387 (connectionpool.py:452)\n",
      "2023-03-24 19:39:35 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Package towncrier scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Scraping package hypothesis (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Proxy list rotated, using 130.41.109.158:8080, next will be 65.108.230.238:45977 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using proxy: {'http': 'http://130.41.109.158:8080'} (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.2; RCT6203W46 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/30.0.0.0 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:35 [   DEBUG] https://pypi.org:443 \"GET /pypi/hypothesis/json HTTP/1.1\" 200 328576 (connectionpool.py:452)\n",
      "2023-03-24 19:39:35 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Package hypothesis scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Scraping package pympler (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Proxy list rotated, using 65.108.230.238:45977, next will be 116.68.207.233:1080 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using proxy: {'http': 'http://65.108.230.238:45977'} (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.132 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:35 [   DEBUG] https://pypi.org:443 \"GET /pypi/pympler/json HTTP/1.1\" 200 6289 (connectionpool.py:452)\n",
      "2023-03-24 19:39:35 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Package pympler scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Scraping package cloudpickle (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Proxy list rotated, using 116.68.207.233:1080, next will be 110.8.238.133:1080 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using proxy: {'http': 'http://116.68.207.233:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:35 [   DEBUG] https://pypi.org:443 \"GET /pypi/cloudpickle/json HTTP/1.1\" 200 14648 (connectionpool.py:452)\n",
      "2023-03-24 19:39:35 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Package cloudpickle scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Scraping package mypy (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Proxy list rotated, using 110.8.238.133:1080, next will be 180.180.12.51:4145 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using proxy: {'http': 'http://110.8.238.133:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_4 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) GSA/7.0.55539 Mobile/12H143 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:39:35 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:36 [   DEBUG] https://pypi.org:443 \"GET /pypi/mypy/json HTTP/1.1\" 200 125194 (connectionpool.py:452)\n",
      "2023-03-24 19:39:36 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Package mypy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Scraping package iniconfig (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Proxy list rotated, using 180.180.12.51:4145, next will be 173.82.140.16:80 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using proxy: {'http': 'http://180.180.12.51:4145'} (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using user agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.1) Gecko/2008070208 Firefox/3.0.1 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:36 [   DEBUG] https://pypi.org:443 \"GET /pypi/iniconfig/json HTTP/1.1\" 200 3147 (connectionpool.py:452)\n",
      "2023-03-24 19:39:36 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Package iniconfig scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Scraping package packaging (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Proxy list rotated, using 173.82.140.16:80, next will be 177.71.77.202:20183 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using proxy: {'http': 'http://173.82.140.16:80'} (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:36 [   DEBUG] https://pypi.org:443 \"GET /pypi/packaging/json HTTP/1.1\" 200 14204 (connectionpool.py:452)\n",
      "2023-03-24 19:39:36 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Package packaging scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Scraping package pluggy (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Proxy list rotated, using 177.71.77.202:20183, next will be 43.153.30.185:443 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using proxy: {'http': 'http://177.71.77.202:20183'} (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.2; SM-T217S Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:36 [   DEBUG] https://pypi.org:443 \"GET /pypi/pluggy/json HTTP/1.1\" 200 7540 (connectionpool.py:452)\n",
      "2023-03-24 19:39:36 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Package pluggy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Scraping package importlib (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Proxy list rotated, using 43.153.30.185:443, next will be 64.225.4.63:9998 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using proxy: {'http': 'http://43.153.30.185:443'} (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.65 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:36 [   DEBUG] https://pypi.org:443 \"GET /pypi/importlib/json HTTP/1.1\" 200 3065 (connectionpool.py:452)\n",
      "2023-03-24 19:39:36 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Package importlib scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Scraping package pre (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Proxy list rotated, using 64.225.4.63:9998, next will be 170.254.255.227:45816 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using proxy: {'http': 'http://64.225.4.63:9998'} (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; U; Android 4.4.3; en-us; KFAPWI Build/KTU84M) AppleWebKit/537.36 (KHTML, like Gecko) Silk/3.68 like Chrome/39.0.2171.93 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:36 [   DEBUG] https://pypi.org:443 \"GET /pypi/pre/json HTTP/1.1\" 200 1227 (connectionpool.py:452)\n",
      "2023-03-24 19:39:36 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Package pre scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Scraping package tox (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Proxy list rotated, using 170.254.255.227:45816, next will be 103.70.206.17:59311 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using proxy: {'http': 'http://170.254.255.227:45816'} (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:36 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:37 [   DEBUG] https://pypi.org:443 \"GET /pypi/tox/json HTTP/1.1\" 200 54078 (connectionpool.py:452)\n",
      "2023-03-24 19:39:37 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Package tox scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Scraping package exceptiongroup (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Proxy list rotated, using 103.70.206.17:59311, next will be 172.98.20.80:10001 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using proxy: {'http': 'http://103.70.206.17:59311'} (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:38.0) Gecko/20100101 Firefox/38.0 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:37 [   DEBUG] https://pypi.org:443 \"GET /pypi/exceptiongroup/json HTTP/1.1\" 200 8097 (connectionpool.py:452)\n",
      "2023-03-24 19:39:37 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Package exceptiongroup scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Scraping package tomli (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Proxy list rotated, using 172.98.20.80:10001, next will be 62.171.156.226:3128 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using proxy: {'http': 'http://172.98.20.80:10001'} (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:37 [   DEBUG] https://pypi.org:443 \"GET /pypi/tomli/json HTTP/1.1\" 200 10753 (connectionpool.py:452)\n",
      "2023-03-24 19:39:37 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Package tomli scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Scraping package colorama (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Proxy list rotated, using 62.171.156.226:3128, next will be 62.33.235.41:4153 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using proxy: {'http': 'http://62.171.156.226:3128'} (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.3; KFTHWI Build/KTU84M) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/34.0.0.0 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:37 [   DEBUG] https://pypi.org:443 \"GET /pypi/colorama/json HTTP/1.1\" 200 21484 (connectionpool.py:452)\n",
      "2023-03-24 19:39:37 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Package colorama scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Scraping package argcomplete (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Proxy list rotated, using 62.33.235.41:4153, next will be 70.63.165.22:32940 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using proxy: {'http': 'http://62.33.235.41:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:37 [   DEBUG] https://pypi.org:443 \"GET /pypi/argcomplete/json HTTP/1.1\" 200 30008 (connectionpool.py:452)\n",
      "2023-03-24 19:39:37 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Package argcomplete scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Scraping package flake8 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Proxy list rotated, using 70.63.165.22:32940, next will be 142.93.250.71:7497 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using proxy: {'http': 'http://70.63.165.22:32940'} (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.0; SAMSUNG-SM-G900A Build/LRX21T) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/2.1 Chrome/34.0.1847.76 Mobile Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:37 [   DEBUG] https://pypi.org:443 \"GET /pypi/flake8/json HTTP/1.1\" 200 24560 (connectionpool.py:452)\n",
      "2023-03-24 19:39:37 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Package flake8 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Scraping package pexpect (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Proxy list rotated, using 142.93.250.71:7497, next will be 202.159.19.213:443 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using proxy: {'http': 'http://142.93.250.71:7497'} (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.118 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:37 [   DEBUG] https://pypi.org:443 \"GET /pypi/pexpect/json HTTP/1.1\" 200 5653 (connectionpool.py:452)\n",
      "2023-03-24 19:39:37 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Package pexpect scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Scraping package wheel (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Proxy list rotated, using 202.159.19.213:443, next will be 139.162.196.67:60586 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using proxy: {'http': 'http://202.159.19.213:443'} (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1; rv:34.0) Gecko/20100101 Firefox/34.0 (logger.py:122)\n",
      "2023-03-24 19:39:37 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:38 [   DEBUG] https://pypi.org:443 \"GET /pypi/wheel/json HTTP/1.1\" 200 20531 (connectionpool.py:452)\n",
      "2023-03-24 19:39:38 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Package wheel scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Scraping package mock (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Proxy list rotated, using 139.162.196.67:60586, next will be 131.108.118.27:2022 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Using proxy: {'http': 'http://139.162.196.67:60586'} (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:35.0) Gecko/20100101 Firefox/35.0 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:38 [   DEBUG] https://pypi.org:443 \"GET /pypi/mock/json HTTP/1.1\" 200 12099 (connectionpool.py:452)\n",
      "2023-03-24 19:39:38 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Package mock scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Scraping package twine (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Proxy list rotated, using 131.108.118.27:2022, next will be 64.225.8.132:9979 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Using proxy: {'http': 'http://131.108.118.27:2022'} (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:38 [   DEBUG] https://pypi.org:443 \"GET /pypi/twine/json HTTP/1.1\" 200 17769 (connectionpool.py:452)\n",
      "2023-03-24 19:39:38 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Package twine scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Scraping package blurb (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Proxy list rotated, using 64.225.8.132:9979, next will be 186.251.253.14:31337 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Using proxy: {'http': 'http://64.225.8.132:9979'} (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; U; Android 4.0.4; en-us; KFJWI Build/IMM76D) AppleWebKit/537.36 (KHTML, like Gecko) Silk/3.68 like Chrome/39.0.2171.93 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:38 [   DEBUG] https://pypi.org:443 \"GET /pypi/blurb/json HTTP/1.1\" 200 8231 (connectionpool.py:452)\n",
      "2023-03-24 19:39:38 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Package blurb scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Scraping package nose (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Proxy list rotated, using 186.251.253.14:31337, next will be 111.199.70.169:1080 (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Using proxy: {'http': 'http://186.251.253.14:31337'} (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/7.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; MS-RTC LM 8; .NET4.0C; .NET4.0E) (logger.py:122)\n",
      "2023-03-24 19:39:38 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:39 [   DEBUG] https://pypi.org:443 \"GET /pypi/nose/json HTTP/1.1\" 200 4623 (connectionpool.py:452)\n",
      "2023-03-24 19:39:39 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Package nose scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Scraping package pygments (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Proxy list rotated, using 111.199.70.169:1080, next will be 89.218.5.108:50733 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using proxy: {'http': 'http://111.199.70.169:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.111 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:39 [   DEBUG] https://pypi.org:443 \"GET /pypi/pygments/json HTTP/1.1\" 200 25111 (connectionpool.py:452)\n",
      "2023-03-24 19:39:39 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Package pygments scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Scraping package requests (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Proxy list rotated, using 89.218.5.108:50733, next will be 185.162.93.62:9050 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using proxy: {'http': 'http://89.218.5.108:50733'} (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:39 [   DEBUG] https://pypi.org:443 \"GET /pypi/requests/json HTTP/1.1\" 200 34915 (connectionpool.py:452)\n",
      "2023-03-24 19:39:39 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Package requests scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Scraping package charset (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Proxy list rotated, using 185.162.93.62:9050, next will be 36.92.25.98:5678 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using proxy: {'http': 'http://185.162.93.62:9050'} (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.102 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:39 [   DEBUG] https://pypi.org:443 \"GET /pypi/charset/json HTTP/1.1\" 200 1519 (connectionpool.py:452)\n",
      "2023-03-24 19:39:39 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Package charset scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Scraping package idna (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Proxy list rotated, using 36.92.25.98:5678, next will be 50.47.75.216:5678 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using proxy: {'http': 'http://36.92.25.98:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; MAGWJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:39 [   DEBUG] https://pypi.org:443 \"GET /pypi/idna/json HTTP/1.1\" 200 11227 (connectionpool.py:452)\n",
      "2023-03-24 19:39:39 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Package idna scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Scraping package urllib3 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Proxy list rotated, using 50.47.75.216:5678, next will be 186.251.255.49:31337 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using proxy: {'http': 'http://50.47.75.216:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 6_0_1 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A523 Safari/8536.25 (logger.py:122)\n",
      "2023-03-24 19:39:39 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:40 [   DEBUG] https://pypi.org:443 \"GET /pypi/urllib3/json HTTP/1.1\" 200 38618 (connectionpool.py:452)\n",
      "2023-03-24 19:39:40 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Package urllib3 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Scraping package certifi (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Proxy list rotated, using 186.251.255.49:31337, next will be 1.0.0.13:80 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using proxy: {'http': 'http://186.251.255.49:31337'} (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Mobile/12H321 [FBAN/FBIOS;FBAV/38.0.0.6.79;FBBV/14316658;FBDV/iPad4,1;FBMD/iPad;FBSN/iPhone OS;FBSV/8.4.1;FBSS/2; FBCR/;FBID/tablet;FBLC/en_US;FBOP/1] (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:40 [   DEBUG] https://pypi.org:443 \"GET /pypi/certifi/json HTTP/1.1\" 200 16503 (connectionpool.py:452)\n",
      "2023-03-24 19:39:40 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Package certifi scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Scraping package PySocks (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Proxy list rotated, using 1.0.0.13:80, next will be 85.132.8.219:4153 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using proxy: {'http': 'http://1.0.0.13:80'} (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:40 [   DEBUG] https://pypi.org:443 \"GET /pypi/PySocks/json HTTP/1.1\" 200 9520 (connectionpool.py:452)\n",
      "2023-03-24 19:39:40 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Package PySocks scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Scraping package chardet (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Proxy list rotated, using 85.132.8.219:4153, next will be 189.2.127.244:4153 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using proxy: {'http': 'http://85.132.8.219:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:40 [   DEBUG] https://pypi.org:443 \"GET /pypi/chardet/json HTTP/1.1\" 200 5971 (connectionpool.py:452)\n",
      "2023-03-24 19:39:40 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Package chardet scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Scraping package xmlschema (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Proxy list rotated, using 189.2.127.244:4153, next will be 5.135.164.151:30256 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using proxy: {'http': 'http://189.2.127.244:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:40 [   DEBUG] https://pypi.org:443 \"GET /pypi/xmlschema/json HTTP/1.1\" 200 29652 (connectionpool.py:452)\n",
      "2023-03-24 19:39:40 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Package xmlschema scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Scraping package elementpath (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Proxy list rotated, using 5.135.164.151:30256, next will be 103.205.128.33:4303 (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using proxy: {'http': 'http://5.135.164.151:30256'} (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/7.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; InfoPath.3; GWX:QUALIFIED) (logger.py:122)\n",
      "2023-03-24 19:39:40 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:41 [   DEBUG] https://pypi.org:443 \"GET /pypi/elementpath/json HTTP/1.1\" 200 22108 (connectionpool.py:452)\n",
      "2023-03-24 19:39:41 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Package elementpath scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Scraping package jinja2 (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Proxy list rotated, using 103.205.128.33:4303, next will be 152.67.10.190:8100 (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Using proxy: {'http': 'http://103.205.128.33:4303'} (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; Trident/7.0; Touch; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:41 [   DEBUG] https://pypi.org:443 \"GET /pypi/jinja2/json HTTP/1.1\" 200 15703 (connectionpool.py:452)\n",
      "2023-03-24 19:39:41 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Package jinja2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Scraping package lxml (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Proxy list rotated, using 152.67.10.190:8100, next will be 118.97.47.250:55443 (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Using proxy: {'http': 'http://152.67.10.190:8100'} (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:41 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:41 [   DEBUG] https://pypi.org:443 \"GET /pypi/lxml/json HTTP/1.1\" 200 332240 (connectionpool.py:452)\n",
      "2023-03-24 19:39:42 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Package lxml scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Scraping package memory (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Proxy list rotated, using 118.97.47.250:55443, next will be 88.119.49.4:4153 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Using proxy: {'http': 'http://118.97.47.250:55443'} (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.65 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:42 [   DEBUG] https://pypi.org:443 \"GET /pypi/memory/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:39:42 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] ScraperError: Package memory not found (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] The package memory, as dependency of xmlschema does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Scraping package Sphinx (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Proxy list rotated, using 88.119.49.4:4153, next will be 103.66.233.161:4145 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Using proxy: {'http': 'http://88.119.49.4:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Using user agent: Mozilla/5.0 (iPhone; CPU iPhone OS 8_1_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12B466 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:42 [   DEBUG] https://pypi.org:443 \"GET /pypi/Sphinx/json HTTP/1.1\" 200 67935 (connectionpool.py:452)\n",
      "2023-03-24 19:39:42 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Package Sphinx scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Scraping package asv (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Proxy list rotated, using 103.66.233.161:4145, next will be 47.98.151.6:16144 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Using proxy: {'http': 'http://103.66.233.161:4145'} (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.2; GT-N5110 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:42 [   DEBUG] https://pypi.org:443 \"GET /pypi/asv/json HTTP/1.1\" 200 3740 (connectionpool.py:452)\n",
      "2023-03-24 19:39:42 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Package asv scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Scraping package mpmath (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Proxy list rotated, using 47.98.151.6:16144, next will be 201.184.155.20:5678 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Using proxy: {'http': 'http://47.98.151.6:16144'} (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:42 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:43 [   DEBUG] https://pypi.org:443 \"GET /pypi/mpmath/json HTTP/1.1\" 200 12283 (connectionpool.py:452)\n",
      "2023-03-24 19:39:43 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Package mpmath scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Scraping package pycodestyle (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Proxy list rotated, using 201.184.155.20:5678, next will be 203.98.76.64:5678 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Using proxy: {'http': 'http://201.184.155.20:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; yie11; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:43 [   DEBUG] https://pypi.org:443 \"GET /pypi/pycodestyle/json HTTP/1.1\" 200 17250 (connectionpool.py:452)\n",
      "2023-03-24 19:39:43 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Package pycodestyle scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Scraping package codecov (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Proxy list rotated, using 203.98.76.64:5678, next will be 198.59.191.234:8080 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Using proxy: {'http': 'http://203.98.76.64:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36 OPR/31.0.1889.174 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:43 [   DEBUG] https://pypi.org:443 \"GET /pypi/codecov/json HTTP/1.1\" 200 21054 (connectionpool.py:452)\n",
      "2023-03-24 19:39:43 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Package codecov scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Scraping package gmpy2 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Proxy list rotated, using 198.59.191.234:8080, next will be 212.69.12.81:1080 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Using proxy: {'http': 'http://198.59.191.234:8080'} (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.10532 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:43 [   DEBUG] https://pypi.org:443 \"GET /pypi/gmpy2/json HTTP/1.1\" 200 68276 (connectionpool.py:452)\n",
      "2023-03-24 19:39:43 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Package gmpy2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Scraping package threadpoolctl (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Proxy list rotated, using 212.69.12.81:1080, next will be 50.96.204.5:18351 (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Using proxy: {'http': 'http://212.69.12.81:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; Touch; MAARJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:43 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:44 [   DEBUG] https://pypi.org:443 \"GET /pypi/threadpoolctl/json HTTP/1.1\" 200 5983 (connectionpool.py:452)\n",
      "2023-03-24 19:39:44 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Package threadpoolctl scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Scraping package scikit (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Proxy list rotated, using 50.96.204.5:18351, next will be 103.47.93.201:1080 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Using proxy: {'http': 'http://50.96.204.5:18351'} (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:44 [   DEBUG] https://pypi.org:443 \"GET /pypi/scikit/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:39:44 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] ScraperError: Package scikit not found (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] The package scikit, as dependency of scipy does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Scraping package pooch (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Proxy list rotated, using 103.47.93.201:1080, next will be 43.153.81.168:443 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Using proxy: {'http': 'http://103.47.93.201:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.0; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:44 [   DEBUG] https://pypi.org:443 \"GET /pypi/pooch/json HTTP/1.1\" 200 14217 (connectionpool.py:452)\n",
      "2023-03-24 19:39:44 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Package pooch scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Scraping package platformdirs (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Proxy list rotated, using 43.153.81.168:443, next will be 116.111.127.9:4001 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Using proxy: {'http': 'http://43.153.81.168:443'} (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:44 [   DEBUG] https://pypi.org:443 \"GET /pypi/platformdirs/json HTTP/1.1\" 200 9893 (connectionpool.py:452)\n",
      "2023-03-24 19:39:44 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Package platformdirs scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Scraping package typing (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Proxy list rotated, using 116.111.127.9:4001, next will be 114.246.180.148:1080 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Using proxy: {'http': 'http://116.111.127.9:4001'} (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.6.1000 Chrome/30.0.1599.101 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:44 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:45 [   DEBUG] https://pypi.org:443 \"GET /pypi/typing/json HTTP/1.1\" 200 6154 (connectionpool.py:452)\n",
      "2023-03-24 19:39:45 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Package typing scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Scraping package proselint (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Proxy list rotated, using 114.246.180.148:1080, next will be 41.93.71.12:80 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Using proxy: {'http': 'http://114.246.180.148:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:45 [   DEBUG] https://pypi.org:443 \"GET /pypi/proselint/json HTTP/1.1\" 200 12304 (connectionpool.py:452)\n",
      "2023-03-24 19:39:45 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Package proselint scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Scraping package appdirs (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Proxy list rotated, using 41.93.71.12:80, next will be 5.59.163.102:5678 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Using proxy: {'http': 'http://41.93.71.12:80'} (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Using user agent: Mozilla/5.0 (X11; CrOS x86_64 7077.95.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.90 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:45 [   DEBUG] https://pypi.org:443 \"GET /pypi/appdirs/json HTTP/1.1\" 200 5668 (connectionpool.py:452)\n",
      "2023-03-24 19:39:45 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Package appdirs scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Scraping package covdefaults (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Proxy list rotated, using 5.59.163.102:5678, next will be 206.189.200.62:13110 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Using proxy: {'http': 'http://5.59.163.102:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:45 [   DEBUG] https://pypi.org:443 \"GET /pypi/covdefaults/json HTTP/1.1\" 200 5468 (connectionpool.py:452)\n",
      "2023-03-24 19:39:45 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Package covdefaults scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Scraping package tqdm (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Proxy list rotated, using 206.189.200.62:13110, next will be 172.67.158.52:80 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Using proxy: {'http': 'http://206.189.200.62:13110'} (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.116 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:45 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:45 [   DEBUG] https://pypi.org:443 \"GET /pypi/tqdm/json HTTP/1.1\" 200 63581 (connectionpool.py:452)\n",
      "2023-03-24 19:39:46 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Package tqdm scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Scraping package py (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Proxy list rotated, using 172.67.158.52:80, next will be 88.202.230.103:43378 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using proxy: {'http': 'http://172.67.158.52:80'} (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:46 [   DEBUG] https://pypi.org:443 \"GET /pypi/py/json HTTP/1.1\" 200 15677 (connectionpool.py:452)\n",
      "2023-03-24 19:39:46 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Package py scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Scraping package ipywidgets (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Proxy list rotated, using 88.202.230.103:43378, next will be 202.159.35.73:443 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using proxy: {'http': 'http://88.202.230.103:43378'} (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_4 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12H143 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:46 [   DEBUG] https://pypi.org:443 \"GET /pypi/ipywidgets/json HTTP/1.1\" 200 39207 (connectionpool.py:452)\n",
      "2023-03-24 19:39:46 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Package ipywidgets scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Scraping package slack (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Proxy list rotated, using 202.159.35.73:443, next will be 89.219.34.146:10043 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using proxy: {'http': 'http://202.159.35.73:443'} (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:32.0) Gecko/20100101 Firefox/32.0 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:46 [   DEBUG] https://pypi.org:443 \"GET /pypi/slack/json HTTP/1.1\" 200 1396 (connectionpool.py:452)\n",
      "2023-03-24 19:39:46 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Package slack scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Scraping package paramiko (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Proxy list rotated, using 89.219.34.146:10043, next will be 61.28.233.217:3128 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using proxy: {'http': 'http://89.219.34.146:10043'} (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E) (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:46 [   DEBUG] https://pypi.org:443 \"GET /pypi/paramiko/json HTTP/1.1\" 200 168018 (connectionpool.py:452)\n",
      "2023-03-24 19:39:46 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Package paramiko scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Scraping package bcrypt (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Proxy list rotated, using 61.28.233.217:3128, next will be 103.205.179.105:5678 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using proxy: {'http': 'http://61.28.233.217:3128'} (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) GSA/8.0.57838 Mobile/12F69 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:39:46 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:47 [   DEBUG] https://pypi.org:443 \"GET /pypi/bcrypt/json HTTP/1.1\" 200 56129 (connectionpool.py:452)\n",
      "2023-03-24 19:39:47 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Package bcrypt scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Scraping package cryptography (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Proxy list rotated, using 103.205.179.105:5678, next will be 2.137.22.252:4153 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Using proxy: {'http': 'http://103.205.179.105:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.7.1000 Chrome/30.0.1599.101 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:47 [   DEBUG] https://pypi.org:443 \"GET /pypi/cryptography/json HTTP/1.1\" 200 286552 (connectionpool.py:452)\n",
      "2023-03-24 19:39:47 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Package cryptography scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Scraping package pynacl (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Proxy list rotated, using 2.137.22.252:4153, next will be 175.100.87.209:5678 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Using proxy: {'http': 'http://2.137.22.252:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.68 Mobile/12H321 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:47 [   DEBUG] https://pypi.org:443 \"GET /pypi/pynacl/json HTTP/1.1\" 200 31387 (connectionpool.py:452)\n",
      "2023-03-24 19:39:47 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Package pynacl scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Scraping package pyasn1 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Proxy list rotated, using 175.100.87.209:5678, next will be 103.228.246.41:3131 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Using proxy: {'http': 'http://175.100.87.209:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.104 AOL/9.8 AOLBuild/4346.13.US Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:47 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyasn1/json HTTP/1.1\" 200 49993 (connectionpool.py:452)\n",
      "2023-03-24 19:39:47 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Package pyasn1 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Scraping package invoke (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Proxy list rotated, using 103.228.246.41:3131, next will be 54.92.167.233:8118 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Using proxy: {'http': 'http://103.228.246.41:3131'} (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:47 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:48 [   DEBUG] https://pypi.org:443 \"GET /pypi/invoke/json HTTP/1.1\" 200 22091 (connectionpool.py:452)\n",
      "2023-03-24 19:39:48 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Package invoke scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Scraping package gssapi (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Proxy list rotated, using 54.92.167.233:8118, next will be 203.198.207.253:80 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using proxy: {'http': 'http://54.92.167.233:8118'} (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:48 [   DEBUG] https://pypi.org:443 \"GET /pypi/gssapi/json HTTP/1.1\" 200 28912 (connectionpool.py:452)\n",
      "2023-03-24 19:39:48 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Package gssapi scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Scraping package pywin32 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Proxy list rotated, using 203.198.207.253:80, next will be 212.120.186.39:52914 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using proxy: {'http': 'http://203.198.207.253:80'} (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.2; SM-T230NU Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.133 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:48 [   DEBUG] https://pypi.org:443 \"GET /pypi/pywin32/json HTTP/1.1\" 200 24439 (connectionpool.py:452)\n",
      "2023-03-24 19:39:48 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Package pywin32 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Scraping package xxhash (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Proxy list rotated, using 212.120.186.39:52914, next will be 82.98.147.36:80 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using proxy: {'http': 'http://212.120.186.39:52914'} (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.13 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:48 [   DEBUG] https://pypi.org:443 \"GET /pypi/xxhash/json HTTP/1.1\" 200 136663 (connectionpool.py:452)\n",
      "2023-03-24 19:39:48 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Package xxhash scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Scraping package pydata (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Proxy list rotated, using 82.98.147.36:80, next will be 200.105.215.22:33630 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using proxy: {'http': 'http://82.98.147.36:80'} (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:35.0) Gecko/20100101 Firefox/35.0 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:48 [   DEBUG] https://pypi.org:443 \"GET /pypi/pydata/json HTTP/1.1\" 200 1051 (connectionpool.py:452)\n",
      "2023-03-24 19:39:48 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Package pydata scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Scraping package beautifulsoup4 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Proxy list rotated, using 200.105.215.22:33630, next will be 144.217.240.185:9300 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using proxy: {'http': 'http://200.105.215.22:33630'} (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:48 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:49 [   DEBUG] https://pypi.org:443 \"GET /pypi/beautifulsoup4/json HTTP/1.1\" 200 14744 (connectionpool.py:452)\n",
      "2023-03-24 19:39:49 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Package beautifulsoup4 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Scraping package soupsieve (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Proxy list rotated, using 144.217.240.185:9300, next will be 109.87.130.6:5678 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Using proxy: {'http': 'http://144.217.240.185:9300'} (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Using user agent: Mozilla/5.0 (iPhone; CPU iPhone OS 8_4 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.68 Mobile/12H143 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:49 [   DEBUG] https://pypi.org:443 \"GET /pypi/soupsieve/json HTTP/1.1\" 200 16136 (connectionpool.py:452)\n",
      "2023-03-24 19:39:49 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Package soupsieve scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Scraping package html5lib (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Proxy list rotated, using 109.87.130.6:5678, next will be 41.149.135.34:1080 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Using proxy: {'http': 'http://109.87.130.6:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:49 [   DEBUG] https://pypi.org:443 \"GET /pypi/html5lib/json HTTP/1.1\" 200 11820 (connectionpool.py:452)\n",
      "2023-03-24 19:39:49 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Package html5lib scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Scraping package matplotlib (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Proxy list rotated, using 41.149.135.34:1080, next will be 103.121.149.69:8080 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Using proxy: {'http': 'http://41.149.135.34:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_4 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) GSA/8.0.57838 Mobile/12H143 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:49 [   DEBUG] https://pypi.org:443 \"GET /pypi/matplotlib/json HTTP/1.1\" 200 221114 (connectionpool.py:452)\n",
      "2023-03-24 19:39:49 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Package matplotlib scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Scraping package numpydoc (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Proxy list rotated, using 103.121.149.69:8080, next will be 79.174.25.124:3128 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Using proxy: {'http': 'http://103.121.149.69:8080'} (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.120 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:49 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:50 [   DEBUG] https://pypi.org:443 \"GET /pypi/numpydoc/json HTTP/1.1\" 200 7192 (connectionpool.py:452)\n",
      "2023-03-24 19:39:50 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Package numpydoc scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Scraping package Jinja2 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Proxy list rotated, using 79.174.25.124:3128, next will be 45.70.206.33:4145 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using proxy: {'http': 'http://79.174.25.124:3128'} (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; U; Android 4.4.3; en-us; KFSAWA Build/KTU84M) AppleWebKit/537.36 (KHTML, like Gecko) Silk/3.68 like Chrome/39.0.2171.93 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:50 [   DEBUG] https://pypi.org:443 \"GET /pypi/Jinja2/json HTTP/1.1\" 200 15703 (connectionpool.py:452)\n",
      "2023-03-24 19:39:50 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Package Jinja2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Scraping package MarkupSafe (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Proxy list rotated, using 45.70.206.33:4145, next will be 124.41.240.177:52480 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using proxy: {'http': 'http://45.70.206.33:4145'} (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.0.1; SAMSUNG SPH-L720 Build/LRX22C) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/2.1 Chrome/34.0.1847.76 Mobile Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:50 [   DEBUG] https://pypi.org:443 \"GET /pypi/MarkupSafe/json HTTP/1.1\" 200 69733 (connectionpool.py:452)\n",
      "2023-03-24 19:39:50 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Package MarkupSafe scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Scraping package Babel (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Proxy list rotated, using 124.41.240.177:52480, next will be 43.132.184.228:8181 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using proxy: {'http': 'http://124.41.240.177:52480'} (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; Trident/7.0; yie11; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:50 [   DEBUG] https://pypi.org:443 \"GET /pypi/Babel/json HTTP/1.1\" 200 10688 (connectionpool.py:452)\n",
      "2023-03-24 19:39:50 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Package Babel scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Scraping package typing_extensions (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Proxy list rotated, using 43.132.184.228:8181, next will be 158.140.190.211:5678 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using proxy: {'http': 'http://43.132.184.228:8181'} (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.7) Gecko/20150824 Firefox/31.9 PaleMoon/25.7.0 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:50 [   DEBUG] https://pypi.org:443 \"GET /pypi/typing_extensions/json HTTP/1.1\" 200 11352 (connectionpool.py:452)\n",
      "2023-03-24 19:39:50 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Package typing_extensions scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Scraping package rich (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Proxy list rotated, using 158.140.190.211:5678, next will be 203.24.102.74:80 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using proxy: {'http': 'http://158.140.190.211:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.59 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:50 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:50 [   DEBUG] https://pypi.org:443 \"GET /pypi/rich/json HTTP/1.1\" 200 60026 (connectionpool.py:452)\n",
      "2023-03-24 19:39:51 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Package rich scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Scraping package markdown (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Proxy list rotated, using 203.24.102.74:80, next will be 110.76.129.229:5678 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using proxy: {'http': 'http://203.24.102.74:80'} (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:51 [   DEBUG] https://pypi.org:443 \"GET /pypi/markdown/json HTTP/1.1\" 200 18225 (connectionpool.py:452)\n",
      "2023-03-24 19:39:51 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Package markdown scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Scraping package pyyaml (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Proxy list rotated, using 110.76.129.229:5678, next will be 134.119.221.137:30884 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using proxy: {'http': 'http://110.76.129.229:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:51 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyyaml/json HTTP/1.1\" 200 58070 (connectionpool.py:452)\n",
      "2023-03-24 19:39:51 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Package pyyaml scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Scraping package click (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Proxy list rotated, using 134.119.221.137:30884, next will be 197.234.13.67:4145 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using proxy: {'http': 'http://134.119.221.137:30884'} (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:51 [   DEBUG] https://pypi.org:443 \"GET /pypi/click/json HTTP/1.1\" 200 16494 (connectionpool.py:452)\n",
      "2023-03-24 19:39:51 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Package click scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Scraping package doit (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Proxy list rotated, using 197.234.13.67:4145, next will be 79.175.134.30:808 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using proxy: {'http': 'http://197.234.13.67:4145'} (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; TNJB; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:51 [   DEBUG] https://pypi.org:443 \"GET /pypi/doit/json HTTP/1.1\" 200 11311 (connectionpool.py:452)\n",
      "2023-03-24 19:39:51 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Package doit scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Scraping package pydevtool (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Proxy list rotated, using 79.175.134.30:808, next will be 154.236.189.27:8080 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using proxy: {'http': 'http://79.175.134.30:808'} (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.0.1; SAMSUNG SM-N910V 4G Build/LRX22C) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/2.1 Chrome/34.0.1847.76 Mobile Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:51 [   DEBUG] https://pypi.org:443 \"GET /pypi/pydevtool/json HTTP/1.1\" 200 1484 (connectionpool.py:452)\n",
      "2023-03-24 19:39:51 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Package pydevtool scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Scraping package pandas (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Proxy list rotated, using 154.236.189.27:8080, next will be 80.14.219.107:3128 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using proxy: {'http': 'http://154.236.189.27:8080'} (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:51 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:52 [   DEBUG] https://pypi.org:443 \"GET /pypi/pandas/json HTTP/1.1\" 200 232995 (connectionpool.py:452)\n",
      "2023-03-24 19:39:52 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Package pandas scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Scraping package python (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Proxy list rotated, using 80.14.219.107:3128, next will be 50.236.148.254:31699 (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Using proxy: {'http': 'http://80.14.219.107:3128'} (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/600.8.9 (KHTML, like Gecko) Version/8.0.8 Safari/600.8.9 (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:52 [   DEBUG] https://pypi.org:443 \"GET /pypi/python/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:39:52 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] ScraperError: Package python not found (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] The package python, as dependency of pandas does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Scraping package pytz (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Proxy list rotated, using 50.236.148.254:31699, next will be 117.241.133.31:5678 (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Using proxy: {'http': 'http://50.236.148.254:31699'} (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.2; SM-T310 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:52 [   DEBUG] https://pypi.org:443 \"GET /pypi/pytz/json HTTP/1.1\" 200 112672 (connectionpool.py:452)\n",
      "2023-03-24 19:39:52 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Package pytz scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:52 [   DEBUG] Scraping package pillow (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Proxy list rotated, using 117.241.133.31:5678, next will be 20.24.43.214:80 (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Using proxy: {'http': 'http://117.241.133.31:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.107 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:53 [   DEBUG] https://pypi.org:443 \"GET /pypi/pillow/json HTTP/1.1\" 200 390255 (connectionpool.py:452)\n",
      "2023-03-24 19:39:53 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Package pillow scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Scraping package olefile (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Proxy list rotated, using 20.24.43.214:80, next will be 95.213.154.54:31337 (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Using proxy: {'http': 'http://20.24.43.214:80'} (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Win64; x64; Trident/4.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E) (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:53 [   DEBUG] https://pypi.org:443 \"GET /pypi/olefile/json HTTP/1.1\" 200 6157 (connectionpool.py:452)\n",
      "2023-03-24 19:39:53 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Package olefile scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Scraping package sphinxext (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Proxy list rotated, using 95.213.154.54:31337, next will be 124.156.87.32:8000 (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Using proxy: {'http': 'http://95.213.154.54:31337'} (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Using user agent: Mozilla/5.0 AppleWebKit/999.0 (KHTML, like Gecko) Chrome/99.0 Safari/999.0 (logger.py:122)\n",
      "2023-03-24 19:39:53 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:54 [   DEBUG] https://pypi.org:443 \"GET /pypi/sphinxext/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:39:54 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] ScraperError: Package sphinxext not found (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] The package sphinxext, as dependency of pillow does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Scraping package check (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Proxy list rotated, using 124.156.87.32:8000, next will be 203.30.190.57:80 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using proxy: {'http': 'http://124.156.87.32:8000'} (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:54 [   DEBUG] https://pypi.org:443 \"GET /pypi/check/json HTTP/1.1\" 200 1584 (connectionpool.py:452)\n",
      "2023-03-24 19:39:54 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Package check scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Scraping package defusedxml (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Proxy list rotated, using 203.30.190.57:80, next will be 115.242.252.174:4153 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using proxy: {'http': 'http://203.30.190.57:80'} (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:54 [   DEBUG] https://pypi.org:443 \"GET /pypi/defusedxml/json HTTP/1.1\" 200 15699 (connectionpool.py:452)\n",
      "2023-03-24 19:39:54 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Package defusedxml scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Scraping package markdown2 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Proxy list rotated, using 115.242.252.174:4153, next will be 171.97.107.130:4145 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using proxy: {'http': 'http://115.242.252.174:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.0.9895 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:54 [   DEBUG] https://pypi.org:443 \"GET /pypi/markdown2/json HTTP/1.1\" 200 10813 (connectionpool.py:452)\n",
      "2023-03-24 19:39:54 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Package markdown2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Scraping package wavedrom (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Proxy list rotated, using 171.97.107.130:4145, next will be 74.205.128.200:80 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using proxy: {'http': 'http://171.97.107.130:4145'} (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.132 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:54 [   DEBUG] https://pypi.org:443 \"GET /pypi/wavedrom/json HTTP/1.1\" 200 5096 (connectionpool.py:452)\n",
      "2023-03-24 19:39:54 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Package wavedrom scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Scraping package pyroma (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Proxy list rotated, using 74.205.128.200:80, next will be 103.47.93.197:1080 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using proxy: {'http': 'http://74.205.128.200:80'} (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Using user agent: Mozilla/5.0 (iPad;U;CPU OS 5_1_1 like Mac OS X; zh-cn)AppleWebKit/534.46.0(KHTML, like Gecko)CriOS/19.0.1084.60 Mobile/9B206 Safari/7534.48.3 (logger.py:122)\n",
      "2023-03-24 19:39:54 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:54 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyroma/json HTTP/1.1\" 200 15908 (connectionpool.py:452)\n",
      "2023-03-24 19:39:55 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Package pyroma scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Scraping package build (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Proxy list rotated, using 103.47.93.197:1080, next will be 115.76.248.226:8080 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using proxy: {'http': 'http://103.47.93.197:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:55 [   DEBUG] https://pypi.org:443 \"GET /pypi/build/json HTTP/1.1\" 200 8061 (connectionpool.py:452)\n",
      "2023-03-24 19:39:55 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Package build scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Scraping package pyproject_hooks (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Proxy list rotated, using 115.76.248.226:8080, next will be 104.25.244.70:80 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using proxy: {'http': 'http://115.76.248.226:8080'} (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; U; Android 4.4.3; en-us; KFARWI Build/KTU84M) AppleWebKit/537.36 (KHTML, like Gecko) Silk/3.68 like Chrome/39.0.2171.93 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:55 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyproject_hooks/json HTTP/1.1\" 200 1761 (connectionpool.py:452)\n",
      "2023-03-24 19:39:55 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Package pyproject_hooks scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Scraping package filelock (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Proxy list rotated, using 104.25.244.70:80, next will be 168.227.158.17:4145 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using proxy: {'http': 'http://104.25.244.70:80'} (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.7.1000 Chrome/30.0.1599.101 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:55 [   DEBUG] https://pypi.org:443 \"GET /pypi/filelock/json HTTP/1.1\" 200 13640 (connectionpool.py:452)\n",
      "2023-03-24 19:39:55 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Package filelock scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Scraping package toml (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Proxy list rotated, using 168.227.158.17:4145, next will be 104.255.170.65:53623 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using proxy: {'http': 'http://168.227.158.17:4145'} (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:55 [   DEBUG] https://pypi.org:443 \"GET /pypi/toml/json HTTP/1.1\" 200 6177 (connectionpool.py:452)\n",
      "2023-03-24 19:39:55 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Package toml scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Scraping package setuptools (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Proxy list rotated, using 104.255.170.65:53623, next will be 157.245.97.60:80 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using proxy: {'http': 'http://104.255.170.65:53623'} (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:55 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:55 [   DEBUG] https://pypi.org:443 \"GET /pypi/setuptools/json HTTP/1.1\" 200 199154 (connectionpool.py:452)\n",
      "2023-03-24 19:39:56 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Package setuptools scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Scraping package virtualenv (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Proxy list rotated, using 157.245.97.60:80, next will be 192.248.125.3:80 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using proxy: {'http': 'http://157.245.97.60:80'} (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:56 [   DEBUG] https://pypi.org:443 \"GET /pypi/virtualenv/json HTTP/1.1\" 200 58937 (connectionpool.py:452)\n",
      "2023-03-24 19:39:56 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Package virtualenv scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Scraping package docutils (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Proxy list rotated, using 192.248.125.3:80, next will be 62.33.235.14:4153 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using proxy: {'http': 'http://192.248.125.3:80'} (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; yie10; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:56 [   DEBUG] https://pypi.org:443 \"GET /pypi/docutils/json HTTP/1.1\" 200 10420 (connectionpool.py:452)\n",
      "2023-03-24 19:39:56 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Package docutils scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Scraping package trove (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Proxy list rotated, using 62.33.235.14:4153, next will be 150.158.135.201:24289 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using proxy: {'http': 'http://62.33.235.14:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; Trident/7.0; BOIE9;ENUS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:56 [   DEBUG] https://pypi.org:443 \"GET /pypi/trove/json HTTP/1.1\" 200 10258 (connectionpool.py:452)\n",
      "2023-03-24 19:39:56 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Package trove scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Scraping package Paste (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Proxy list rotated, using 150.158.135.201:24289, next will be 128.199.96.222:27088 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using proxy: {'http': 'http://150.158.135.201:24289'} (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using user agent: Mozilla/5.0 (iPhone; CPU iPhone OS 8_4 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.68 Mobile/12H143 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:56 [   DEBUG] https://pypi.org:443 \"GET /pypi/Paste/json HTTP/1.1\" 200 24349 (connectionpool.py:452)\n",
      "2023-03-24 19:39:56 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Package Paste scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Scraping package PasteDeploy (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Proxy list rotated, using 128.199.96.222:27088, next will be 168.195.50.225:4153 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using proxy: {'http': 'http://128.199.96.222:27088'} (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; MASAJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:56 [   DEBUG] https://pypi.org:443 \"GET /pypi/PasteDeploy/json HTTP/1.1\" 200 7898 (connectionpool.py:452)\n",
      "2023-03-24 19:39:56 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Package PasteDeploy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Scraping package PyMySQL (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Proxy list rotated, using 168.195.50.225:4153, next will be 104.20.125.124:80 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using proxy: {'http': 'http://168.195.50.225:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; GIL 3.5; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:56 [   DEBUG] https://pypi.org:443 \"GET /pypi/PyMySQL/json HTTP/1.1\" 200 13849 (connectionpool.py:452)\n",
      "2023-03-24 19:39:56 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Package PyMySQL scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Scraping package Routes (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Proxy list rotated, using 104.20.125.124:80, next will be 103.233.152.181:1080 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using proxy: {'http': 'http://104.20.125.124:80'} (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:56 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:57 [   DEBUG] https://pypi.org:443 \"GET /pypi/Routes/json HTTP/1.1\" 200 19689 (connectionpool.py:452)\n",
      "2023-03-24 19:39:57 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Package Routes scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Scraping package SQLAlchemy (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Proxy list rotated, using 103.233.152.181:1080, next will be 213.171.214.19:8001 (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Using proxy: {'http': 'http://103.233.152.181:1080'} (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Using user agent: Mozilla/5.0 (X11; CrOS x86_64 6680.78.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.102 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:57 [   DEBUG] https://pypi.org:443 \"GET /pypi/SQLAlchemy/json HTTP/1.1\" 200 416798 (connectionpool.py:452)\n",
      "2023-03-24 19:39:57 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Package SQLAlchemy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Scraping package WebOb (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Proxy list rotated, using 213.171.214.19:8001, next will be 149.20.253.66:12551 (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Using proxy: {'http': 'http://213.171.214.19:8001'} (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 AOL/9.7 AOLBuild/4343.4049.US Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:57 [   DEBUG] https://pypi.org:443 \"GET /pypi/WebOb/json HTTP/1.1\" 200 21922 (connectionpool.py:452)\n",
      "2023-03-24 19:39:57 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Package WebOb scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Scraping package diskimage (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Proxy list rotated, using 149.20.253.66:12551, next will be 43.129.223.147:38080 (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Using proxy: {'http': 'http://149.20.253.66:12551'} (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; TNJB; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:57 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:58 [   DEBUG] https://pypi.org:443 \"GET /pypi/diskimage/json HTTP/1.1\" 200 2782 (connectionpool.py:452)\n",
      "2023-03-24 19:39:58 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Package diskimage scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Scraping package docker (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Proxy list rotated, using 43.129.223.147:38080, next will be 152.67.222.114:8001 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Using proxy: {'http': 'http://43.129.223.147:38080'} (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:58 [   DEBUG] https://pypi.org:443 \"GET /pypi/docker/json HTTP/1.1\" 200 18792 (connectionpool.py:452)\n",
      "2023-03-24 19:39:58 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Package docker scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Scraping package eventlet (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Proxy list rotated, using 152.67.222.114:8001, next will be 213.191.194.24:80 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Using proxy: {'http': 'http://152.67.222.114:8001'} (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; MAGWJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:58 [   DEBUG] https://pypi.org:443 \"GET /pypi/eventlet/json HTTP/1.1\" 200 15127 (connectionpool.py:452)\n",
      "2023-03-24 19:39:58 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Package eventlet scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Scraping package httplib2 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Proxy list rotated, using 213.191.194.24:80, next will be 146.56.136.237:9090 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Using proxy: {'http': 'http://213.191.194.24:80'} (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:58 [   DEBUG] https://pypi.org:443 \"GET /pypi/httplib2/json HTTP/1.1\" 200 11642 (connectionpool.py:452)\n",
      "2023-03-24 19:39:58 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Package httplib2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Scraping package iso8601 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Proxy list rotated, using 146.56.136.237:9090, next will be 103.79.96.213:4153 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Using proxy: {'http': 'http://146.56.136.237:9090'} (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; U; Android 4.4.3; en-us; KFSAWA Build/KTU84M) AppleWebKit/537.36 (KHTML, like Gecko) Silk/3.68 like Chrome/39.0.2171.93 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:58 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:59 [   DEBUG] https://pypi.org:443 \"GET /pypi/iso8601/json HTTP/1.1\" 200 11944 (connectionpool.py:452)\n",
      "2023-03-24 19:39:59 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Package iso8601 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Scraping package jsonschema (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Proxy list rotated, using 103.79.96.213:4153, next will be 190.217.58.90:5678 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Using proxy: {'http': 'http://103.79.96.213:4153'} (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:59 [   DEBUG] https://pypi.org:443 \"GET /pypi/jsonschema/json HTTP/1.1\" 200 27751 (connectionpool.py:452)\n",
      "2023-03-24 19:39:59 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Package jsonschema scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Scraping package keystonemiddleware (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Proxy list rotated, using 190.217.58.90:5678, next will be 41.221.158.186:31932 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Using proxy: {'http': 'http://190.217.58.90:5678'} (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.1.1; SAMSUNG SM-G920T Build/LMY47X) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/3.2 Chrome/38.0.2125.102 Mobile Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:59 [   DEBUG] https://pypi.org:443 \"GET /pypi/keystonemiddleware/json HTTP/1.1\" 200 24505 (connectionpool.py:452)\n",
      "2023-03-24 19:39:59 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Package keystonemiddleware scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Scraping package netaddr (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Proxy list rotated, using 41.221.158.186:31932, next will be 121.66.198.76:4145 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Using proxy: {'http': 'http://41.221.158.186:31932'} (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Using user agent: Mozilla/5.0 (Windows Phone 8.1; ARM; Trident/7.0; Touch; rv:11.0; IEMobile/11.0; NOKIA; Lumia 635) like Gecko (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:39:59 [   DEBUG] https://pypi.org:443 \"GET /pypi/netaddr/json HTTP/1.1\" 200 18123 (connectionpool.py:452)\n",
      "2023-03-24 19:39:59 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Package netaddr scraped successfully (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Scraping package oslo (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Proxy list rotated, using 121.66.198.76:4145, next will be 131.0.140.2:4145 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Using proxy: {'http': 'http://121.66.198.76:4145'} (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:38.0) Gecko/20100101 Firefox/38.0 (logger.py:122)\n",
      "2023-03-24 19:39:59 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:00 [   DEBUG] https://pypi.org:443 \"GET /pypi/oslo/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:00 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] ScraperError: Package oslo not found (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] The package oslo, as dependency of trove does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Scraping package osprofiler (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Proxy list rotated, using 131.0.140.2:4145, next will be 167.99.0.66:5262 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Using proxy: {'http': 'http://131.0.140.2:4145'} (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_1_2 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.68 Mobile/12B440 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:00 [   DEBUG] https://pypi.org:443 \"GET /pypi/osprofiler/json HTTP/1.1\" 200 17173 (connectionpool.py:452)\n",
      "2023-03-24 19:40:00 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Package osprofiler scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Scraping package passlib (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Proxy list rotated, using 167.99.0.66:5262, next will be 95.183.140.94:80 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Using proxy: {'http': 'http://167.99.0.66:5262'} (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:00 [   DEBUG] https://pypi.org:443 \"GET /pypi/passlib/json HTTP/1.1\" 200 9381 (connectionpool.py:452)\n",
      "2023-03-24 19:40:00 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Package passlib scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Scraping package pbr (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Proxy list rotated, using 95.183.140.94:80, next will be 84.237.248.137:4153 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Using proxy: {'http': 'http://95.183.140.94:80'} (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:00 [   DEBUG] https://pypi.org:443 \"GET /pypi/pbr/json HTTP/1.1\" 200 25167 (connectionpool.py:452)\n",
      "2023-03-24 19:40:00 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Package pbr scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Scraping package psycopg2 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Proxy list rotated, using 84.237.248.137:4153, next will be 178.212.54.137:5678 (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Using proxy: {'http': 'http://84.237.248.137:4153'} (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Using user agent: Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; WOW64; Trident/7.0) (logger.py:122)\n",
      "2023-03-24 19:40:00 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:01 [   DEBUG] https://pypi.org:443 \"GET /pypi/psycopg2/json HTTP/1.1\" 200 79871 (connectionpool.py:452)\n",
      "2023-03-24 19:40:01 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Package psycopg2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Scraping package python (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Proxy list rotated, using 178.212.54.137:5678, next will be 202.162.197.155:5678 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using proxy: {'http': 'http://178.212.54.137:5678'} (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.78.2 (KHTML, like Gecko) Version/7.0.6 Safari/537.78.2 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:01 [   DEBUG] https://pypi.org:443 \"GET /pypi/python/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:01 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] ScraperError: Package python not found (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] The package python, as dependency of trove does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Scraping package semantic (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Proxy list rotated, using 202.162.197.155:5678, next will be 16.163.217.247:8888 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using proxy: {'http': 'http://202.162.197.155:5678'} (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) GSA/8.0.57838 Mobile/11D257 Safari/9537.53 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:01 [   DEBUG] https://pypi.org:443 \"GET /pypi/semantic/json HTTP/1.1\" 200 2610 (connectionpool.py:452)\n",
      "2023-03-24 19:40:01 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Package semantic scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Scraping package sqlalchemy (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Proxy list rotated, using 16.163.217.247:8888, next will be 79.79.18.76:5678 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using proxy: {'http': 'http://16.163.217.247:8888'} (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; U; Android 4.0.3; en-us) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.59 Mobile Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:01 [   DEBUG] https://pypi.org:443 \"GET /pypi/sqlalchemy/json HTTP/1.1\" 200 416798 (connectionpool.py:452)\n",
      "2023-03-24 19:40:01 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Package sqlalchemy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Scraping package stevedore (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Proxy list rotated, using 79.79.18.76:5678, next will be 5.9.167.130:30000 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using proxy: {'http': 'http://79.79.18.76:5678'} (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.65 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:01 [   DEBUG] https://pypi.org:443 \"GET /pypi/stevedore/json HTTP/1.1\" 200 23479 (connectionpool.py:452)\n",
      "2023-03-24 19:40:01 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Package stevedore scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Scraping package xmltodict (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Proxy list rotated, using 5.9.167.130:30000, next will be 132.145.129.174:6969 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using proxy: {'http': 'http://5.9.167.130:30000'} (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:01 [   DEBUG] https://pypi.org:443 \"GET /pypi/xmltodict/json HTTP/1.1\" 200 8289 (connectionpool.py:452)\n",
      "2023-03-24 19:40:01 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Package xmltodict scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Scraping package zest (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Proxy list rotated, using 132.145.129.174:6969, next will be 15.236.135.81:80 (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using proxy: {'http': 'http://132.145.129.174:6969'} (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; InfoPath.3) (logger.py:122)\n",
      "2023-03-24 19:40:01 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:02 [   DEBUG] https://pypi.org:443 \"GET /pypi/zest/json HTTP/1.1\" 200 2570 (connectionpool.py:452)\n",
      "2023-03-24 19:40:02 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Package zest scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Scraping package nb2plots (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Proxy list rotated, using 15.236.135.81:80, next will be 103.56.205.89:32650 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using proxy: {'http': 'http://15.236.135.81:80'} (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:02 [   DEBUG] https://pypi.org:443 \"GET /pypi/nb2plots/json HTTP/1.1\" 200 5213 (connectionpool.py:452)\n",
      "2023-03-24 19:40:02 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Package nb2plots scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Scraping package ipython (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Proxy list rotated, using 103.56.205.89:32650, next will be 95.183.140.89:80 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using proxy: {'http': 'http://103.56.205.89:32650'} (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:02 [   DEBUG] https://pypi.org:443 \"GET /pypi/ipython/json HTTP/1.1\" 200 55754 (connectionpool.py:452)\n",
      "2023-03-24 19:40:02 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Package ipython scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Scraping package backcall (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Proxy list rotated, using 95.183.140.89:80, next will be 45.77.153.183:10284 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using proxy: {'http': 'http://95.183.140.89:80'} (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:02 [   DEBUG] https://pypi.org:443 \"GET /pypi/backcall/json HTTP/1.1\" 200 2013 (connectionpool.py:452)\n",
      "2023-03-24 19:40:02 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Package backcall scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Scraping package decorator (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Proxy list rotated, using 45.77.153.183:10284, next will be 45.122.44.17:5678 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using proxy: {'http': 'http://45.77.153.183:10284'} (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; InfoPath.3) (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:02 [   DEBUG] https://pypi.org:443 \"GET /pypi/decorator/json HTTP/1.1\" 200 11883 (connectionpool.py:452)\n",
      "2023-03-24 19:40:02 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Package decorator scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Scraping package jedi (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Proxy list rotated, using 45.122.44.17:5678, next will be 103.47.93.247:1080 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using proxy: {'http': 'http://45.122.44.17:5678'} (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using user agent: Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.3; WOW64; Trident/7.0) (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:02 [   DEBUG] https://pypi.org:443 \"GET /pypi/jedi/json HTTP/1.1\" 200 17490 (connectionpool.py:452)\n",
      "2023-03-24 19:40:02 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Package jedi scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Scraping package parso (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Proxy list rotated, using 103.47.93.247:1080, next will be 119.15.90.78:5678 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using proxy: {'http': 'http://103.47.93.247:1080'} (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_4 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) GSA/8.0.57838 Mobile/12H143 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:02 [   DEBUG] https://pypi.org:443 \"GET /pypi/parso/json HTTP/1.1\" 200 11559 (connectionpool.py:452)\n",
      "2023-03-24 19:40:02 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Package parso scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Scraping package Pygments (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Proxy list rotated, using 119.15.90.78:5678, next will be 181.209.96.229:4153 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using proxy: {'http': 'http://119.15.90.78:5678'} (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/34.0.1847.116 Chrome/34.0.1847.116 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:02 [   DEBUG] https://pypi.org:443 \"GET /pypi/Pygments/json HTTP/1.1\" 200 25111 (connectionpool.py:452)\n",
      "2023-03-24 19:40:02 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Package Pygments scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Scraping package alabaster (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Proxy list rotated, using 181.209.96.229:4153, next will be 190.120.252.113:4145 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using proxy: {'http': 'http://181.209.96.229:4153'} (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_1_2 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.68 Mobile/12B440 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:40:02 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:03 [   DEBUG] https://pypi.org:443 \"GET /pypi/alabaster/json HTTP/1.1\" 200 6960 (connectionpool.py:452)\n",
      "2023-03-24 19:40:03 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Package alabaster scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Scraping package babel (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Proxy list rotated, using 190.120.252.113:4145, next will be 188.166.56.246:80 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using proxy: {'http': 'http://190.120.252.113:4145'} (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.13 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:03 [   DEBUG] https://pypi.org:443 \"GET /pypi/babel/json HTTP/1.1\" 200 10688 (connectionpool.py:452)\n",
      "2023-03-24 19:40:03 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Package babel scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Scraping package commonmark (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Proxy list rotated, using 188.166.56.246:80, next will be 13.81.217.201:80 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using proxy: {'http': 'http://188.166.56.246:80'} (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.0.2; SM-T530NU Build/LRX22G) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:03 [   DEBUG] https://pypi.org:443 \"GET /pypi/commonmark/json HTTP/1.1\" 200 8606 (connectionpool.py:452)\n",
      "2023-03-24 19:40:03 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Package commonmark scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Scraping package future (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Proxy list rotated, using 13.81.217.201:80, next will be 24.234.142.122:31008 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using proxy: {'http': 'http://13.81.217.201:80'} (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET4.0C; .NET4.0E; MS-RTC LM 8; InfoPath.3) (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:03 [   DEBUG] https://pypi.org:443 \"GET /pypi/future/json HTTP/1.1\" 200 10347 (connectionpool.py:452)\n",
      "2023-03-24 19:40:03 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Package future scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Scraping package imagesize (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Proxy list rotated, using 24.234.142.122:31008, next will be 123.59.100.245:1080 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using proxy: {'http': 'http://24.234.142.122:31008'} (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.89 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:03 [   DEBUG] https://pypi.org:443 \"GET /pypi/imagesize/json HTTP/1.1\" 200 4235 (connectionpool.py:452)\n",
      "2023-03-24 19:40:03 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Package imagesize scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Scraping package pyparsing (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Proxy list rotated, using 123.59.100.245:1080, next will be 40.119.236.22:80 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using proxy: {'http': 'http://123.59.100.245:1080'} (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.65 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:03 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyparsing/json HTTP/1.1\" 200 42073 (connectionpool.py:452)\n",
      "2023-03-24 19:40:03 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Package pyparsing scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Scraping package readthedocs (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Proxy list rotated, using 40.119.236.22:80, next will be 104.20.198.49:80 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using proxy: {'http': 'http://40.119.236.22:80'} (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:03 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:04 [   DEBUG] https://pypi.org:443 \"GET /pypi/readthedocs/json HTTP/1.1\" 200 544 (connectionpool.py:452)\n",
      "2023-03-24 19:40:04 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Package readthedocs scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Scraping package recommonmark (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Proxy list rotated, using 104.20.198.49:80, next will be 170.79.182.82:11337 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using proxy: {'http': 'http://104.20.198.49:80'} (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_1_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) GSA/7.0.55539 Mobile/12B466 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:04 [   DEBUG] https://pypi.org:443 \"GET /pypi/recommonmark/json HTTP/1.1\" 200 4067 (connectionpool.py:452)\n",
      "2023-03-24 19:40:04 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Package recommonmark scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Scraping package six (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Proxy list rotated, using 170.79.182.82:11337, next will be 85.237.62.189:3629 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using proxy: {'http': 'http://170.79.182.82:11337'} (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:04 [   DEBUG] https://pypi.org:443 \"GET /pypi/six/json HTTP/1.1\" 200 8375 (connectionpool.py:452)\n",
      "2023-03-24 19:40:04 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Package six scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Scraping package snowballstemmer (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Proxy list rotated, using 85.237.62.189:3629, next will be 103.152.9.142:80 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using proxy: {'http': 'http://85.237.62.189:3629'} (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:04 [   DEBUG] https://pypi.org:443 \"GET /pypi/snowballstemmer/json HTTP/1.1\" 200 5036 (connectionpool.py:452)\n",
      "2023-03-24 19:40:04 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Package snowballstemmer scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Scraping package sphinxcontrib (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Proxy list rotated, using 103.152.9.142:80, next will be 37.131.164.48:59341 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using proxy: {'http': 'http://103.152.9.142:80'} (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; Touch; MDDCJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:04 [   DEBUG] https://pypi.org:443 \"GET /pypi/sphinxcontrib/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:04 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] ScraperError: Package sphinxcontrib not found (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] The package sphinxcontrib, as dependency of jedi does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Scraping package Django (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Proxy list rotated, using 37.131.164.48:59341, next will be 103.69.108.78:8191 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using proxy: {'http': 'http://37.131.164.48:59341'} (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/600.4.10 (KHTML, like Gecko) Version/8.0.4 Safari/600.4.10 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:04 [   DEBUG] https://pypi.org:443 \"GET /pypi/Django/json HTTP/1.1\" 200 86281 (connectionpool.py:452)\n",
      "2023-03-24 19:40:04 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Package Django scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Scraping package docopt (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Proxy list rotated, using 103.69.108.78:8191, next will be 103.81.12.1:44832 (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using proxy: {'http': 'http://103.69.108.78:8191'} (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; BOIE9;ENUS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:04 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:05 [   DEBUG] https://pypi.org:443 \"GET /pypi/docopt/json HTTP/1.1\" 200 8857 (connectionpool.py:452)\n",
      "2023-03-24 19:40:05 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Package docopt scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Scraping package pickleshare (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Proxy list rotated, using 103.81.12.1:44832, next will be 172.67.231.3:80 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using proxy: {'http': 'http://103.81.12.1:44832'} (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.0; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:05 [   DEBUG] https://pypi.org:443 \"GET /pypi/pickleshare/json HTTP/1.1\" 200 4546 (connectionpool.py:452)\n",
      "2023-03-24 19:40:05 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Package pickleshare scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Scraping package pathlib2 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Proxy list rotated, using 172.67.231.3:80, next will be 103.51.44.9:4145 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using proxy: {'http': 'http://172.67.231.3:80'} (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:05 [   DEBUG] https://pypi.org:443 \"GET /pypi/pathlib2/json HTTP/1.1\" 200 6228 (connectionpool.py:452)\n",
      "2023-03-24 19:40:05 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Package pathlib2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Scraping package prompt (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Proxy list rotated, using 103.51.44.9:4145, next will be 75.119.204.206:30795 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using proxy: {'http': 'http://103.51.44.9:4145'} (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:38.0) Gecko/20100101 Firefox/38.0 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:05 [   DEBUG] https://pypi.org:443 \"GET /pypi/prompt/json HTTP/1.1\" 200 4489 (connectionpool.py:452)\n",
      "2023-03-24 19:40:05 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Package prompt scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Scraping package stack (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Proxy list rotated, using 75.119.204.206:30795, next will be 58.84.56.19:4153 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using proxy: {'http': 'http://75.119.204.206:30795'} (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:05 [   DEBUG] https://pypi.org:443 \"GET /pypi/stack/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:05 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] ScraperError: Package stack not found (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] The package stack, as dependency of ipython does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Scraping package traitlets (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Proxy list rotated, using 58.84.56.19:4153, next will be 139.59.186.196:7497 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using proxy: {'http': 'http://58.84.56.19:4153'} (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:05 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:06 [   DEBUG] https://pypi.org:443 \"GET /pypi/traitlets/json HTTP/1.1\" 200 17998 (connectionpool.py:452)\n",
      "2023-03-24 19:40:06 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Package traitlets scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Scraping package appnope (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Proxy list rotated, using 139.59.186.196:7497, next will be 46.227.36.152:1080 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using proxy: {'http': 'http://139.59.186.196:7497'} (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; InfoPath.2) (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:06 [   DEBUG] https://pypi.org:443 \"GET /pypi/appnope/json HTTP/1.1\" 200 4243 (connectionpool.py:452)\n",
      "2023-03-24 19:40:06 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Package appnope scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Scraping package black (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Proxy list rotated, using 46.227.36.152:1080, next will be 64.20.51.62:8000 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using proxy: {'http': 'http://46.227.36.152:1080'} (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:06 [   DEBUG] https://pypi.org:443 \"GET /pypi/black/json HTTP/1.1\" 200 55841 (connectionpool.py:452)\n",
      "2023-03-24 19:40:06 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Package black scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Scraping package pathspec (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Proxy list rotated, using 64.20.51.62:8000, next will be 81.200.156.124:3128 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using proxy: {'http': 'http://64.20.51.62:8000'} (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Mobile/12H321 [Pinterest/iOS] (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:06 [   DEBUG] https://pypi.org:443 \"GET /pypi/pathspec/json HTTP/1.1\" 200 12248 (connectionpool.py:452)\n",
      "2023-03-24 19:40:06 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Package pathspec scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Scraping package typed (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Proxy list rotated, using 81.200.156.124:3128, next will be 47.206.214.4:54321 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using proxy: {'http': 'http://81.200.156.124:3128'} (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; Touch; MALCJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:06 [   DEBUG] https://pypi.org:443 \"GET /pypi/typed/json HTTP/1.1\" 200 1252 (connectionpool.py:452)\n",
      "2023-03-24 19:40:06 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Package typed scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Scraping package aiohttp (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Proxy list rotated, using 47.206.214.4:54321, next will be 50.16.22.43:80 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using proxy: {'http': 'http://47.206.214.4:54321'} (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; rv:34.0) Gecko/20100101 Firefox/34.0 (logger.py:122)\n",
      "2023-03-24 19:40:06 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:07 [   DEBUG] https://pypi.org:443 \"GET /pypi/aiohttp/json HTTP/1.1\" 200 402186 (connectionpool.py:452)\n",
      "2023-03-24 19:40:07 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:07 [   DEBUG] Package aiohttp scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:07 [   DEBUG] Scraping package tokenize (logger.py:122)\n",
      "2023-03-24 19:40:07 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:07 [   DEBUG] Proxy list rotated, using 50.16.22.43:80, next will be 186.251.252.9:31337 (logger.py:122)\n",
      "2023-03-24 19:40:07 [   DEBUG] Using proxy: {'http': 'http://50.16.22.43:80'} (logger.py:122)\n",
      "2023-03-24 19:40:07 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:07 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.0.2; SAMSUNG-SM-G920A Build/LRX22G) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/3.0 Chrome/38.0.2125.102 Mobile Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:07 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:08 [   DEBUG] https://pypi.org:443 \"GET /pypi/tokenize/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:08 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] ScraperError: Package tokenize not found (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] The package tokenize, as dependency of black does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Scraping package uvloop (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Proxy list rotated, using 186.251.252.9:31337, next will be 150.107.207.137:57230 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using proxy: {'http': 'http://186.251.252.9:31337'} (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using user agent: Mozilla/5.0 (X11; CrOS x86_64 7262.52.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.86 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:08 [   DEBUG] https://pypi.org:443 \"GET /pypi/uvloop/json HTTP/1.1\" 200 61030 (connectionpool.py:452)\n",
      "2023-03-24 19:40:08 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Package uvloop scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Scraping package ipykernel (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Proxy list rotated, using 150.107.207.137:57230, next will be 218.1.200.97:57114 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using proxy: {'http': 'http://150.107.207.137:57230'} (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; Touch; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:08 [   DEBUG] https://pypi.org:443 \"GET /pypi/ipykernel/json HTTP/1.1\" 200 39268 (connectionpool.py:452)\n",
      "2023-03-24 19:40:08 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Package ipykernel scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Scraping package comm (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Proxy list rotated, using 218.1.200.97:57114, next will be 103.124.95.254:55338 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using proxy: {'http': 'http://218.1.200.97:57114'} (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; Touch; ASU2JS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:08 [   DEBUG] https://pypi.org:443 \"GET /pypi/comm/json HTTP/1.1\" 200 3814 (connectionpool.py:452)\n",
      "2023-03-24 19:40:08 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Package comm scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Scraping package debugpy (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Proxy list rotated, using 103.124.95.254:55338, next will be 82.137.245.41:1080 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using proxy: {'http': 'http://103.124.95.254:55338'} (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 SE 2.X MetaSr 1.0 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:08 [   DEBUG] https://pypi.org:443 \"GET /pypi/debugpy/json HTTP/1.1\" 200 155148 (connectionpool.py:452)\n",
      "2023-03-24 19:40:08 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Package debugpy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Scraping package jupyter (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Proxy list rotated, using 82.137.245.41:1080, next will be 43.153.177.137:443 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using proxy: {'http': 'http://82.137.245.41:1080'} (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.13 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:08 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:09 [   DEBUG] https://pypi.org:443 \"GET /pypi/jupyter/json HTTP/1.1\" 200 1289 (connectionpool.py:452)\n",
      "2023-03-24 19:40:09 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Package jupyter scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Scraping package nest (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Proxy list rotated, using 43.153.177.137:443, next will be 94.131.114.69:3128 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Using proxy: {'http': 'http://43.153.177.137:443'} (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1; rv:37.0) Gecko/20100101 Firefox/37.0 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:09 [   DEBUG] https://pypi.org:443 \"GET /pypi/nest/json HTTP/1.1\" 200 2253 (connectionpool.py:452)\n",
      "2023-03-24 19:40:09 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Package nest scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Scraping package psutil (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Proxy list rotated, using 94.131.114.69:3128, next will be 161.35.70.249:8080 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Using proxy: {'http': 'http://94.131.114.69:3128'} (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:09 [   DEBUG] https://pypi.org:443 \"GET /pypi/psutil/json HTTP/1.1\" 200 180959 (connectionpool.py:452)\n",
      "2023-03-24 19:40:09 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Package psutil scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Scraping package pyzmq (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Proxy list rotated, using 161.35.70.249:8080, next will be 103.4.117.161:8080 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Using proxy: {'http': 'http://161.35.70.249:8080'} (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; WOW64; rv:39.0) Gecko/20100101 Firefox/39.0 (logger.py:122)\n",
      "2023-03-24 19:40:09 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:09 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyzmq/json HTTP/1.1\" 200 315528 (connectionpool.py:452)\n",
      "2023-03-24 19:40:10 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Package pyzmq scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Scraping package tornado (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Proxy list rotated, using 103.4.117.161:8080, next will be 66.23.233.210:51657 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Using proxy: {'http': 'http://103.4.117.161:8080'} (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.2; SM-T520 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:10 [   DEBUG] https://pypi.org:443 \"GET /pypi/tornado/json HTTP/1.1\" 200 49695 (connectionpool.py:452)\n",
      "2023-03-24 19:40:10 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Package tornado scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Scraping package curio (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Proxy list rotated, using 66.23.233.210:51657, next will be 116.212.142.231:33427 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Using proxy: {'http': 'http://66.23.233.210:51657'} (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; InfoPath.3; .NET4.0C; .NET4.0E; MS-RTC LM 8) (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:10 [   DEBUG] https://pypi.org:443 \"GET /pypi/curio/json HTTP/1.1\" 200 3284 (connectionpool.py:452)\n",
      "2023-03-24 19:40:10 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Package curio scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Scraping package trio (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Proxy list rotated, using 116.212.142.231:33427, next will be 201.234.24.1:4153 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Using proxy: {'http': 'http://116.212.142.231:33427'} (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.152 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:10 [   DEBUG] https://pypi.org:443 \"GET /pypi/trio/json HTTP/1.1\" 200 10137 (connectionpool.py:452)\n",
      "2023-03-24 19:40:10 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Package trio scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Scraping package sphinxcontrib (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Proxy list rotated, using 201.234.24.1:4153, next will be 47.243.168.116:25812 (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Using proxy: {'http': 'http://201.234.24.1:4153'} (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET4.0C; .NET4.0E; MS-RTC LM 8; InfoPath.3) (logger.py:122)\n",
      "2023-03-24 19:40:10 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:11 [   DEBUG] https://pypi.org:443 \"GET /pypi/sphinxcontrib/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:11 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] ScraperError: Package sphinxcontrib not found (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] The package sphinxcontrib, as dependency of ipykernel does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Scraping package pyqt5 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Proxy list rotated, using 47.243.168.116:25812, next will be 20.191.183.50:3129 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Using proxy: {'http': 'http://47.243.168.116:25812'} (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.0; SAMSUNG-SM-G870A Build/LRX21T) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/2.1 Chrome/34.0.1847.76 Mobile Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:11 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyqt5/json HTTP/1.1\" 200 23556 (connectionpool.py:452)\n",
      "2023-03-24 19:40:11 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Package pyqt5 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Scraping package pyside6 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Proxy list rotated, using 20.191.183.50:3129, next will be 203.32.120.195:80 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Using proxy: {'http': 'http://20.191.183.50:3129'} (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.2.5 (KHTML, like Gecko) Version/7.1.2 Safari/537.85.11 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:11 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyside6/json HTTP/1.1\" 200 16449 (connectionpool.py:452)\n",
      "2023-03-24 19:40:11 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Package pyside6 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Scraping package flaky (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Proxy list rotated, using 203.32.120.195:80, next will be 109.172.114.16:45785 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Using proxy: {'http': 'http://203.32.120.195:80'} (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:11 [   DEBUG] https://pypi.org:443 \"GET /pypi/flaky/json HTTP/1.1\" 200 11549 (connectionpool.py:452)\n",
      "2023-03-24 19:40:11 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Package flaky scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Scraping package ipyparallel (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Proxy list rotated, using 109.172.114.16:45785, next will be 117.102.115.156:4153 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Using proxy: {'http': 'http://109.172.114.16:45785'} (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/536.30.1 (KHTML, like Gecko) Version/6.0.5 Safari/536.30.1 (logger.py:122)\n",
      "2023-03-24 19:40:11 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:12 [   DEBUG] https://pypi.org:443 \"GET /pypi/ipyparallel/json HTTP/1.1\" 200 16690 (connectionpool.py:452)\n",
      "2023-03-24 19:40:12 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Package ipyparallel scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Scraping package docrepr (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Proxy list rotated, using 117.102.115.156:4153, next will be 103.51.45.9:4145 (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Using proxy: {'http': 'http://117.102.115.156:4153'} (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:12 [   DEBUG] https://pypi.org:443 \"GET /pypi/docrepr/json HTTP/1.1\" 200 2872 (connectionpool.py:452)\n",
      "2023-03-24 19:40:12 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Package docrepr scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Scraping package playwright (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Proxy list rotated, using 103.51.45.9:4145, next will be 177.125.204.78:4145 (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Using proxy: {'http': 'http://103.51.45.9:4145'} (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:12 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:13 [   DEBUG] https://pypi.org:443 \"GET /pypi/playwright/json HTTP/1.1\" 200 45167 (connectionpool.py:452)\n",
      "2023-03-24 19:40:13 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Package playwright scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Scraping package testpath (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Proxy list rotated, using 177.125.204.78:4145, next will be 212.34.239.253:1080 (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Using proxy: {'http': 'http://177.125.204.78:4145'} (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET4.0C; .NET4.0E) (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:13 [   DEBUG] https://pypi.org:443 \"GET /pypi/testpath/json HTTP/1.1\" 200 3983 (connectionpool.py:452)\n",
      "2023-03-24 19:40:13 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Package testpath scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Scraping package nbconvert (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Proxy list rotated, using 212.34.239.253:1080, next will be 18.156.174.21:14880 (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Using proxy: {'http': 'http://212.34.239.253:1080'} (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:13 [   DEBUG] https://pypi.org:443 \"GET /pypi/nbconvert/json HTTP/1.1\" 200 25787 (connectionpool.py:452)\n",
      "2023-03-24 19:40:13 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Package nbconvert scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Scraping package bleach (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Proxy list rotated, using 18.156.174.21:14880, next will be 20.24.81.3:8080 (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Using proxy: {'http': 'http://18.156.174.21:14880'} (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; InfoPath.3; .NET4.0C; .NET4.0E) (logger.py:122)\n",
      "2023-03-24 19:40:13 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:14 [   DEBUG] https://pypi.org:443 \"GET /pypi/bleach/json HTTP/1.1\" 200 24391 (connectionpool.py:452)\n",
      "2023-03-24 19:40:14 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Package bleach scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Scraping package jupyterlab (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Proxy list rotated, using 20.24.81.3:8080, next will be 113.195.221.20:9999 (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Using proxy: {'http': 'http://20.24.81.3:8080'} (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.104 AOL/9.8 AOLBuild/4346.13.US Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:14 [   DEBUG] https://pypi.org:443 \"GET /pypi/jupyterlab/json HTTP/1.1\" 200 144585 (connectionpool.py:452)\n",
      "2023-03-24 19:40:14 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Package jupyterlab scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Scraping package markupsafe (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Proxy list rotated, using 113.195.221.20:9999, next will be 201.217.49.2:80 (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Using proxy: {'http': 'http://113.195.221.20:9999'} (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:14 [   DEBUG] https://pypi.org:443 \"GET /pypi/markupsafe/json HTTP/1.1\" 200 69733 (connectionpool.py:452)\n",
      "2023-03-24 19:40:14 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Package markupsafe scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Scraping package mistune (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Proxy list rotated, using 201.217.49.2:80, next will be 37.99.254.217:5678 (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Using proxy: {'http': 'http://201.217.49.2:80'} (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534+ (KHTML, like Gecko) MsnBot-Media /1.0b (logger.py:122)\n",
      "2023-03-24 19:40:14 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:15 [   DEBUG] https://pypi.org:443 \"GET /pypi/mistune/json HTTP/1.1\" 200 16013 (connectionpool.py:452)\n",
      "2023-03-24 19:40:15 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Package mistune scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Scraping package nbclient (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Proxy list rotated, using 37.99.254.217:5678, next will be 114.99.1.157:8004 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using proxy: {'http': 'http://37.99.254.217:5678'} (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.13 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:15 [   DEBUG] https://pypi.org:443 \"GET /pypi/nbclient/json HTTP/1.1\" 200 14213 (connectionpool.py:452)\n",
      "2023-03-24 19:40:15 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Package nbclient scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Scraping package nbformat (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Proxy list rotated, using 114.99.1.157:8004, next will be 202.65.192.252:80 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using proxy: {'http': 'http://114.99.1.157:8004'} (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:40.0) Gecko/20100101 Firefox/40.0.2 Waterfox/40.0.2 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:15 [   DEBUG] https://pypi.org:443 \"GET /pypi/nbformat/json HTTP/1.1\" 200 11752 (connectionpool.py:452)\n",
      "2023-03-24 19:40:15 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Package nbformat scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Scraping package pandocfilters (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Proxy list rotated, using 202.65.192.252:80, next will be 182.16.245.54:30617 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using proxy: {'http': 'http://202.65.192.252:80'} (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:37.0) Gecko/20100101 Firefox/37.0 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:15 [   DEBUG] https://pypi.org:443 \"GET /pypi/pandocfilters/json HTTP/1.1\" 200 6550 (connectionpool.py:452)\n",
      "2023-03-24 19:40:15 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Package pandocfilters scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Scraping package tinycss2 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Proxy list rotated, using 182.16.245.54:30617, next will be 43.130.134.188:80 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using proxy: {'http': 'http://182.16.245.54:30617'} (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.0.9895 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:15 [   DEBUG] https://pypi.org:443 \"GET /pypi/tinycss2/json HTTP/1.1\" 200 5218 (connectionpool.py:452)\n",
      "2023-03-24 19:40:15 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Package tinycss2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Scraping package nbsphinx (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Proxy list rotated, using 43.130.134.188:80, next will be 115.96.208.124:8080 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using proxy: {'http': 'http://43.130.134.188:80'} (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:15 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:16 [   DEBUG] https://pypi.org:443 \"GET /pypi/nbsphinx/json HTTP/1.1\" 200 16490 (connectionpool.py:452)\n",
      "2023-03-24 19:40:16 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Package nbsphinx scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Scraping package sphinxcontrib (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Proxy list rotated, using 115.96.208.124:8080, next will be 165.227.139.174:53913 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Using proxy: {'http': 'http://115.96.208.124:8080'} (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.124 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:16 [   DEBUG] https://pypi.org:443 \"GET /pypi/sphinxcontrib/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:16 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] ScraperError: Package sphinxcontrib not found (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] The package sphinxcontrib, as dependency of nbconvert does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Scraping package pyqtwebengine (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Proxy list rotated, using 165.227.139.174:53913, next will be 178.63.244.28:4003 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Using proxy: {'http': 'http://165.227.139.174:53913'} (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; yie11; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:16 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyqtwebengine/json HTTP/1.1\" 200 10517 (connectionpool.py:452)\n",
      "2023-03-24 19:40:16 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Package pyqtwebengine scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Scraping package pyppeteer (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Proxy list rotated, using 178.63.244.28:4003, next will be 185.160.217.2:8123 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Using proxy: {'http': 'http://178.63.244.28:4003'} (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; Touch; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:16 [   DEBUG] https://pypi.org:443 \"GET /pypi/pyppeteer/json HTTP/1.1\" 200 9531 (connectionpool.py:452)\n",
      "2023-03-24 19:40:16 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Package pyppeteer scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Scraping package notebook (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Proxy list rotated, using 185.160.217.2:8123, next will be 212.69.12.181:1080 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Using proxy: {'http': 'http://185.160.217.2:8123'} (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:16 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:16 [   DEBUG] https://pypi.org:443 \"GET /pypi/notebook/json HTTP/1.1\" 200 36636 (connectionpool.py:452)\n",
      "2023-03-24 19:40:17 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Package notebook scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Scraping package argon2 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Proxy list rotated, using 212.69.12.181:1080, next will be 86.123.217.199:8080 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using proxy: {'http': 'http://212.69.12.181:1080'} (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.65 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:17 [   DEBUG] https://pypi.org:443 \"GET /pypi/argon2/json HTTP/1.1\" 200 2078 (connectionpool.py:452)\n",
      "2023-03-24 19:40:17 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Package argon2 scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Scraping package Send2Trash (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Proxy list rotated, using 86.123.217.199:8080, next will be 114.69.244.66:60616 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using proxy: {'http': 'http://86.123.217.199:8080'} (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:17 [   DEBUG] https://pypi.org:443 \"GET /pypi/Send2Trash/json HTTP/1.1\" 200 6958 (connectionpool.py:452)\n",
      "2023-03-24 19:40:17 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Package Send2Trash scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Scraping package terminado (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Proxy list rotated, using 114.69.244.66:60616, next will be 103.60.138.33:4153 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using proxy: {'http': 'http://114.69.244.66:60616'} (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; Touch; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:17 [   DEBUG] https://pypi.org:443 \"GET /pypi/terminado/json HTTP/1.1\" 200 12384 (connectionpool.py:452)\n",
      "2023-03-24 19:40:17 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Package terminado scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Scraping package prometheus (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Proxy list rotated, using 103.60.138.33:4153, next will be 81.210.61.2:65000 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using proxy: {'http': 'http://103.60.138.33:4153'} (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using user agent: Mozilla/5.0 (iPhone; CPU iPhone OS 8_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12F70 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:17 [   DEBUG] https://pypi.org:443 \"GET /pypi/prometheus/json HTTP/1.1\" 200 4143 (connectionpool.py:452)\n",
      "2023-03-24 19:40:17 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Package prometheus scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Scraping package nbclassic (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Proxy list rotated, using 81.210.61.2:65000, next will be 146.59.14.159:80 (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using proxy: {'http': 'http://81.210.61.2:65000'} (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:17 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:18 [   DEBUG] https://pypi.org:443 \"GET /pypi/nbclassic/json HTTP/1.1\" 200 14011 (connectionpool.py:452)\n",
      "2023-03-24 19:40:18 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Package nbclassic scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Scraping package sphinxcontrib (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Proxy list rotated, using 146.59.14.159:80, next will be 103.210.29.193:31433 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using proxy: {'http': 'http://146.59.14.159:80'} (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; ASJB; ASJB; MAAU; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:18 [   DEBUG] https://pypi.org:443 \"GET /pypi/sphinxcontrib/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:18 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] ScraperError: Package sphinxcontrib not found (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] The package sphinxcontrib, as dependency of notebook does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Scraping package json (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Proxy list rotated, using 103.210.29.193:31433, next will be 104.248.154.128:3128 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using proxy: {'http': 'http://103.210.29.193:31433'} (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.22 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:18 [   DEBUG] https://pypi.org:443 \"GET /pypi/json/json HTTP/1.1\" 404 24 (connectionpool.py:452)\n",
      "2023-03-24 19:40:18 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] ScraperError: Package json not found (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] The package json, as dependency of notebook does not exist in the data source PyPI Scraper (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Scraping package nbval (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Proxy list rotated, using 104.248.154.128:3128, next will be 185.66.59.219:42647 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using proxy: {'http': 'http://104.248.154.128:3128'} (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:37.0) Gecko/20100101 Firefox/37.0 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:18 [   DEBUG] https://pypi.org:443 \"GET /pypi/nbval/json HTTP/1.1\" 200 8983 (connectionpool.py:452)\n",
      "2023-03-24 19:40:18 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Package nbval scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Scraping package selenium (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Proxy list rotated, using 185.66.59.219:42647, next will be 34.170.89.64:80 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using proxy: {'http': 'http://185.66.59.219:42647'} (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.2; WOW64; Trident/7.0; .NET4.0E; .NET4.0C; .NET CLR 3.5.30729; .NET CLR 2.0.50727; .NET CLR 3.0.30729) (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:18 [   DEBUG] https://pypi.org:443 \"GET /pypi/selenium/json HTTP/1.1\" 200 39335 (connectionpool.py:452)\n",
      "2023-03-24 19:40:18 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Package selenium scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Scraping package qtconsole (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Proxy list rotated, using 34.170.89.64:80, next will be 217.219.86.250:5678 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using proxy: {'http': 'http://34.170.89.64:80'} (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:18 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:19 [   DEBUG] https://pypi.org:443 \"GET /pypi/qtconsole/json HTTP/1.1\" 200 15736 (connectionpool.py:452)\n",
      "2023-03-24 19:40:19 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Package qtconsole scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Scraping package qtpy (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Proxy list rotated, using 217.219.86.250:5678, next will be 185.202.7.160:4153 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using proxy: {'http': 'http://217.219.86.250:5678'} (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using user agent: Mozilla/5.0 (iPhone; CPU iPhone OS 8_1_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12B466 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:19 [   DEBUG] https://pypi.org:443 \"GET /pypi/qtpy/json HTTP/1.1\" 200 14709 (connectionpool.py:452)\n",
      "2023-03-24 19:40:19 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Package qtpy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Scraping package sphinxtesters (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Proxy list rotated, using 185.202.7.160:4153, next will be 161.35.6.250:443 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using proxy: {'http': 'http://185.202.7.160:4153'} (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using user agent: Mozilla/5.0 (Android; Mobile; rv:40.0) Gecko/40.0 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:19 [   DEBUG] https://pypi.org:443 \"GET /pypi/sphinxtesters/json HTTP/1.1\" 200 2911 (connectionpool.py:452)\n",
      "2023-03-24 19:40:19 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Package sphinxtesters scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Scraping package texext (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Proxy list rotated, using 161.35.6.250:443, next will be 43.153.168.234:443 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using proxy: {'http': 'http://161.35.6.250:443'} (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.114 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:19 [   DEBUG] https://pypi.org:443 \"GET /pypi/texext/json HTTP/1.1\" 200 6308 (connectionpool.py:452)\n",
      "2023-03-24 19:40:19 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Package texext scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Scraping package sympy (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Proxy list rotated, using 43.153.168.234:443, next will be 185.6.10.41:28150 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using proxy: {'http': 'http://43.153.168.234:443'} (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:19 [   DEBUG] https://pypi.org:443 \"GET /pypi/sympy/json HTTP/1.1\" 200 19501 (connectionpool.py:452)\n",
      "2023-03-24 19:40:19 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Package sympy scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Scraping package scripttester (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Proxy list rotated, using 185.6.10.41:28150, next will be 190.14.224.244:3629 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using proxy: {'http': 'http://185.6.10.41:28150'} (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:19 [   DEBUG] https://pypi.org:443 \"GET /pypi/scripttester/json HTTP/1.1\" 200 1961 (connectionpool.py:452)\n",
      "2023-03-24 19:40:19 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Package scripttester scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Scraping package pygraphviz (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Proxy list rotated, using 190.14.224.244:3629, next will be 39.107.46.15:9091 (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using proxy: {'http': 'http://190.14.224.244:3629'} (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Using user agent: Mozilla/5.0 (Windows Phone 8.1; ARM; Trident/7.0; Touch; rv:11.0; IEMobile/11.0; NOKIA; Lumia 635) like Gecko (logger.py:122)\n",
      "2023-03-24 19:40:19 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:20 [   DEBUG] https://pypi.org:443 \"GET /pypi/pygraphviz/json HTTP/1.1\" 200 10419 (connectionpool.py:452)\n",
      "2023-03-24 19:40:20 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Package pygraphviz scraped successfully (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Scraping package pydot (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Proxy list rotated, using 39.107.46.15:9091, next will be 103.116.202.241:5678 (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Using proxy: {'http': 'http://39.107.46.15:9091'} (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.13 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Starting new HTTPS connection (1): pypi.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:40:20 [   DEBUG] https://pypi.org:443 \"GET /pypi/pydot/json HTTP/1.1\" 200 6156 (connectionpool.py:452)\n",
      "2023-03-24 19:40:20 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:40:20 [   DEBUG] Package pydot scraped successfully (logger.py:122)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'networkx': [{'name': 'numpy', 'version': None},\n",
       "  {'name': 'scipy', 'version': None},\n",
       "  {'name': 'matplotlib', 'version': None},\n",
       "  {'name': 'pandas', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'pydata', 'version': None},\n",
       "  {'name': 'numpydoc', 'version': None},\n",
       "  {'name': 'pillow', 'version': None},\n",
       "  {'name': 'nb2plots', 'version': None},\n",
       "  {'name': 'texext', 'version': None},\n",
       "  {'name': 'lxml', 'version': None},\n",
       "  {'name': 'pygraphviz', 'version': None},\n",
       "  {'name': 'pydot', 'version': None},\n",
       "  {'name': 'sympy', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'codecov', 'version': None}],\n",
       " 'numpy': [],\n",
       " 'scipy': [{'name': 'numpy', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'asv', 'version': None},\n",
       "  {'name': 'mpmath', 'version': None},\n",
       "  {'name': 'gmpy2', 'version': None},\n",
       "  {'name': 'threadpoolctl', 'version': None},\n",
       "  {'name': 'scikit', 'version': None},\n",
       "  {'name': 'pooch', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'pydata', 'version': None},\n",
       "  {'name': 'matplotlib', 'version': None},\n",
       "  {'name': 'numpydoc', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'typing_extensions', 'version': None},\n",
       "  {'name': 'pycodestyle', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'rich', 'version': None},\n",
       "  {'name': 'click', 'version': None},\n",
       "  {'name': 'doit', 'version': None},\n",
       "  {'name': 'pydevtool', 'version': None}],\n",
       " 'pytest': [{'name': 'attrs', 'version': None},\n",
       "  {'name': 'iniconfig', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'pluggy', 'version': None},\n",
       "  {'name': 'exceptiongroup', 'version': None},\n",
       "  {'name': 'tomli', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'colorama', 'version': None},\n",
       "  {'name': 'argcomplete', 'version': None},\n",
       "  {'name': 'hypothesis', 'version': None},\n",
       "  {'name': 'mock', 'version': None},\n",
       "  {'name': 'nose', 'version': None},\n",
       "  {'name': 'pygments', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'xmlschema', 'version': None}],\n",
       " 'attrs': [{'name': 'attrs', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'furo', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'myst', 'version': None},\n",
       "  {'name': 'zope', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'towncrier', 'version': None},\n",
       "  {'name': 'hypothesis', 'version': None},\n",
       "  {'name': 'pympler', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'cloudpickle', 'version': None},\n",
       "  {'name': 'mypy', 'version': None}],\n",
       " 'coverage': [{'name': 'tomli', 'version': None}],\n",
       " 'furo': [{'name': 'beautifulsoup4', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'pygments', 'version': None}],\n",
       " 'sphinx': [{'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'Jinja2', 'version': None},\n",
       "  {'name': 'Pygments', 'version': None},\n",
       "  {'name': 'docutils', 'version': None},\n",
       "  {'name': 'snowballstemmer', 'version': None},\n",
       "  {'name': 'babel', 'version': None},\n",
       "  {'name': 'alabaster', 'version': None},\n",
       "  {'name': 'imagesize', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'colorama', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'isort', 'version': None},\n",
       "  {'name': 'ruff', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'types', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'html5lib', 'version': None},\n",
       "  {'name': 'cython', 'version': None}],\n",
       " 'myst': [{'name': 'google', 'version': None},\n",
       "  {'name': 'pytz', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'urllib3', 'version': None},\n",
       "  {'name': 'tenacity', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'mock', 'version': None},\n",
       "  {'name': 'responses', 'version': None},\n",
       "  {'name': 'tox', 'version': None},\n",
       "  {'name': 'readme', 'version': None},\n",
       "  {'name': 'twine', 'version': None},\n",
       "  {'name': 'check', 'version': None}],\n",
       " 'zope': [{'name': 'AccessControl', 'version': None},\n",
       "  {'name': 'Acquisition', 'version': None},\n",
       "  {'name': 'BTrees', 'version': None},\n",
       "  {'name': 'Chameleon', 'version': None},\n",
       "  {'name': 'DateTime', 'version': None},\n",
       "  {'name': 'DocumentTemplate', 'version': None},\n",
       "  {'name': 'ExtensionClass', 'version': None},\n",
       "  {'name': 'MultiMapping', 'version': None},\n",
       "  {'name': 'PasteDeploy', 'version': None},\n",
       "  {'name': 'Persistence', 'version': None},\n",
       "  {'name': 'RestrictedPython', 'version': None},\n",
       "  {'name': 'ZConfig', 'version': None},\n",
       "  {'name': 'ZODB', 'version': None},\n",
       "  {'name': 'setuptools', 'version': None},\n",
       "  {'name': 'transaction', 'version': None},\n",
       "  {'name': 'waitress', 'version': None},\n",
       "  {'name': 'zExceptions', 'version': None},\n",
       "  {'name': 'z3c', 'version': None},\n",
       "  {'name': 'zope', 'version': None},\n",
       "  {'name': 'multipart', 'version': None},\n",
       "  {'name': 'Sphinx', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'tempstorage', 'version': None},\n",
       "  {'name': 'Paste', 'version': None}],\n",
       " 'towncrier': [{'name': 'click', 'version': None},\n",
       "  {'name': 'incremental', 'version': None},\n",
       "  {'name': 'jinja2', 'version': None},\n",
       "  {'name': 'setuptools', 'version': None},\n",
       "  {'name': 'tomli', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'furo', 'version': None},\n",
       "  {'name': 'twisted', 'version': None}],\n",
       " 'hypothesis': [{'name': 'attrs', 'version': None},\n",
       "  {'name': 'sortedcontainers', 'version': None},\n",
       "  {'name': 'exceptiongroup', 'version': None},\n",
       "  {'name': 'black', 'version': None},\n",
       "  {'name': 'click', 'version': None},\n",
       "  {'name': 'django', 'version': None},\n",
       "  {'name': 'dpcontracts', 'version': None},\n",
       "  {'name': 'lark', 'version': None},\n",
       "  {'name': 'libcst', 'version': None},\n",
       "  {'name': 'numpy', 'version': None},\n",
       "  {'name': 'pandas', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'python', 'version': None},\n",
       "  {'name': 'pytz', 'version': None},\n",
       "  {'name': 'redis', 'version': None},\n",
       "  {'name': 'rich', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'backports', 'version': None},\n",
       "  {'name': 'tzdata', 'version': None}],\n",
       " 'pympler': [],\n",
       " 'cloudpickle': [],\n",
       " 'mypy': [{'name': 'typing', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'tomli', 'version': None},\n",
       "  {'name': 'typed', 'version': None},\n",
       "  {'name': 'psutil', 'version': None},\n",
       "  {'name': 'pip', 'version': None},\n",
       "  {'name': 'lxml', 'version': None}],\n",
       " 'iniconfig': [],\n",
       " 'packaging': [],\n",
       " 'pluggy': [{'name': 'importlib', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'tox', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'importlib': [],\n",
       " 'pre': [],\n",
       " 'tox': [{'name': 'cachetools', 'version': None},\n",
       "  {'name': 'chardet', 'version': None},\n",
       "  {'name': 'colorama', 'version': None},\n",
       "  {'name': 'filelock', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'platformdirs', 'version': None},\n",
       "  {'name': 'pluggy', 'version': None},\n",
       "  {'name': 'pyproject', 'version': None},\n",
       "  {'name': 'tomli', 'version': None},\n",
       "  {'name': 'typing', 'version': None},\n",
       "  {'name': 'virtualenv', 'version': None},\n",
       "  {'name': 'furo', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'towncrier', 'version': None},\n",
       "  {'name': 'build', 'version': None},\n",
       "  {'name': 'covdefaults', 'version': None},\n",
       "  {'name': 'devpi', 'version': None},\n",
       "  {'name': 'diff', 'version': None},\n",
       "  {'name': 'distlib', 'version': None},\n",
       "  {'name': 'flaky', 'version': None},\n",
       "  {'name': 'hatch', 'version': None},\n",
       "  {'name': 'hatchling', 'version': None},\n",
       "  {'name': 'psutil', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 're', 'version': None},\n",
       "  {'name': 'time', 'version': None},\n",
       "  {'name': 'wheel', 'version': None}],\n",
       " 'exceptiongroup': [{'name': 'pytest', 'version': None}],\n",
       " 'tomli': [],\n",
       " 'colorama': [],\n",
       " 'argcomplete': [{'name': 'importlib', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'pexpect', 'version': None},\n",
       "  {'name': 'wheel', 'version': None}],\n",
       " 'flake8': [{'name': 'mccabe', 'version': None},\n",
       "  {'name': 'pycodestyle', 'version': None},\n",
       "  {'name': 'pyflakes', 'version': None}],\n",
       " 'pexpect': [{'name': 'ptyprocess', 'version': None}],\n",
       " 'wheel': [{'name': 'pytest', 'version': None}],\n",
       " 'mock': [{'name': 'twine', 'version': None},\n",
       "  {'name': 'wheel', 'version': None},\n",
       "  {'name': 'blurb', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'twine': [{'name': 'pkginfo', 'version': None},\n",
       "  {'name': 'readme', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'urllib3', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'keyring', 'version': None},\n",
       "  {'name': 'rfc3986', 'version': None},\n",
       "  {'name': 'rich', 'version': None}],\n",
       " 'blurb': [],\n",
       " 'nose': [],\n",
       " 'pygments': [{'name': 'importlib', 'version': None}],\n",
       " 'requests': [{'name': 'charset', 'version': None},\n",
       "  {'name': 'idna', 'version': None},\n",
       "  {'name': 'urllib3', 'version': None},\n",
       "  {'name': 'certifi', 'version': None},\n",
       "  {'name': 'PySocks', 'version': None},\n",
       "  {'name': 'chardet', 'version': None}],\n",
       " 'charset': [],\n",
       " 'idna': [],\n",
       " 'urllib3': [{'name': 'brotlicffi', 'version': None},\n",
       "  {'name': 'brotli', 'version': None},\n",
       "  {'name': 'brotlipy', 'version': None},\n",
       "  {'name': 'pyOpenSSL', 'version': None},\n",
       "  {'name': 'cryptography', 'version': None},\n",
       "  {'name': 'idna', 'version': None},\n",
       "  {'name': 'certifi', 'version': None},\n",
       "  {'name': 'urllib3', 'version': None},\n",
       "  {'name': 'ipaddress', 'version': None},\n",
       "  {'name': 'PySocks', 'version': None}],\n",
       " 'certifi': [],\n",
       " 'PySocks': [],\n",
       " 'chardet': [],\n",
       " 'xmlschema': [{'name': 'elementpath', 'version': None},\n",
       "  {'name': 'jinja2', 'version': None},\n",
       "  {'name': 'tox', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'lxml', 'version': None},\n",
       "  {'name': 'memory', 'version': None},\n",
       "  {'name': 'Sphinx', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'mypy', 'version': None}],\n",
       " 'elementpath': [{'name': 'tox', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'lxml', 'version': None},\n",
       "  {'name': 'xmlschema', 'version': None},\n",
       "  {'name': 'Sphinx', 'version': None},\n",
       "  {'name': 'memory', 'version': None},\n",
       "  {'name': 'memray', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'mypy', 'version': None}],\n",
       " 'jinja2': [{'name': 'MarkupSafe', 'version': None},\n",
       "  {'name': 'Babel', 'version': None}],\n",
       " 'lxml': [{'name': 'cssselect', 'version': None},\n",
       "  {'name': 'html5lib', 'version': None},\n",
       "  {'name': 'BeautifulSoup4', 'version': None},\n",
       "  {'name': 'Cython', 'version': None}],\n",
       " 'Sphinx': [{'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'Jinja2', 'version': None},\n",
       "  {'name': 'Pygments', 'version': None},\n",
       "  {'name': 'docutils', 'version': None},\n",
       "  {'name': 'snowballstemmer', 'version': None},\n",
       "  {'name': 'babel', 'version': None},\n",
       "  {'name': 'alabaster', 'version': None},\n",
       "  {'name': 'imagesize', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'colorama', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'isort', 'version': None},\n",
       "  {'name': 'ruff', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'types', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'html5lib', 'version': None},\n",
       "  {'name': 'cython', 'version': None}],\n",
       " 'asv': [],\n",
       " 'mpmath': [{'name': 'pytest', 'version': None},\n",
       "  {'name': 'pycodestyle', 'version': None},\n",
       "  {'name': 'codecov', 'version': None},\n",
       "  {'name': 'wheel', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'gmpy2', 'version': None}],\n",
       " 'pycodestyle': [],\n",
       " 'codecov': [{'name': 'requests', 'version': None},\n",
       "  {'name': 'coverage', 'version': None}],\n",
       " 'gmpy2': [],\n",
       " 'threadpoolctl': [],\n",
       " 'pooch': [{'name': 'platformdirs', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'tqdm', 'version': None},\n",
       "  {'name': 'paramiko', 'version': None},\n",
       "  {'name': 'xxhash', 'version': None}],\n",
       " 'platformdirs': [{'name': 'typing', 'version': None},\n",
       "  {'name': 'furo', 'version': None},\n",
       "  {'name': 'proselint', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'appdirs', 'version': None},\n",
       "  {'name': 'covdefaults', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'typing': [],\n",
       " 'proselint': [{'name': 'click', 'version': None},\n",
       "  {'name': 'future', 'version': None},\n",
       "  {'name': 'six', 'version': None}],\n",
       " 'appdirs': [],\n",
       " 'covdefaults': [{'name': 'coverage', 'version': None}],\n",
       " 'tqdm': [{'name': 'colorama', 'version': None},\n",
       "  {'name': 'py', 'version': None},\n",
       "  {'name': 'twine', 'version': None},\n",
       "  {'name': 'wheel', 'version': None},\n",
       "  {'name': 'ipywidgets', 'version': None},\n",
       "  {'name': 'slack', 'version': None},\n",
       "  {'name': 'requests', 'version': None}],\n",
       " 'py': [],\n",
       " 'ipywidgets': [{'name': 'ipython', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'widgetsnbextension', 'version': None},\n",
       "  {'name': 'jupyterlab', 'version': None},\n",
       "  {'name': 'jsonschema', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'pytz', 'version': None}],\n",
       " 'slack': [],\n",
       " 'paramiko': [{'name': 'bcrypt', 'version': None},\n",
       "  {'name': 'cryptography', 'version': None},\n",
       "  {'name': 'pynacl', 'version': None},\n",
       "  {'name': 'pyasn1', 'version': None},\n",
       "  {'name': 'invoke', 'version': None},\n",
       "  {'name': 'gssapi', 'version': None},\n",
       "  {'name': 'pywin32', 'version': None}],\n",
       " 'bcrypt': [{'name': 'pytest', 'version': None},\n",
       "  {'name': 'mypy', 'version': None}],\n",
       " 'cryptography': [{'name': 'cffi', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'pyenchant', 'version': None},\n",
       "  {'name': 'twine', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'black', 'version': None},\n",
       "  {'name': 'ruff', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'check', 'version': None},\n",
       "  {'name': 'setuptools', 'version': None},\n",
       "  {'name': 'bcrypt', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'pretend', 'version': None},\n",
       "  {'name': 'iso8601', 'version': None},\n",
       "  {'name': 'tox', 'version': None}],\n",
       " 'pynacl': [],\n",
       " 'pyasn1': [],\n",
       " 'invoke': [],\n",
       " 'gssapi': [{'name': 'decorator', 'version': None}],\n",
       " 'pywin32': [],\n",
       " 'xxhash': [],\n",
       " 'pydata': [{'name': 'beautifulsoup4', 'version': None},\n",
       "  {'name': 'lxml', 'version': None}],\n",
       " 'beautifulsoup4': [{'name': 'soupsieve', 'version': None},\n",
       "  {'name': 'html5lib', 'version': None},\n",
       "  {'name': 'lxml', 'version': None}],\n",
       " 'soupsieve': [],\n",
       " 'html5lib': [{'name': 'six', 'version': None},\n",
       "  {'name': 'webencodings', 'version': None},\n",
       "  {'name': 'genshi', 'version': None},\n",
       "  {'name': 'chardet', 'version': None},\n",
       "  {'name': 'lxml', 'version': None}],\n",
       " 'matplotlib': [],\n",
       " 'numpydoc': [{'name': 'sphinx', 'version': None},\n",
       "  {'name': 'Jinja2', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'matplotlib', 'version': None}],\n",
       " 'Jinja2': [{'name': 'MarkupSafe', 'version': None},\n",
       "  {'name': 'Babel', 'version': None}],\n",
       " 'MarkupSafe': [],\n",
       " 'Babel': [{'name': 'pytz', 'version': None}],\n",
       " 'typing_extensions': [],\n",
       " 'rich': [{'name': 'typing', 'version': None},\n",
       "  {'name': 'pygments', 'version': None},\n",
       "  {'name': 'ipywidgets', 'version': None},\n",
       "  {'name': 'markdown', 'version': None}],\n",
       " 'markdown': [{'name': 'importlib', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'pyyaml', 'version': None}],\n",
       " 'pyyaml': [],\n",
       " 'click': [{'name': 'colorama', 'version': None},\n",
       "  {'name': 'importlib', 'version': None}],\n",
       " 'doit': [],\n",
       " 'pydevtool': [{'name': 'doit', 'version': None}],\n",
       " 'pandas': [{'name': 'python', 'version': None},\n",
       "  {'name': 'pytz', 'version': None},\n",
       "  {'name': 'numpy', 'version': None},\n",
       "  {'name': 'hypothesis', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'pytz': [],\n",
       " 'pillow': [{'name': 'furo', 'version': None},\n",
       "  {'name': 'olefile', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinxext', 'version': None},\n",
       "  {'name': 'check', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'defusedxml', 'version': None},\n",
       "  {'name': 'markdown2', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'pyroma', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'olefile': [],\n",
       " 'check': [],\n",
       " 'defusedxml': [],\n",
       " 'markdown2': [{'name': 'pygments', 'version': None},\n",
       "  {'name': 'wavedrom', 'version': None}],\n",
       " 'wavedrom': [],\n",
       " 'pyroma': [{'name': 'build', 'version': None},\n",
       "  {'name': 'docutils', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'pygments', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'setuptools', 'version': None},\n",
       "  {'name': 'trove', 'version': None},\n",
       "  {'name': 'zest', 'version': None}],\n",
       " 'build': [{'name': 'pytest', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'pyproject_hooks', 'version': None},\n",
       "  {'name': 'colorama', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'tomli', 'version': None},\n",
       "  {'name': 'furo', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'filelock', 'version': None},\n",
       "  {'name': 'toml', 'version': None},\n",
       "  {'name': 'wheel', 'version': None},\n",
       "  {'name': 'setuptools', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'typing', 'version': None},\n",
       "  {'name': 'virtualenv', 'version': None}],\n",
       " 'pyproject_hooks': [{'name': 'tomli', 'version': None}],\n",
       " 'filelock': [{'name': 'furo', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'covdefaults', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'toml': [],\n",
       " 'setuptools': [{'name': 'sphinx', 'version': None},\n",
       "  {'name': 'jaraco', 'version': None},\n",
       "  {'name': 'rst', 'version': None},\n",
       "  {'name': 'furo', 'version': None},\n",
       "  {'name': 'pygments', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'virtualenv', 'version': None},\n",
       "  {'name': 'wheel', 'version': None},\n",
       "  {'name': 'pip', 'version': None},\n",
       "  {'name': 'build', 'version': None},\n",
       "  {'name': 'filelock', 'version': None},\n",
       "  {'name': 'ini2toml', 'version': None},\n",
       "  {'name': 'tomli', 'version': None}],\n",
       " 'virtualenv': [{'name': 'distlib', 'version': None},\n",
       "  {'name': 'filelock', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'platformdirs', 'version': None},\n",
       "  {'name': 'furo', 'version': None},\n",
       "  {'name': 'proselint', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'towncrier', 'version': None},\n",
       "  {'name': 'covdefaults', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'flaky', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'docutils': [],\n",
       " 'trove': [{'name': 'Jinja2', 'version': None},\n",
       "  {'name': 'Paste', 'version': None},\n",
       "  {'name': 'PasteDeploy', 'version': None},\n",
       "  {'name': 'PyMySQL', 'version': None},\n",
       "  {'name': 'Routes', 'version': None},\n",
       "  {'name': 'SQLAlchemy', 'version': None},\n",
       "  {'name': 'WebOb', 'version': None},\n",
       "  {'name': 'cryptography', 'version': None},\n",
       "  {'name': 'diskimage', 'version': None},\n",
       "  {'name': 'docker', 'version': None},\n",
       "  {'name': 'eventlet', 'version': None},\n",
       "  {'name': 'httplib2', 'version': None},\n",
       "  {'name': 'iso8601', 'version': None},\n",
       "  {'name': 'jsonschema', 'version': None},\n",
       "  {'name': 'keystonemiddleware', 'version': None},\n",
       "  {'name': 'lxml', 'version': None},\n",
       "  {'name': 'netaddr', 'version': None},\n",
       "  {'name': 'oslo', 'version': None},\n",
       "  {'name': 'osprofiler', 'version': None},\n",
       "  {'name': 'passlib', 'version': None},\n",
       "  {'name': 'pbr', 'version': None},\n",
       "  {'name': 'pexpect', 'version': None},\n",
       "  {'name': 'psycopg2', 'version': None},\n",
       "  {'name': 'python', 'version': None},\n",
       "  {'name': 'semantic', 'version': None},\n",
       "  {'name': 'sqlalchemy', 'version': None},\n",
       "  {'name': 'stevedore', 'version': None},\n",
       "  {'name': 'xmltodict', 'version': None}],\n",
       " 'Paste': [{'name': 'setuptools', 'version': None},\n",
       "  {'name': 'six', 'version': None},\n",
       "  {'name': 'flup', 'version': None},\n",
       "  {'name': 'python', 'version': None}],\n",
       " 'PasteDeploy': [{'name': 'importlib', 'version': None},\n",
       "  {'name': 'Sphinx', 'version': None},\n",
       "  {'name': 'pylons', 'version': None},\n",
       "  {'name': 'Paste', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'PyMySQL': [{'name': 'PyNaCl', 'version': None},\n",
       "  {'name': 'cryptography', 'version': None}],\n",
       " 'Routes': [{'name': 'six', 'version': None},\n",
       "  {'name': 'repoze', 'version': None},\n",
       "  {'name': 'Sphinx', 'version': None},\n",
       "  {'name': 'webob', 'version': None}],\n",
       " 'SQLAlchemy': [],\n",
       " 'WebOb': [{'name': 'Sphinx', 'version': None},\n",
       "  {'name': 'pylons', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'coverage', 'version': None}],\n",
       " 'diskimage': [{'name': 'pytsk3', 'version': None},\n",
       "  {'name': 'libewf', 'version': None}],\n",
       " 'docker': [{'name': 'packaging', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'urllib3', 'version': None},\n",
       "  {'name': 'websocket', 'version': None},\n",
       "  {'name': 'pywin32', 'version': None},\n",
       "  {'name': 'paramiko', 'version': None}],\n",
       " 'eventlet': [{'name': 'dnspython', 'version': None},\n",
       "  {'name': 'greenlet', 'version': None},\n",
       "  {'name': 'six', 'version': None},\n",
       "  {'name': 'monotonic', 'version': None}],\n",
       " 'httplib2': [{'name': 'pyparsing', 'version': None}],\n",
       " 'iso8601': [],\n",
       " 'jsonschema': [{'name': 'attrs', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'pkgutil', 'version': None},\n",
       "  {'name': 'pyrsistent', 'version': None},\n",
       "  {'name': 'typing', 'version': None},\n",
       "  {'name': 'fqdn', 'version': None},\n",
       "  {'name': 'idna', 'version': None},\n",
       "  {'name': 'isoduration', 'version': None},\n",
       "  {'name': 'jsonpointer', 'version': None},\n",
       "  {'name': 'rfc3339', 'version': None},\n",
       "  {'name': 'rfc3987', 'version': None},\n",
       "  {'name': 'uri', 'version': None},\n",
       "  {'name': 'webcolors', 'version': None},\n",
       "  {'name': 'rfc3986', 'version': None}],\n",
       " 'keystonemiddleware': [{'name': 'WebOb', 'version': None},\n",
       "  {'name': 'keystoneauth1', 'version': None},\n",
       "  {'name': 'oslo', 'version': None},\n",
       "  {'name': 'pbr', 'version': None},\n",
       "  {'name': 'pycadf', 'version': None},\n",
       "  {'name': 'python', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'six', 'version': None},\n",
       "  {'name': 'WebTest', 'version': None},\n",
       "  {'name': 'bandit', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'cryptography', 'version': None},\n",
       "  {'name': 'fixtures', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'hacking', 'version': None},\n",
       "  {'name': 'oslotest', 'version': None},\n",
       "  {'name': 'pep8', 'version': None},\n",
       "  {'name': 'stestr', 'version': None},\n",
       "  {'name': 'stevedore', 'version': None},\n",
       "  {'name': 'testresources', 'version': None},\n",
       "  {'name': 'testtools', 'version': None}],\n",
       " 'netaddr': [],\n",
       " 'osprofiler': [{'name': 'PrettyTable', 'version': None},\n",
       "  {'name': 'WebOb', 'version': None},\n",
       "  {'name': 'netaddr', 'version': None},\n",
       "  {'name': 'oslo', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'bandit', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'ddt', 'version': None},\n",
       "  {'name': 'docutils', 'version': None},\n",
       "  {'name': 'elasticsearch', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'hacking', 'version': None},\n",
       "  {'name': 'jaeger', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'pymongo', 'version': None},\n",
       "  {'name': 'redis', 'version': None},\n",
       "  {'name': 'stestr', 'version': None},\n",
       "  {'name': 'testtools', 'version': None}],\n",
       " 'passlib': [{'name': 'argon2', 'version': None},\n",
       "  {'name': 'bcrypt', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'cloud', 'version': None},\n",
       "  {'name': 'cryptography', 'version': None}],\n",
       " 'pbr': [],\n",
       " 'psycopg2': [],\n",
       " 'semantic': [],\n",
       " 'sqlalchemy': [],\n",
       " 'stevedore': [{'name': 'pbr', 'version': None}],\n",
       " 'xmltodict': [],\n",
       " 'zest': [],\n",
       " 'nb2plots': [{'name': 'ipython', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'numpy', 'version': None},\n",
       "  {'name': 'matplotlib', 'version': None},\n",
       "  {'name': 'six', 'version': None},\n",
       "  {'name': 'sphinxtesters', 'version': None},\n",
       "  {'name': 'texext', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'mock', 'version': None},\n",
       "  {'name': 'scripttester', 'version': None}],\n",
       " 'ipython': [{'name': 'backcall', 'version': None},\n",
       "  {'name': 'decorator', 'version': None},\n",
       "  {'name': 'jedi', 'version': None},\n",
       "  {'name': 'matplotlib', 'version': None},\n",
       "  {'name': 'pickleshare', 'version': None},\n",
       "  {'name': 'prompt', 'version': None},\n",
       "  {'name': 'pygments', 'version': None},\n",
       "  {'name': 'stack', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'pexpect', 'version': None},\n",
       "  {'name': 'appnope', 'version': None},\n",
       "  {'name': 'colorama', 'version': None},\n",
       "  {'name': 'black', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'setuptools', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'docrepr', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'typing', 'version': None},\n",
       "  {'name': 'testpath', 'version': None},\n",
       "  {'name': 'nbconvert', 'version': None},\n",
       "  {'name': 'nbformat', 'version': None},\n",
       "  {'name': 'ipywidgets', 'version': None},\n",
       "  {'name': 'notebook', 'version': None},\n",
       "  {'name': 'ipyparallel', 'version': None},\n",
       "  {'name': 'qtconsole', 'version': None},\n",
       "  {'name': 'curio', 'version': None},\n",
       "  {'name': 'numpy', 'version': None},\n",
       "  {'name': 'pandas', 'version': None},\n",
       "  {'name': 'trio', 'version': None}],\n",
       " 'backcall': [],\n",
       " 'decorator': [],\n",
       " 'jedi': [{'name': 'parso', 'version': None},\n",
       "  {'name': 'Jinja2', 'version': None},\n",
       "  {'name': 'MarkupSafe', 'version': None},\n",
       "  {'name': 'Pygments', 'version': None},\n",
       "  {'name': 'alabaster', 'version': None},\n",
       "  {'name': 'babel', 'version': None},\n",
       "  {'name': 'chardet', 'version': None},\n",
       "  {'name': 'commonmark', 'version': None},\n",
       "  {'name': 'docutils', 'version': None},\n",
       "  {'name': 'future', 'version': None},\n",
       "  {'name': 'idna', 'version': None},\n",
       "  {'name': 'imagesize', 'version': None},\n",
       "  {'name': 'mock', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'pyparsing', 'version': None},\n",
       "  {'name': 'pytz', 'version': None},\n",
       "  {'name': 'readthedocs', 'version': None},\n",
       "  {'name': 'recommonmark', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'six', 'version': None},\n",
       "  {'name': 'snowballstemmer', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'urllib3', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'Django', 'version': None},\n",
       "  {'name': 'attrs', 'version': None},\n",
       "  {'name': 'colorama', 'version': None},\n",
       "  {'name': 'docopt', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'parso': [{'name': 'flake8', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'docopt', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'Pygments': [{'name': 'importlib', 'version': None}],\n",
       " 'alabaster': [],\n",
       " 'babel': [{'name': 'pytz', 'version': None}],\n",
       " 'commonmark': [{'name': 'future', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'hypothesis', 'version': None}],\n",
       " 'future': [],\n",
       " 'imagesize': [],\n",
       " 'pyparsing': [{'name': 'railroad', 'version': None},\n",
       "  {'name': 'jinja2', 'version': None}],\n",
       " 'readthedocs': [],\n",
       " 'recommonmark': [{'name': 'commonmark', 'version': None},\n",
       "  {'name': 'docutils', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None}],\n",
       " 'six': [],\n",
       " 'snowballstemmer': [],\n",
       " 'Django': [{'name': 'asgiref', 'version': None},\n",
       "  {'name': 'sqlparse', 'version': None},\n",
       "  {'name': 'backports', 'version': None},\n",
       "  {'name': 'tzdata', 'version': None},\n",
       "  {'name': 'argon2', 'version': None},\n",
       "  {'name': 'bcrypt', 'version': None}],\n",
       " 'docopt': [],\n",
       " 'pickleshare': [{'name': 'pathlib2', 'version': None}],\n",
       " 'pathlib2': [{'name': 'six', 'version': None},\n",
       "  {'name': 'scandir', 'version': None},\n",
       "  {'name': 'typing', 'version': None}],\n",
       " 'prompt': [],\n",
       " 'traitlets': [{'name': 'myst', 'version': None},\n",
       "  {'name': 'pydata', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'argcomplete', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'appnope': [],\n",
       " 'black': [{'name': 'click', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'pathspec', 'version': None},\n",
       "  {'name': 'platformdirs', 'version': None},\n",
       "  {'name': 'tomli', 'version': None},\n",
       "  {'name': 'typed', 'version': None},\n",
       "  {'name': 'typing', 'version': None},\n",
       "  {'name': 'colorama', 'version': None},\n",
       "  {'name': 'aiohttp', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'tokenize', 'version': None},\n",
       "  {'name': 'uvloop', 'version': None}],\n",
       " 'pathspec': [],\n",
       " 'typed': [],\n",
       " 'aiohttp': [{'name': 'attrs', 'version': None},\n",
       "  {'name': 'charset', 'version': None},\n",
       "  {'name': 'multidict', 'version': None},\n",
       "  {'name': 'async', 'version': None},\n",
       "  {'name': 'yarl', 'version': None},\n",
       "  {'name': 'frozenlist', 'version': None},\n",
       "  {'name': 'aiosignal', 'version': None},\n",
       "  {'name': 'idna', 'version': None},\n",
       "  {'name': 'asynctest', 'version': None},\n",
       "  {'name': 'typing', 'version': None},\n",
       "  {'name': 'aiodns', 'version': None},\n",
       "  {'name': 'Brotli', 'version': None},\n",
       "  {'name': 'cchardet', 'version': None}],\n",
       " 'uvloop': [{'name': 'Cython', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'Sphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'flake8', 'version': None},\n",
       "  {'name': 'psutil', 'version': None},\n",
       "  {'name': 'pycodestyle', 'version': None},\n",
       "  {'name': 'pyOpenSSL', 'version': None},\n",
       "  {'name': 'mypy', 'version': None},\n",
       "  {'name': 'aiohttp', 'version': None}],\n",
       " 'ipykernel': [{'name': 'appnope', 'version': None},\n",
       "  {'name': 'comm', 'version': None},\n",
       "  {'name': 'debugpy', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'matplotlib', 'version': None},\n",
       "  {'name': 'nest', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'psutil', 'version': None},\n",
       "  {'name': 'pyzmq', 'version': None},\n",
       "  {'name': 'tornado', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'curio', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'trio', 'version': None},\n",
       "  {'name': 'myst', 'version': None},\n",
       "  {'name': 'pydata', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'pyqt5', 'version': None},\n",
       "  {'name': 'pyside6', 'version': None},\n",
       "  {'name': 'flaky', 'version': None},\n",
       "  {'name': 'ipyparallel', 'version': None},\n",
       "  {'name': 'pre', 'version': None}],\n",
       " 'comm': [{'name': 'traitlets', 'version': None},\n",
       "  {'name': 'black', 'version': None},\n",
       "  {'name': 'mdformat', 'version': None},\n",
       "  {'name': 'ruff', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'mypy', 'version': None}],\n",
       " 'debugpy': [],\n",
       " 'jupyter': [],\n",
       " 'nest': [{'name': 'matplotlib', 'version': None},\n",
       "  {'name': 'numpy', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'tqdm', 'version': None}],\n",
       " 'psutil': [{'name': 'ipaddress', 'version': None},\n",
       "  {'name': 'mock', 'version': None},\n",
       "  {'name': 'enum34', 'version': None},\n",
       "  {'name': 'pywin32', 'version': None},\n",
       "  {'name': 'wmi', 'version': None}],\n",
       " 'pyzmq': [],\n",
       " 'tornado': [],\n",
       " 'curio': [],\n",
       " 'trio': [{'name': 'attrs', 'version': None},\n",
       "  {'name': 'sortedcontainers', 'version': None},\n",
       "  {'name': 'async', 'version': None},\n",
       "  {'name': 'idna', 'version': None},\n",
       "  {'name': 'outcome', 'version': None},\n",
       "  {'name': 'sniffio', 'version': None},\n",
       "  {'name': 'cffi', 'version': None},\n",
       "  {'name': 'exceptiongroup', 'version': None}],\n",
       " 'pyqt5': [{'name': 'PyQt5', 'version': None}],\n",
       " 'pyside6': [{'name': 'shiboken6', 'version': None},\n",
       "  {'name': 'PySide6', 'version': None}],\n",
       " 'flaky': [],\n",
       " 'ipyparallel': [{'name': 'decorator', 'version': None},\n",
       "  {'name': 'entrypoints', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'psutil', 'version': None},\n",
       "  {'name': 'python', 'version': None},\n",
       "  {'name': 'pyzmq', 'version': None},\n",
       "  {'name': 'tornado', 'version': None},\n",
       "  {'name': 'tqdm', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'asv', 'version': None},\n",
       "  {'name': 'jupyterlab', 'version': None},\n",
       "  {'name': 'notebook', 'version': None},\n",
       "  {'name': 'retrolab', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'testpath', 'version': None}],\n",
       " 'docrepr': [{'name': 'docutils', 'version': None},\n",
       "  {'name': 'jinja2', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'matplotlib', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'numpy', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'playwright', 'version': None}],\n",
       " 'playwright': [{'name': 'greenlet', 'version': None},\n",
       "  {'name': 'pyee', 'version': None},\n",
       "  {'name': 'typing', 'version': None}],\n",
       " 'testpath': [{'name': 'pytest', 'version': None}],\n",
       " 'nbconvert': [{'name': 'beautifulsoup4', 'version': None},\n",
       "  {'name': 'bleach', 'version': None},\n",
       "  {'name': 'defusedxml', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'jinja2', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'jupyterlab', 'version': None},\n",
       "  {'name': 'markupsafe', 'version': None},\n",
       "  {'name': 'mistune', 'version': None},\n",
       "  {'name': 'nbclient', 'version': None},\n",
       "  {'name': 'nbformat', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'pandocfilters', 'version': None},\n",
       "  {'name': 'pygments', 'version': None},\n",
       "  {'name': 'tinycss2', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'nbconvert', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'myst', 'version': None},\n",
       "  {'name': 'nbsphinx', 'version': None},\n",
       "  {'name': 'pydata', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'pyqtwebengine', 'version': None},\n",
       "  {'name': 'tornado', 'version': None},\n",
       "  {'name': 'ipywidgets', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'pyppeteer', 'version': None}],\n",
       " 'bleach': [{'name': 'six', 'version': None},\n",
       "  {'name': 'webencodings', 'version': None},\n",
       "  {'name': 'tinycss2', 'version': None}],\n",
       " 'jupyterlab': [{'name': 'ipython', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'tornado', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'jupyterlab', 'version': None},\n",
       "  {'name': 'nbclassic', 'version': None},\n",
       "  {'name': 'notebook', 'version': None},\n",
       "  {'name': 'jinja2', 'version': None},\n",
       "  {'name': 'tomli', 'version': None},\n",
       "  {'name': 'check', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'virtualenv', 'version': None}],\n",
       " 'markupsafe': [],\n",
       " 'mistune': [],\n",
       " 'nbclient': [{'name': 'jupyter', 'version': None},\n",
       "  {'name': 'nbformat', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'autodoc', 'version': None},\n",
       "  {'name': 'mock', 'version': None},\n",
       "  {'name': 'moto', 'version': None},\n",
       "  {'name': 'myst', 'version': None},\n",
       "  {'name': 'nbclient', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'ipywidgets', 'version': None},\n",
       "  {'name': 'nbconvert', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'testpath', 'version': None},\n",
       "  {'name': 'xmltodict', 'version': None}],\n",
       " 'nbformat': [{'name': 'fastjsonschema', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'jsonschema', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'myst', 'version': None},\n",
       "  {'name': 'pydata', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'pep440', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'testpath', 'version': None}],\n",
       " 'pandocfilters': [],\n",
       " 'tinycss2': [{'name': 'webencodings', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'sphinx_rtd_theme', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'isort', 'version': None},\n",
       "  {'name': 'flake8', 'version': None}],\n",
       " 'nbsphinx': [{'name': 'docutils', 'version': None},\n",
       "  {'name': 'jinja2', 'version': None},\n",
       "  {'name': 'nbconvert', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'nbformat', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None}],\n",
       " 'pyqtwebengine': [{'name': 'PyQt5', 'version': None},\n",
       "  {'name': 'PyQtWebEngine', 'version': None}],\n",
       " 'pyppeteer': [{'name': 'appdirs', 'version': None},\n",
       "  {'name': 'importlib', 'version': None},\n",
       "  {'name': 'pyee', 'version': None},\n",
       "  {'name': 'tqdm', 'version': None},\n",
       "  {'name': 'urllib3', 'version': None},\n",
       "  {'name': 'websockets', 'version': None},\n",
       "  {'name': 'certifi', 'version': None}],\n",
       " 'notebook': [{'name': 'jinja2', 'version': None},\n",
       "  {'name': 'tornado', 'version': None},\n",
       "  {'name': 'pyzmq', 'version': None},\n",
       "  {'name': 'argon2', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'nbformat', 'version': None},\n",
       "  {'name': 'nbconvert', 'version': None},\n",
       "  {'name': 'nest', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'Send2Trash', 'version': None},\n",
       "  {'name': 'terminado', 'version': None},\n",
       "  {'name': 'prometheus', 'version': None},\n",
       "  {'name': 'nbclassic', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'nbsphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'myst', 'version': None},\n",
       "  {'name': 'json', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'testpath', 'version': None},\n",
       "  {'name': 'nbval', 'version': None},\n",
       "  {'name': 'selenium', 'version': None}],\n",
       " 'argon2': [],\n",
       " 'Send2Trash': [{'name': 'pyobjc', 'version': None},\n",
       "  {'name': 'pywin32', 'version': None}],\n",
       " 'terminado': [{'name': 'ptyprocess', 'version': None},\n",
       "  {'name': 'pywinpty', 'version': None},\n",
       "  {'name': 'tornado', 'version': None},\n",
       "  {'name': 'myst', 'version': None},\n",
       "  {'name': 'pydata', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'pre', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'prometheus': [],\n",
       " 'nbclassic': [{'name': 'jinja2', 'version': None},\n",
       "  {'name': 'tornado', 'version': None},\n",
       "  {'name': 'pyzmq', 'version': None},\n",
       "  {'name': 'argon2', 'version': None},\n",
       "  {'name': 'traitlets', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'nbformat', 'version': None},\n",
       "  {'name': 'notebook', 'version': None},\n",
       "  {'name': 'nbconvert', 'version': None},\n",
       "  {'name': 'nest', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'Send2Trash', 'version': None},\n",
       "  {'name': 'terminado', 'version': None},\n",
       "  {'name': 'prometheus', 'version': None},\n",
       "  {'name': 'sphinx', 'version': None},\n",
       "  {'name': 'nbsphinx', 'version': None},\n",
       "  {'name': 'sphinxcontrib', 'version': None},\n",
       "  {'name': 'myst', 'version': None},\n",
       "  {'name': 'json', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'coverage', 'version': None},\n",
       "  {'name': 'requests', 'version': None},\n",
       "  {'name': 'testpath', 'version': None},\n",
       "  {'name': 'nbval', 'version': None}],\n",
       " 'nbval': [{'name': 'pytest', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'nbformat', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'coverage', 'version': None}],\n",
       " 'selenium': [{'name': 'urllib3', 'version': None},\n",
       "  {'name': 'trio', 'version': None},\n",
       "  {'name': 'certifi', 'version': None}],\n",
       " 'qtconsole': [{'name': 'traitlets', 'version': None},\n",
       "  {'name': 'ipython', 'version': None},\n",
       "  {'name': 'jupyter', 'version': None},\n",
       "  {'name': 'pygments', 'version': None},\n",
       "  {'name': 'ipykernel', 'version': None},\n",
       "  {'name': 'qtpy', 'version': None},\n",
       "  {'name': 'pyzmq', 'version': None},\n",
       "  {'name': 'packaging', 'version': None},\n",
       "  {'name': 'Sphinx', 'version': None},\n",
       "  {'name': 'flaky', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'qtpy': [{'name': 'packaging', 'version': None},\n",
       "  {'name': 'pytest', 'version': None}],\n",
       " 'sphinxtesters': [],\n",
       " 'texext': [{'name': 'sphinx', 'version': None},\n",
       "  {'name': 'docutils', 'version': None},\n",
       "  {'name': 'sympy', 'version': None},\n",
       "  {'name': 'matplotlib', 'version': None},\n",
       "  {'name': 'pytest', 'version': None},\n",
       "  {'name': 'sphinxtesters', 'version': None}],\n",
       " 'sympy': [{'name': 'mpmath', 'version': None}],\n",
       " 'scripttester': [],\n",
       " 'pygraphviz': [],\n",
       " 'pydot': [{'name': 'pyparsing', 'version': None}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from olivia_finder.data_source.scrapers.pypi import PypiScraper\n",
    "pypi_scraper = PypiScraper()\n",
    "networkx_network = pypi_scraper.generate_package_dependency_network(\"networkx\")\n",
    "networkx_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 19:33:18 [   DEBUG] Added SSLProxies to proxy builders (logger.py:122)\n",
      "2023-03-24 19:33:18 [   DEBUG] Added FreeProxyList to proxy builders (logger.py:122)\n",
      "2023-03-24 19:33:18 [   DEBUG] Added GeonodeProxy to proxy builders (logger.py:122)\n",
      "2023-03-24 19:33:18 [   DEBUG] Starting new HTTPS connection (1): www.sslproxies.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:18 [   DEBUG] https://www.sslproxies.org:443 \"GET / HTTP/1.1\" 200 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:18 [   DEBUG] Found 100 proxies from SSLProxies (logger.py:122)\n",
      "2023-03-24 19:33:18 [   DEBUG] Starting new HTTPS connection (1): free-proxy-list.net:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:19 [   DEBUG] https://free-proxy-list.net:443 \"GET /anonymous-proxy.html HTTP/1.1\" 200 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:19 [   DEBUG] Found 100 proxies from FreeProxyList (logger.py:122)\n",
      "2023-03-24 19:33:19 [   DEBUG] Starting new HTTPS connection (1): proxylist.geonode.com:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:27 [   DEBUG] https://proxylist.geonode.com:443 \"GET /api/proxy-list?limit=500&page=1&sort_by=lastChecked&sort_type=desc HTTP/1.1\" 200 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:27 [   DEBUG] Found 500 proxies from GeonodeProxy (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Proxies len: 682 (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Proxy Handler initialized with 682 proxies (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Useragents loaded from file: /home/dnllns/Documentos/repositorios/olivia-finder/olivia_finder/olivia_finder/myrequests/data/useragents.txt (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Scraping package drake (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Proxy list rotated, using 159.138.130.126:8999, next will be 34.135.166.24:80 (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Using proxy: {'http': 'http://159.138.130.126:8999'} (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; rv:37.0) Gecko/20100101 Firefox/37.0 (logger.py:122)\n",
      "2023-03-24 19:33:27 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:28 [   DEBUG] https://cran.r-project.org:443 \"GET /package=drake HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:28 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/drake/index.html HTTP/1.1\" 200 2550 (connectionpool.py:452)\n",
      "2023-03-24 19:33:28 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] ScraperError: Error while obtaining the version of the package drake (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package drake (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] ScraperError: Error while obtaining the imports of the package drake (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Package drake scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Proxy list rotated, using 34.135.166.24:80, next will be 170.254.255.240:45816 (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Using proxy: {'http': 'http://34.135.166.24:80'} (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.116 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:28 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:28 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:28 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] The package R, as dependency of drake does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Scraping package base64url (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Proxy list rotated, using 170.254.255.240:45816, next will be 186.251.255.101:31337 (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Using proxy: {'http': 'http://170.254.255.240:45816'} (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.76.4 (KHTML, like Gecko) Version/7.0.4 Safari/537.76.4 (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:28 [   DEBUG] https://cran.r-project.org:443 \"GET /package=base64url HTTP/1.1\" 303 336 (connectionpool.py:452)\n",
      "2023-03-24 19:33:28 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/base64url/index.html HTTP/1.1\" 200 1923 (connectionpool.py:452)\n",
      "2023-03-24 19:33:28 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] ScraperError: Error while obtaining the version of the package base64url (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package base64url (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] The package base64url, as dependency of drake does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Scraping package digest (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Proxy list rotated, using 186.251.255.101:31337, next will be 177.220.243.130:4153 (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Using proxy: {'http': 'http://186.251.255.101:31337'} (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; EIE10;ENUSMSN; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:33:28 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:29 [   DEBUG] https://cran.r-project.org:443 \"GET /package=digest HTTP/1.1\" 303 333 (connectionpool.py:452)\n",
      "2023-03-24 19:33:29 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/digest/index.html HTTP/1.1\" 200 5910 (connectionpool.py:452)\n",
      "2023-03-24 19:33:29 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] ScraperError: Error while obtaining the version of the package digest (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package digest (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] ScraperError: Error while obtaining the imports of the package digest (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Package digest scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Proxy list rotated, using 177.220.243.130:4153, next will be 121.1.41.162:111 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Using proxy: {'http': 'http://177.220.243.130:4153'} (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1; rv:30.0) Gecko/20100101 Firefox/30.0 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:29 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:29 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:29 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] The package R, as dependency of digest does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Scraping package utils (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Proxy list rotated, using 121.1.41.162:111, next will be 89.25.23.212:4153 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Using proxy: {'http': 'http://121.1.41.162:111'} (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:29 [   DEBUG] https://cran.r-project.org:443 \"GET /package=utils HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:29 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/utils/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:29 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] ScraperError: Package utils not found (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] The package utils, as dependency of digest does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Scraping package igraph (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Proxy list rotated, using 89.25.23.212:4153, next will be 218.1.200.232:57114 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Using proxy: {'http': 'http://89.25.23.212:4153'} (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) GSA/6.0.51363 Mobile/11D257 Safari/9537.53 (logger.py:122)\n",
      "2023-03-24 19:33:29 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:30 [   DEBUG] https://cran.r-project.org:443 \"GET /package=igraph HTTP/1.1\" 303 333 (connectionpool.py:452)\n",
      "2023-03-24 19:33:30 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/igraph/index.html HTTP/1.1\" 200 11876 (connectionpool.py:452)\n",
      "2023-03-24 19:33:30 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] ScraperError: Error while obtaining the version of the package igraph (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package igraph (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] ScraperError: Error while obtaining the imports of the package igraph (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Package igraph scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Scraping package methods (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Proxy list rotated, using 218.1.200.232:57114, next will be 51.91.20.206:20959 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Using proxy: {'http': 'http://218.1.200.232:57114'} (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.68 Mobile/12H321 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:30 [   DEBUG] https://cran.r-project.org:443 \"GET /package=methods HTTP/1.1\" 303 334 (connectionpool.py:452)\n",
      "2023-03-24 19:33:30 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/methods/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:30 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] ScraperError: Package methods not found (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] The package methods, as dependency of igraph does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Scraping package graphics (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Proxy list rotated, using 51.91.20.206:20959, next will be 51.89.117.242:45436 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Using proxy: {'http': 'http://51.91.20.206:20959'} (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:30 [   DEBUG] https://cran.r-project.org:443 \"GET /package=graphics HTTP/1.1\" 303 335 (connectionpool.py:452)\n",
      "2023-03-24 19:33:30 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/graphics/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:30 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] ScraperError: Package graphics not found (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] The package graphics, as dependency of igraph does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Scraping package grDevices (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Proxy list rotated, using 51.89.117.242:45436, next will be 65.109.84.104:80 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Using proxy: {'http': 'http://51.89.117.242:45436'} (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Linux i686; rv:38.0) Gecko/20100101 Firefox/38.0 (logger.py:122)\n",
      "2023-03-24 19:33:30 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:30 [   DEBUG] https://cran.r-project.org:443 \"GET /package=grDevices HTTP/1.1\" 303 336 (connectionpool.py:452)\n",
      "2023-03-24 19:33:31 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/grDevices/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:31 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] ScraperError: Package grDevices not found (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] The package grDevices, as dependency of igraph does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Scraping package magrittr (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Proxy list rotated, using 65.109.84.104:80, next will be 193.8.87.43:4444 (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Using proxy: {'http': 'http://65.109.84.104:80'} (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.0.1; VS985 4G Build/LRX21Y) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Mobile Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:31 [   DEBUG] https://cran.r-project.org:443 \"GET /package=magrittr HTTP/1.1\" 303 335 (connectionpool.py:452)\n",
      "2023-03-24 19:33:31 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/magrittr/index.html HTTP/1.1\" 200 24013 (connectionpool.py:452)\n",
      "2023-03-24 19:33:31 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] ScraperError: Error while obtaining the version of the package magrittr (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package magrittr (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] ScraperError: Error while obtaining the imports of the package magrittr (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] The package magrittr, as dependency of igraph does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Scraping package Matrix (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Proxy list rotated, using 193.8.87.43:4444, next will be 130.41.109.158:8080 (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Using proxy: {'http': 'http://193.8.87.43:4444'} (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:31 [   DEBUG] https://cran.r-project.org:443 \"GET /package=Matrix HTTP/1.1\" 303 333 (connectionpool.py:452)\n",
      "2023-03-24 19:33:31 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/Matrix/index.html HTTP/1.1\" 200 18199 (connectionpool.py:452)\n",
      "2023-03-24 19:33:31 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] ScraperError: Error while obtaining the version of the package Matrix (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package Matrix (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] ScraperError: Error while obtaining the imports of the package Matrix (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Package Matrix scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Proxy list rotated, using 130.41.109.158:8080, next will be 65.108.230.238:45977 (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Using proxy: {'http': 'http://130.41.109.158:8080'} (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; MSBrowserIE; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:33:31 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:32 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:32 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:32 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] The package R, as dependency of Matrix does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Scraping package methods (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Proxy list rotated, using 65.108.230.238:45977, next will be 116.68.207.233:1080 (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Using proxy: {'http': 'http://65.108.230.238:45977'} (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; TNJB; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:32 [   DEBUG] https://cran.r-project.org:443 \"GET /package=methods HTTP/1.1\" 303 334 (connectionpool.py:452)\n",
      "2023-03-24 19:33:32 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/methods/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:32 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] ScraperError: Package methods not found (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] The package methods, as dependency of Matrix does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Scraping package graphics (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Proxy list rotated, using 116.68.207.233:1080, next will be 110.8.238.133:1080 (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Using proxy: {'http': 'http://116.68.207.233:1080'} (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; MAARJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:32 [   DEBUG] https://cran.r-project.org:443 \"GET /package=graphics HTTP/1.1\" 303 335 (connectionpool.py:452)\n",
      "2023-03-24 19:33:32 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/graphics/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:32 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] ScraperError: Package graphics not found (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] The package graphics, as dependency of Matrix does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Scraping package grid (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Proxy list rotated, using 110.8.238.133:1080, next will be 180.180.12.51:4145 (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Using proxy: {'http': 'http://110.8.238.133:1080'} (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_0 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12A365 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:33:32 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:32 [   DEBUG] https://cran.r-project.org:443 \"GET /package=grid HTTP/1.1\" 303 331 (connectionpool.py:452)\n",
      "2023-03-24 19:33:33 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/grid/index.html HTTP/1.1\" 200 459 (connectionpool.py:452)\n",
      "2023-03-24 19:33:33 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] ScraperError: Package grid not found (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] The package grid, as dependency of Matrix does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Scraping package lattice (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Proxy list rotated, using 180.180.12.51:4145, next will be 173.82.140.16:80 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Using proxy: {'http': 'http://180.180.12.51:4145'} (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; EIE10;ENUSMCM; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:33 [   DEBUG] https://cran.r-project.org:443 \"GET /package=lattice HTTP/1.1\" 303 334 (connectionpool.py:452)\n",
      "2023-03-24 19:33:33 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/lattice/index.html HTTP/1.1\" 200 8663 (connectionpool.py:452)\n",
      "2023-03-24 19:33:33 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] ScraperError: Error while obtaining the version of the package lattice (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package lattice (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] ScraperError: Error while obtaining the imports of the package lattice (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Package lattice scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Proxy list rotated, using 173.82.140.16:80, next will be 177.71.77.202:20183 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Using proxy: {'http': 'http://173.82.140.16:80'} (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; rv:40.0) Gecko/20100101 Firefox/40.0 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:33 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:33 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:33 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] The package R, as dependency of lattice does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Scraping package grid (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Proxy list rotated, using 177.71.77.202:20183, next will be 43.153.30.185:443 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Using proxy: {'http': 'http://177.71.77.202:20183'} (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.125 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:33 [   DEBUG] https://cran.r-project.org:443 \"GET /package=grid HTTP/1.1\" 303 331 (connectionpool.py:452)\n",
      "2023-03-24 19:33:33 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/grid/index.html HTTP/1.1\" 200 459 (connectionpool.py:452)\n",
      "2023-03-24 19:33:33 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] ScraperError: Package grid not found (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] The package grid, as dependency of lattice does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Scraping package grDevices (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Proxy list rotated, using 43.153.30.185:443, next will be 64.225.4.63:9998 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Using proxy: {'http': 'http://43.153.30.185:443'} (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:33 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:34 [   DEBUG] https://cran.r-project.org:443 \"GET /package=grDevices HTTP/1.1\" 303 336 (connectionpool.py:452)\n",
      "2023-03-24 19:33:34 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/grDevices/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:34 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] ScraperError: Package grDevices not found (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] The package grDevices, as dependency of lattice does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Scraping package graphics (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Proxy list rotated, using 64.225.4.63:9998, next will be 170.254.255.227:45816 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Using proxy: {'http': 'http://64.225.4.63:9998'} (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/536.30.1 (KHTML, like Gecko) Version/6.0.5 Safari/536.30.1 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:34 [   DEBUG] https://cran.r-project.org:443 \"GET /package=graphics HTTP/1.1\" 303 335 (connectionpool.py:452)\n",
      "2023-03-24 19:33:34 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/graphics/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:34 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] ScraperError: Package graphics not found (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] The package graphics, as dependency of lattice does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Scraping package stats (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Proxy list rotated, using 170.254.255.227:45816, next will be 103.70.206.17:59311 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Using proxy: {'http': 'http://170.254.255.227:45816'} (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 AOL/9.7 AOLBuild/4343.4049.US Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:34 [   DEBUG] https://cran.r-project.org:443 \"GET /package=stats HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:34 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/stats/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:34 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] ScraperError: Package stats not found (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] The package stats, as dependency of lattice does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Scraping package utils (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Proxy list rotated, using 103.70.206.17:59311, next will be 172.98.20.80:10001 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Using proxy: {'http': 'http://103.70.206.17:59311'} (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 Safari/537.36 SE 2.X MetaSr 1.0 (logger.py:122)\n",
      "2023-03-24 19:33:34 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:35 [   DEBUG] https://cran.r-project.org:443 \"GET /package=utils HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:35 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/utils/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:35 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] ScraperError: Package utils not found (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] The package utils, as dependency of lattice does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Scraping package stats (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Proxy list rotated, using 172.98.20.80:10001, next will be 62.171.156.226:3128 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Using proxy: {'http': 'http://172.98.20.80:10001'} (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 7_1_1 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) CriOS/45.0.2454.68 Mobile/11D201 Safari/9537.53 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:35 [   DEBUG] https://cran.r-project.org:443 \"GET /package=stats HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:35 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/stats/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:35 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] ScraperError: Package stats not found (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] The package stats, as dependency of Matrix does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Scraping package utils (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Proxy list rotated, using 62.171.156.226:3128, next will be 62.33.235.41:4153 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Using proxy: {'http': 'http://62.171.156.226:3128'} (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:35 [   DEBUG] https://cran.r-project.org:443 \"GET /package=utils HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:35 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/utils/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:35 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] ScraperError: Package utils not found (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] The package utils, as dependency of Matrix does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Scraping package pkgconfig (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Proxy list rotated, using 62.33.235.41:4153, next will be 70.63.165.22:32940 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Using proxy: {'http': 'http://62.33.235.41:4153'} (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 5.0.2; LG-V410/V41020c Build/LRX22G) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/34.0.1847.118 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:35 [   DEBUG] https://cran.r-project.org:443 \"GET /package=pkgconfig HTTP/1.1\" 303 336 (connectionpool.py:452)\n",
      "2023-03-24 19:33:35 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/pkgconfig/index.html HTTP/1.1\" 200 1415 (connectionpool.py:452)\n",
      "2023-03-24 19:33:35 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] ScraperError: Error while obtaining the version of the package pkgconfig (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package pkgconfig (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] The package pkgconfig, as dependency of igraph does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Scraping package rlang (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Proxy list rotated, using 70.63.165.22:32940, next will be 142.93.250.71:7497 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Using proxy: {'http': 'http://70.63.165.22:32940'} (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:35 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:36 [   DEBUG] https://cran.r-project.org:443 \"GET /package=rlang HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:36 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/rlang/index.html HTTP/1.1\" 200 20662 (connectionpool.py:452)\n",
      "2023-03-24 19:33:36 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] ScraperError: Error while obtaining the version of the package rlang (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package rlang (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] ScraperError: Error while obtaining the imports of the package rlang (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Package rlang scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Proxy list rotated, using 142.93.250.71:7497, next will be 202.159.19.213:443 (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Using proxy: {'http': 'http://142.93.250.71:7497'} (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.146 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:36 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:36 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:36 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] The package R, as dependency of rlang does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Scraping package utils (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Proxy list rotated, using 202.159.19.213:443, next will be 139.162.196.67:60586 (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Using proxy: {'http': 'http://202.159.19.213:443'} (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.154 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:36 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:36 [   DEBUG] https://cran.r-project.org:443 \"GET /package=utils HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:37 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/utils/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:37 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] ScraperError: Package utils not found (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] The package utils, as dependency of rlang does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Scraping package stats (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Proxy list rotated, using 139.162.196.67:60586, next will be 131.108.118.27:2022 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Using proxy: {'http': 'http://139.162.196.67:60586'} (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.132 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:37 [   DEBUG] https://cran.r-project.org:443 \"GET /package=stats HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:37 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/stats/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:37 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] ScraperError: Package stats not found (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] The package stats, as dependency of igraph does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Scraping package utils (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Proxy list rotated, using 131.108.118.27:2022, next will be 64.225.8.132:9979 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Using proxy: {'http': 'http://131.108.118.27:2022'} (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.125 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:37 [   DEBUG] https://cran.r-project.org:443 \"GET /package=utils HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:37 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/utils/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:37 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] ScraperError: Package utils not found (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] The package utils, as dependency of igraph does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Scraping package methods (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Proxy list rotated, using 64.225.8.132:9979, next will be 186.251.253.14:31337 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Using proxy: {'http': 'http://64.225.8.132:9979'} (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.2.2; Le Pan TC802A Build/JDQ39) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:37 [   DEBUG] https://cran.r-project.org:443 \"GET /package=methods HTTP/1.1\" 303 334 (connectionpool.py:452)\n",
      "2023-03-24 19:33:37 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/methods/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:37 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] ScraperError: Package methods not found (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] The package methods, as dependency of drake does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Scraping package parallel (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Proxy list rotated, using 186.251.253.14:31337, next will be 111.199.70.169:1080 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Using proxy: {'http': 'http://186.251.253.14:31337'} (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.2; QMV7B Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:37 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:38 [   DEBUG] https://cran.r-project.org:443 \"GET /package=parallel HTTP/1.1\" 303 335 (connectionpool.py:452)\n",
      "2023-03-24 19:33:38 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/parallel/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:38 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] ScraperError: Package parallel not found (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] The package parallel, as dependency of drake does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Scraping package storr (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Proxy list rotated, using 111.199.70.169:1080, next will be 89.218.5.108:50733 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Using proxy: {'http': 'http://111.199.70.169:1080'} (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 AOL/9.7 AOLBuild/4343.4043.US Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:38 [   DEBUG] https://cran.r-project.org:443 \"GET /package=storr HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:38 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/storr/index.html HTTP/1.1\" 200 1794 (connectionpool.py:452)\n",
      "2023-03-24 19:33:38 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] ScraperError: Error while obtaining the version of the package storr (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package storr (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] ScraperError: Error while obtaining the imports of the package storr (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Package storr scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Proxy list rotated, using 89.218.5.108:50733, next will be 185.162.93.62:9050 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Using proxy: {'http': 'http://89.218.5.108:50733'} (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:38 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:38 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:38 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] The package R, as dependency of storr does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Scraping package R6 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Proxy list rotated, using 185.162.93.62:9050, next will be 36.92.25.98:5678 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Using proxy: {'http': 'http://185.162.93.62:9050'} (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0 (logger.py:122)\n",
      "2023-03-24 19:33:38 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:39 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R6 HTTP/1.1\" 303 329 (connectionpool.py:452)\n",
      "2023-03-24 19:33:39 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R6/index.html HTTP/1.1\" 200 6129 (connectionpool.py:452)\n",
      "2023-03-24 19:33:39 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] ScraperError: Error while obtaining the version of the package R6 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package R6 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] ScraperError: Error while obtaining the imports of the package R6 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] The package R6, as dependency of storr does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Scraping package tidyselect (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Proxy list rotated, using 36.92.25.98:5678, next will be 50.47.75.216:5678 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Using proxy: {'http': 'http://36.92.25.98:5678'} (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:39 [   DEBUG] https://cran.r-project.org:443 \"GET /package=tidyselect HTTP/1.1\" 303 337 (connectionpool.py:452)\n",
      "2023-03-24 19:33:39 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/tidyselect/index.html HTTP/1.1\" 200 5595 (connectionpool.py:452)\n",
      "2023-03-24 19:33:39 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] ScraperError: Error while obtaining the version of the package tidyselect (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package tidyselect (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] ScraperError: Error while obtaining the imports of the package tidyselect (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Package tidyselect scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Proxy list rotated, using 50.47.75.216:5678, next will be 186.251.255.49:31337 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Using proxy: {'http': 'http://50.47.75.216:5678'} (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.2; GT-N5110 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.84 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:39 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:39 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:39 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] The package R, as dependency of tidyselect does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Scraping package cli (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Proxy list rotated, using 186.251.255.49:31337, next will be 1.0.0.13:80 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Using proxy: {'http': 'http://186.251.255.49:31337'} (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53 (logger.py:122)\n",
      "2023-03-24 19:33:39 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:39 [   DEBUG] https://cran.r-project.org:443 \"GET /package=cli HTTP/1.1\" 303 330 (connectionpool.py:452)\n",
      "2023-03-24 19:33:40 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/cli/index.html HTTP/1.1\" 200 6133 (connectionpool.py:452)\n",
      "2023-03-24 19:33:40 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] ScraperError: Error while obtaining the version of the package cli (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package cli (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] ScraperError: Error while obtaining the imports of the package cli (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Package cli scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Proxy list rotated, using 1.0.0.13:80, next will be 85.132.8.219:4153 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Using proxy: {'http': 'http://1.0.0.13:80'} (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Using user agent: Mozilla/5.0 (Linux; Android 4.4.3; KFASWI Build/KTU84M) AppleWebKit/537.36 (KHTML, like Gecko) Silk/44.1.81 like Chrome/44.0.2403.128 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:40 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:40 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:40 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] The package R, as dependency of cli does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Scraping package utils (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Proxy list rotated, using 85.132.8.219:4153, next will be 189.2.127.244:4153 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Using proxy: {'http': 'http://85.132.8.219:4153'} (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/7.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; InfoPath.3; GWX:QUALIFIED) (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:40 [   DEBUG] https://cran.r-project.org:443 \"GET /package=utils HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:40 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/utils/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:40 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] ScraperError: Package utils not found (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] The package utils, as dependency of cli does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Scraping package glue (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Proxy list rotated, using 189.2.127.244:4153, next will be 5.135.164.151:30256 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Using proxy: {'http': 'http://189.2.127.244:4153'} (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/600.6.3 (KHTML, like Gecko) Version/8.0.6 Safari/600.6.3 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:40 [   DEBUG] https://cran.r-project.org:443 \"GET /package=glue HTTP/1.1\" 303 331 (connectionpool.py:452)\n",
      "2023-03-24 19:33:40 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/glue/index.html HTTP/1.1\" 200 7933 (connectionpool.py:452)\n",
      "2023-03-24 19:33:40 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] ScraperError: Error while obtaining the version of the package glue (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package glue (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] ScraperError: Error while obtaining the imports of the package glue (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Package glue scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Proxy list rotated, using 5.135.164.151:30256, next will be 103.205.128.33:4303 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Using proxy: {'http': 'http://5.135.164.151:30256'} (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.124 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:40 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:41 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:41 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:41 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] The package R, as dependency of glue does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Scraping package methods (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Proxy list rotated, using 103.205.128.33:4303, next will be 152.67.10.190:8100 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Using proxy: {'http': 'http://103.205.128.33:4303'} (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.8.9 (KHTML, like Gecko) Version/7.1.8 Safari/537.85.17 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:41 [   DEBUG] https://cran.r-project.org:443 \"GET /package=methods HTTP/1.1\" 303 334 (connectionpool.py:452)\n",
      "2023-03-24 19:33:41 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/methods/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:41 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] ScraperError: Package methods not found (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] The package methods, as dependency of glue does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Scraping package lifecycle (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Proxy list rotated, using 152.67.10.190:8100, next will be 118.97.47.250:55443 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Using proxy: {'http': 'http://152.67.10.190:8100'} (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:41 [   DEBUG] https://cran.r-project.org:443 \"GET /package=lifecycle HTTP/1.1\" 303 336 (connectionpool.py:452)\n",
      "2023-03-24 19:33:41 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/lifecycle/index.html HTTP/1.1\" 200 4497 (connectionpool.py:452)\n",
      "2023-03-24 19:33:41 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] ScraperError: Error while obtaining the version of the package lifecycle (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package lifecycle (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] ScraperError: Error while obtaining the imports of the package lifecycle (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Package lifecycle scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Proxy list rotated, using 118.97.47.250:55443, next will be 88.119.49.4:4153 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Using proxy: {'http': 'http://118.97.47.250:55443'} (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Using user agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25 (logger.py:122)\n",
      "2023-03-24 19:33:41 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:42 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:42 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:42 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] The package R, as dependency of lifecycle does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Scraping package vctrs (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Proxy list rotated, using 88.119.49.4:4153, next will be 103.66.233.161:4145 (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Using proxy: {'http': 'http://88.119.49.4:4153'} (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:42 [   DEBUG] https://cran.r-project.org:443 \"GET /package=vctrs HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:42 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/vctrs/index.html HTTP/1.1\" 200 3946 (connectionpool.py:452)\n",
      "2023-03-24 19:33:42 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] ScraperError: Error while obtaining the version of the package vctrs (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package vctrs (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] ScraperError: Error while obtaining the imports of the package vctrs (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Package vctrs scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Proxy list rotated, using 103.66.233.161:4145, next will be 47.98.151.6:16144 (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Using proxy: {'http': 'http://103.66.233.161:4145'} (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.107 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:42 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:42 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:42 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] The package R, as dependency of vctrs does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Scraping package withr (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Proxy list rotated, using 47.98.151.6:16144, next will be 201.184.155.20:5678 (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Using proxy: {'http': 'http://47.98.151.6:16144'} (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Using user agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E) (logger.py:122)\n",
      "2023-03-24 19:33:42 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:42 [   DEBUG] https://cran.r-project.org:443 \"GET /package=withr HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:42 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/withr/index.html HTTP/1.1\" 200 6356 (connectionpool.py:452)\n",
      "2023-03-24 19:33:42 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] ScraperError: Error while obtaining the version of the package withr (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package withr (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] ScraperError: Error while obtaining the imports of the package withr (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Package withr scraped successfully (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Scraping package R (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Proxy list rotated, using 201.184.155.20:5678, next will be 203.98.76.64:5678 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Using proxy: {'http': 'http://201.184.155.20:5678'} (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko; Google Web Preview) Chrome/27.0.1453 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:43 [   DEBUG] https://cran.r-project.org:443 \"GET /package=R HTTP/1.1\" 303 328 (connectionpool.py:452)\n",
      "2023-03-24 19:33:43 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/R/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:43 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] ScraperError: Package R not found (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] The package R, as dependency of withr does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Scraping package graphics (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Proxy list rotated, using 203.98.76.64:5678, next will be 198.59.191.234:8080 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Using proxy: {'http': 'http://203.98.76.64:5678'} (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; MASMJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:43 [   DEBUG] https://cran.r-project.org:443 \"GET /package=graphics HTTP/1.1\" 303 335 (connectionpool.py:452)\n",
      "2023-03-24 19:33:43 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/graphics/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:43 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] ScraperError: Package graphics not found (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] The package graphics, as dependency of withr does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Scraping package grDevices (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Proxy list rotated, using 198.59.191.234:8080, next will be 212.69.12.81:1080 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Using proxy: {'http': 'http://198.59.191.234:8080'} (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Using user agent: Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:38.0) Gecko/20100101 Firefox/38.0 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:43 [   DEBUG] https://cran.r-project.org:443 \"GET /package=grDevices HTTP/1.1\" 303 336 (connectionpool.py:452)\n",
      "2023-03-24 19:33:43 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/grDevices/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:43 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] ScraperError: Package grDevices not found (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] The package grDevices, as dependency of withr does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Scraping package stats (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Proxy list rotated, using 212.69.12.81:1080, next will be 50.96.204.5:18351 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Using proxy: {'http': 'http://212.69.12.81:1080'} (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Using user agent: Mozilla/5.0 (iPad; CPU OS 8_0_2 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12A405 Safari/600.1.4 (logger.py:122)\n",
      "2023-03-24 19:33:43 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:44 [   DEBUG] https://cran.r-project.org:443 \"GET /package=stats HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:44 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/stats/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:44 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] ScraperError: Package stats not found (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] The package stats, as dependency of withr does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Scraping package txtq (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Proxy list rotated, using 50.96.204.5:18351, next will be 103.47.93.201:1080 (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Using proxy: {'http': 'http://50.96.204.5:18351'} (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Using user agent: Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; MATBJS; rv:11.0) like Gecko (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:44 [   DEBUG] https://cran.r-project.org:443 \"GET /package=txtq HTTP/1.1\" 303 331 (connectionpool.py:452)\n",
      "2023-03-24 19:33:44 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/txtq/index.html HTTP/1.1\" 200 1757 (connectionpool.py:452)\n",
      "2023-03-24 19:33:44 [   DEBUG] Response status code: 200 (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] ScraperError: Error while obtaining the version of the package txtq (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] ScraperError: Error while obtaining the dependencies of the package txtq (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] The package txtq, as dependency of drake does not exist in the data source CRAN Scraper (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Scraping package utils (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Getting next proxy (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Proxy list rotated, using 103.47.93.201:1080, next will be 43.153.81.168:443 (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Using proxy: {'http': 'http://103.47.93.201:1080'} (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Getting next useragent (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Using user agent: Mozilla/5.0 (X11; CrOS x86_64 6457.107.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36 (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] Starting new HTTPS connection (1): cran.r-project.org:443 (connectionpool.py:973)\n",
      "2023-03-24 19:33:44 [   DEBUG] https://cran.r-project.org:443 \"GET /package=utils HTTP/1.1\" 303 332 (connectionpool.py:452)\n",
      "2023-03-24 19:33:44 [   DEBUG] https://cran.r-project.org:443 \"GET /web/packages/utils/index.html HTTP/1.1\" 404 None (connectionpool.py:452)\n",
      "2023-03-24 19:33:44 [   DEBUG] Response status code: 404 (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] ScraperError: Package utils not found (logger.py:122)\n",
      "2023-03-24 19:33:44 [   DEBUG] The package utils, as dependency of drake does not exist in the data source CRAN Scraper (logger.py:122)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'drake': [{'name': 'R', 'version': 'â¥ 3.3.0'},\n",
       "  {'name': 'base64url', 'version': ''},\n",
       "  {'name': 'digest', 'version': 'â¥ 0.6.21'},\n",
       "  {'name': 'igraph', 'version': ''},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'parallel', 'version': ''},\n",
       "  {'name': 'rlang', 'version': 'â¥ 0.2.0'},\n",
       "  {'name': 'storr', 'version': 'â¥ 1.1.0'},\n",
       "  {'name': 'tidyselect', 'version': 'â¥ 1.0.0'},\n",
       "  {'name': 'txtq', 'version': 'â¥ 0.2.3'},\n",
       "  {'name': 'utils', 'version': ''},\n",
       "  {'name': 'vctrs', 'version': 'â¥ 0.2.0'}],\n",
       " 'digest': [{'name': 'R', 'version': 'â¥ 3.3.0'},\n",
       "  {'name': 'utils', 'version': ''}],\n",
       " 'igraph': [{'name': 'methods', 'version': ''},\n",
       "  {'name': 'graphics', 'version': ''},\n",
       "  {'name': 'grDevices', 'version': ''},\n",
       "  {'name': 'magrittr', 'version': ''},\n",
       "  {'name': 'Matrix', 'version': ''},\n",
       "  {'name': 'pkgconfig', 'version': 'â¥ 2.0.0'},\n",
       "  {'name': 'rlang', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''}],\n",
       " 'Matrix': [{'name': 'R', 'version': 'â¥ 3.5.0'},\n",
       "  {'name': 'methods', 'version': ''},\n",
       "  {'name': 'graphics', 'version': ''},\n",
       "  {'name': 'grid', 'version': ''},\n",
       "  {'name': 'lattice', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''}],\n",
       " 'lattice': [{'name': 'R', 'version': 'â¥ 3.0.0'},\n",
       "  {'name': 'grid', 'version': ''},\n",
       "  {'name': 'grDevices', 'version': ''},\n",
       "  {'name': 'graphics', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''},\n",
       "  {'name': 'utils', 'version': ''}],\n",
       " 'rlang': [{'name': 'R', 'version': 'â¥ 3.5.0'},\n",
       "  {'name': 'utils', 'version': ''}],\n",
       " 'storr': [{'name': 'R', 'version': 'â¥ 3.3.0'},\n",
       "  {'name': 'R6', 'version': 'â¥ 2.1.0'},\n",
       "  {'name': 'digest', 'version': ''}],\n",
       " 'tidyselect': [{'name': 'R', 'version': 'â¥ 3.4'},\n",
       "  {'name': 'cli', 'version': 'â¥ 3.3.0'},\n",
       "  {'name': 'glue', 'version': 'â¥ 1.3.0'},\n",
       "  {'name': 'lifecycle', 'version': 'â¥ 1.0.3'},\n",
       "  {'name': 'rlang', 'version': 'â¥ 1.0.4'},\n",
       "  {'name': 'vctrs', 'version': 'â¥ 0.4.1'},\n",
       "  {'name': 'withr', 'version': ''}],\n",
       " 'cli': [{'name': 'R', 'version': 'â¥ 3.4'}, {'name': 'utils', 'version': ''}],\n",
       " 'glue': [{'name': 'R', 'version': 'â¥ 3.4'},\n",
       "  {'name': 'methods', 'version': ''}],\n",
       " 'lifecycle': [{'name': 'R', 'version': 'â¥ 3.4'},\n",
       "  {'name': 'cli', 'version': 'â¥ 3.4.0'},\n",
       "  {'name': 'glue', 'version': ''},\n",
       "  {'name': 'rlang', 'version': 'â¥ 1.0.6'}],\n",
       " 'vctrs': [{'name': 'R', 'version': 'â¥ 3.5.0'},\n",
       "  {'name': 'cli', 'version': 'â¥ 3.4.0'},\n",
       "  {'name': 'glue', 'version': ''},\n",
       "  {'name': 'lifecycle', 'version': 'â¥ 1.0.3'},\n",
       "  {'name': 'rlang', 'version': 'â¥ 1.1.0'}],\n",
       " 'withr': [{'name': 'R', 'version': 'â¥ 3.2.0'},\n",
       "  {'name': 'graphics', 'version': ''},\n",
       "  {'name': 'grDevices', 'version': ''},\n",
       "  {'name': 'stats', 'version': ''}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from olivia_finder.data_source.scrapers.cran import CranScraper\n",
    "cran_scraper = CranScraper()\n",
    "drake_network = cran_scraper.generate_package_dependency_network(\"drake\")\n",
    "drake_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'express': [{'name': 'accepts', 'version': '~1.3.8'},\n",
       "  {'name': 'array-flatten', 'version': '1.1.1'},\n",
       "  {'name': 'body-parser', 'version': '1.20.1'},\n",
       "  {'name': 'content-disposition', 'version': '0.5.4'},\n",
       "  {'name': 'content-type', 'version': '~1.0.4'},\n",
       "  {'name': 'cookie', 'version': '0.5.0'},\n",
       "  {'name': 'cookie-signature', 'version': '1.0.6'},\n",
       "  {'name': 'debug', 'version': '2.6.9'},\n",
       "  {'name': 'depd', 'version': '2.0.0'},\n",
       "  {'name': 'encodeurl', 'version': '~1.0.2'},\n",
       "  {'name': 'escape-html', 'version': '~1.0.3'},\n",
       "  {'name': 'etag', 'version': '~1.8.1'},\n",
       "  {'name': 'finalhandler', 'version': '1.2.0'},\n",
       "  {'name': 'fresh', 'version': '0.5.2'},\n",
       "  {'name': 'http-errors', 'version': '2.0.0'},\n",
       "  {'name': 'merge-descriptors', 'version': '1.0.1'},\n",
       "  {'name': 'methods', 'version': '~1.1.2'},\n",
       "  {'name': 'on-finished', 'version': '2.4.1'},\n",
       "  {'name': 'parseurl', 'version': '~1.3.3'},\n",
       "  {'name': 'path-to-regexp', 'version': '0.1.7'},\n",
       "  {'name': 'proxy-addr', 'version': '~2.0.7'},\n",
       "  {'name': 'qs', 'version': '6.11.0'},\n",
       "  {'name': 'range-parser', 'version': '~1.2.1'},\n",
       "  {'name': 'safe-buffer', 'version': '5.2.1'},\n",
       "  {'name': 'send', 'version': '0.18.0'},\n",
       "  {'name': 'serve-static', 'version': '1.15.0'},\n",
       "  {'name': 'setprototypeof', 'version': '1.2.0'},\n",
       "  {'name': 'statuses', 'version': '2.0.1'},\n",
       "  {'name': 'type-is', 'version': '~1.6.18'},\n",
       "  {'name': 'utils-merge', 'version': '1.0.1'},\n",
       "  {'name': 'vary', 'version': '~1.1.2'}],\n",
       " 'accepts': [{'name': 'mime-types', 'version': '~2.1.34'},\n",
       "  {'name': 'negotiator', 'version': '0.6.3'}],\n",
       " 'mime-types': [{'name': 'mime-db', 'version': '1.52.0'}],\n",
       " 'mime-db': [],\n",
       " 'negotiator': [],\n",
       " 'array-flatten': [],\n",
       " 'body-parser': [{'name': 'bytes', 'version': '3.1.2'},\n",
       "  {'name': 'content-type', 'version': '~1.0.5'},\n",
       "  {'name': 'debug', 'version': '2.6.9'},\n",
       "  {'name': 'depd', 'version': '2.0.0'},\n",
       "  {'name': 'destroy', 'version': '1.2.0'},\n",
       "  {'name': 'http-errors', 'version': '2.0.0'},\n",
       "  {'name': 'iconv-lite', 'version': '0.4.24'},\n",
       "  {'name': 'on-finished', 'version': '2.4.1'},\n",
       "  {'name': 'qs', 'version': '6.11.0'},\n",
       "  {'name': 'raw-body', 'version': '2.5.2'},\n",
       "  {'name': 'type-is', 'version': '~1.6.18'},\n",
       "  {'name': 'unpipe', 'version': '1.0.0'}],\n",
       " 'bytes': [],\n",
       " 'content-type': [],\n",
       " 'debug': [{'name': 'ms', 'version': '2.1.2'}],\n",
       " 'ms': [],\n",
       " 'depd': [],\n",
       " 'destroy': [],\n",
       " 'http-errors': [{'name': 'depd', 'version': '2.0.0'},\n",
       "  {'name': 'inherits', 'version': '2.0.4'},\n",
       "  {'name': 'setprototypeof', 'version': '1.2.0'},\n",
       "  {'name': 'statuses', 'version': '2.0.1'},\n",
       "  {'name': 'toidentifier', 'version': '1.0.1'}],\n",
       " 'inherits': [],\n",
       " 'setprototypeof': [],\n",
       " 'statuses': [],\n",
       " 'toidentifier': [],\n",
       " 'iconv-lite': [{'name': 'safer-buffer', 'version': '>= 2.1.2 < 3.0.0'}],\n",
       " 'safer-buffer': [],\n",
       " 'on-finished': [{'name': 'ee-first', 'version': '1.1.1'}],\n",
       " 'ee-first': [],\n",
       " 'qs': [{'name': 'side-channel', 'version': '1.0.4'}],\n",
       " 'side-channel': [{'name': 'call-bind', 'version': '1.0.0'},\n",
       "  {'name': 'get-intrinsic', 'version': '1.0.2'},\n",
       "  {'name': 'object-inspect', 'version': '1.9.0'}],\n",
       " 'call-bind': [{'name': 'function-bind', 'version': '1.1.1'},\n",
       "  {'name': 'get-intrinsic', 'version': '1.0.2'}],\n",
       " 'get-intrinsic': [{'name': 'function-bind', 'version': '1.1.1'},\n",
       "  {'name': 'has', 'version': '1.0.3'},\n",
       "  {'name': 'has-symbols', 'version': '1.0.3'}],\n",
       " 'object-inspect': [],\n",
       " 'raw-body': [{'name': 'bytes', 'version': '3.1.2'},\n",
       "  {'name': 'http-errors', 'version': '2.0.0'},\n",
       "  {'name': 'iconv-lite', 'version': '0.4.24'},\n",
       "  {'name': 'unpipe', 'version': '1.0.0'}],\n",
       " 'unpipe': [],\n",
       " 'type-is': [{'name': 'media-typer', 'version': '0.3.0'},\n",
       "  {'name': 'mime-types', 'version': '~2.1.24'}],\n",
       " 'media-typer': [],\n",
       " 'content-disposition': [{'name': 'safe-buffer', 'version': '5.2.1'}],\n",
       " 'safe-buffer': [],\n",
       " 'cookie': [],\n",
       " 'cookie-signature': [],\n",
       " 'encodeurl': [],\n",
       " 'escape-html': [],\n",
       " 'etag': [],\n",
       " 'finalhandler': [{'name': 'debug', 'version': '2.6.9'},\n",
       "  {'name': 'encodeurl', 'version': '~1.0.2'},\n",
       "  {'name': 'escape-html', 'version': '~1.0.3'},\n",
       "  {'name': 'on-finished', 'version': '2.4.1'},\n",
       "  {'name': 'parseurl', 'version': '~1.3.3'},\n",
       "  {'name': 'statuses', 'version': '2.0.1'},\n",
       "  {'name': 'unpipe', 'version': '~1.0.0'}],\n",
       " 'parseurl': [],\n",
       " 'fresh': [],\n",
       " 'merge-descriptors': [],\n",
       " 'methods': [],\n",
       " 'path-to-regexp': [],\n",
       " 'proxy-addr': [{'name': 'forwarded', 'version': '0.2.0'},\n",
       "  {'name': 'ipaddr.js', 'version': '1.9.1'}],\n",
       " 'forwarded': [],\n",
       " 'ipaddr.js': [],\n",
       " 'range-parser': [],\n",
       " 'send': [{'name': 'debug', 'version': '2.6.9'},\n",
       "  {'name': 'depd', 'version': '2.0.0'},\n",
       "  {'name': 'destroy', 'version': '1.2.0'},\n",
       "  {'name': 'encodeurl', 'version': '~1.0.2'},\n",
       "  {'name': 'escape-html', 'version': '~1.0.3'},\n",
       "  {'name': 'etag', 'version': '~1.8.1'},\n",
       "  {'name': 'fresh', 'version': '0.5.2'},\n",
       "  {'name': 'http-errors', 'version': '2.0.0'},\n",
       "  {'name': 'mime', 'version': '1.6.0'},\n",
       "  {'name': 'ms', 'version': '2.1.3'},\n",
       "  {'name': 'on-finished', 'version': '2.4.1'},\n",
       "  {'name': 'range-parser', 'version': '~1.2.1'},\n",
       "  {'name': 'statuses', 'version': '2.0.1'}],\n",
       " 'mime': [],\n",
       " 'serve-static': [{'name': 'encodeurl', 'version': '~1.0.2'},\n",
       "  {'name': 'escape-html', 'version': '~1.0.3'},\n",
       "  {'name': 'parseurl', 'version': '~1.3.3'},\n",
       "  {'name': 'send', 'version': '0.18.0'}],\n",
       " 'utils-merge': [],\n",
       " 'vary': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from olivia_finder.data_source.scrapers.npm import NpmScraper\n",
    "npm_scraper = NpmScraper()\n",
    "express_network = npm_scraper.generate_package_dependency_network(\"express\")\n",
    "express_network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Package manager**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olivia_finder.package_manager import PackageManager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare the class**\n",
    "\n",
    "Initialize the packagemanager class with the implementation of the scraper we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olivia_finder.data_source.scrapers.pypi import PypiScraper\n",
    "pypi_scraper_pm = PackageManager(PypiScraper())\n",
    "\n",
    "from olivia_finder.data_source.scrapers.npm import NpmScraper\n",
    "npm_scraper_pm = PackageManager(NpmScraper())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or init the class from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olivia_finder.data_source.csv_network import CSVNetwork\n",
    "\n",
    "# Cran data from scraping\n",
    "cran_scraped_csv_pm = PackageManager(\n",
    "    CSVNetwork(\n",
    "        \"results/csv_datasets/cran_adjlist_scraping.csv\",   # Path to the CSV file\n",
    "        \"CRAN\",                                             # Name of the data source\n",
    "        \"CRAN as a CSV file\",                               # Description of the data source\n",
    "        dependent_field=\"name\",                             # Name of the field that contains the dependencies\n",
    "        dependency_field=\"dependency\",                      # Name of the field that contains the name of the package\n",
    "        dependent_version_field=\"version\",                  # Name of the field that contains the version of the package\n",
    "        dependency_version_field=\"dependency_version\",     # Name of the field that contains the version of the dependency\n",
    "        dependent_url_field=\"url\",                          # Name of the field that contains the URL of the package\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cran data from libraries.io\n",
    "cran_librariesio_csv_pm = PackageManager(\n",
    "    CSVNetwork(\n",
    "        \"results/csv_datasets/cran_librariesio_dependencies.csv\",   # Path to the CSV file  \n",
    "        \"CRAN\",                                                     # Name of the data source\n",
    "        \"CRAN as a CSV file\",                                       # Description of the data source\n",
    "        dependent_field=\"Project Name\",                             # Name of the field that contains the dependencies\n",
    "        dependency_field=\"Dependency Name\",                         # Name of the field that contains the name of the package\n",
    "        dependent_version_field=\"Version Number\",                   # Name of the field that contains the version of the package\n",
    "        dependency_version_field=\"Dependency Requirements\"          # Name of the field that contains the version of the dependency\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obtain packages**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a package from package manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package:\n",
      "  name: networkx\n",
      "  version: 3.0\n",
      "  url: https://pypi.org/project/networkx/\n",
      "  dependencies:\n",
      "    numpy:(>=1.20)\n",
      "    scipy:(>=1.8)\n",
      "    matplotlib:(>=3.4)\n",
      "    pandas:(>=1.3)\n",
      "    pre-commit:(>=2.20)\n",
      "    mypy:(>=0.991)\n",
      "    sphinx:(==5.2.3)\n",
      "    pydata-sphinx-theme:(>=0.11)\n",
      "    sphinx-gallery:(>=0.11)\n",
      "    numpydoc:(>=1.5)\n",
      "    pillow:(>=9.2)\n",
      "    nb2plots:(>=0.6)\n",
      "    texext:(>=0.6.7)\n",
      "    lxml:(>=4.6)\n",
      "    pygraphviz:(>=1.10)\n",
      "    pydot:(>=1.4.2)\n",
      "    sympy:(>=1.10)\n",
      "    pytest:(>=7.2)\n",
      "    pytest-cov:(>=4.0)\n",
      "    codecov:(>=2.1)\n"
     ]
    }
   ],
   "source": [
    "networkx = pypi_scraper_pm.obtain_package(\"networkx\")\n",
    "networkx.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package:\n",
      "  name: A3\n",
      "  version: 1.0.0\n",
      "  url: https://cran.r-project.org/package=A3\n",
      "  dependencies:\n",
      "    R:â¥ 2.15.0\n",
      "    xtable:nan\n",
      "    pbapply:nan\n"
     ]
    }
   ],
   "source": [
    "cran_scraped_csv_pm.obtain_package(\"A3\").print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get packages from a list of package names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webscraping-based implementation obtains the data manager website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<olivia_finder.package.Package at 0x7fed5937bd90>,\n",
       " <olivia_finder.package.Package at 0x7fed27ae2250>,\n",
       " <olivia_finder.package.Package at 0x7fed27ae26a0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packages = pypi_scraper_pm.obtain_packages([\"networkx\", \"numpy\", \"pandas\"])\n",
    "packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV file-based implementation obtains file data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<olivia_finder.package.Package at 0x7f28bada7340>,\n",
       " <olivia_finder.package.Package at 0x7f28bad7c5e0>,\n",
       " <olivia_finder.package.Package at 0x7f28bad7e0b0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cran_packages = cran_scraped_csv_pm.obtain_packages([\"A3\", \"pbapply\", \"xtable\"])\n",
    "cran_packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all packages from a package manager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:***\n",
    "\n",
    "The functionality of storing packages in the PackageManager object has been implemented\n",
    "\n",
    "-   Can be activated by flag\n",
    "\n",
    "    ```python\n",
    "    extend=True\n",
    "    ```\n",
    "\n",
    "The functionality of showing the progress of obtaining packages has been implemented\n",
    "\n",
    "-   Can be activated by flag\n",
    "\n",
    "    ```python\n",
    "    show_progress=True\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the packages from a package manager can take a while, so it is recommended to save the data to a CSV file for later use.\n",
    "\n",
    "We can see that the execution time for half a million packages (Pypi) is around 7 hours.\n",
    "In the case of Bioconductor, to obtain the 2000 packages it contains, the execution time is around 4 minutes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   From **Spraper** data source implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|ââ        | 66423/438514 [1:14:50<6:40:39, 15.48it/s] "
     ]
    }
   ],
   "source": [
    "pypi_packages = pypi_scraper_pm.obtain_packages(extend=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 2183/2183 [03:47<00:00,  9.59it/s]\n"
     ]
    }
   ],
   "source": [
    "bioc_scraper_pm = PackageManager(BiocScraper())\n",
    "bioconductor_packages = bioc_scraper_pm.obtain_packages(extend=True, show_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   From **CSVNetwork** data source implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 18195/18195 [02:04<00:00, 145.67it/s]\n"
     ]
    }
   ],
   "source": [
    "cran_packages = cran_scraped_csv_pm.obtain_packages(extend=True, show_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen there is inconsistency among the different data sources, it is recommended to use the most up-to-date source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data persistence**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality of saving the PackageManager object in disk and loading of it has also been implemented, in order to maintain persistence and not repeat processes such as WebScraping."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the PackageManager object**\n",
    "\n",
    "We can save the object through the `save` function\n",
    "\n",
    "The file extension is irrelevant since it is a binary serialization, but by agreement the extension has been chosen **.olvpm** \"to identify the PackageManager files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cran_scraped_csv_pm.save(\"results/package_managers/cran.olvpm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the PackageManager object**\n",
    "\n",
    "We can load the PackageManager object through the static method\n",
    "\n",
    "```python \n",
    "    PackageManager.load(path:str)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cran_loaded_csv_pm = PackageManager.load(\"results/package_managers/cran.olvpm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the CSV format**\n",
    "\n",
    "We can export the data of the packages to a CSV, with a structure similar to that of the data of Libraries.\n",
    "\n",
    "We can use the following function to generate a Pandas Dataframe and then write the file as CSV\n",
    "\n",
    "-   \n",
    "    ```python\n",
    "    pandas_df = package_manager.export_adjlist()\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dependency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3</td>\n",
       "      <td>xtable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3</td>\n",
       "      <td>pbapply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AATtools</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AATtools</td>\n",
       "      <td>magrittr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name dependency\n",
       "0        A3          R\n",
       "1        A3     xtable\n",
       "2        A3    pbapply\n",
       "3  AATtools          R\n",
       "4  AATtools   magrittr"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the package manager as a adjacency list\n",
    "cran_df = cran_loaded_csv_pm.export_adjlist()\n",
    "cran_df.to_csv(\"results/csv_datasets/cran_full_adjlist.csv\", index=False)\n",
    "cran_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
